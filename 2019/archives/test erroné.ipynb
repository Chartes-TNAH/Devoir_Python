{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f6/1d/d925cfb4f324ede997f6d47bea4d9babba51b49e87a767c170b77005889d/nltk-3.4.5.zip (1.5MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5MB 101kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six in /snap/jupyter/6/lib/python3.7/site-packages (from nltk) (1.12.0)\n",
      "Building wheels for collected packages: nltk\n",
      "  Building wheel for nltk (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/formation1/snap/jupyter/6/.cache/pip/wheels/96/86/f6/68ab24c23f207c0077381a5e3904b2815136b879538a24b483\n",
      "Successfully built nltk\n",
      "Installing collected packages: nltk\n",
      "Successfully installed nltk-3.4.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/formation1/snap/jupyter/6/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Any, Callable\n",
    "\n",
    "import nltk\n",
    "from nltk.cluster import KMeansClusterer\n",
    "from numpy.core.multiarray import ndarray\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import unicodedata\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "\n",
    "import ssl \n",
    "\n",
    "ssl._create_default_https_context = ssl._create_unverified_context \n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_punct(token): #permet de traiter la ponctuation lors de la lemmatisation nltk\n",
    "    return all(unicodedata.category(char).startswith('P') for char in token)\n",
    "\n",
    "\n",
    "class TextNormalizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, language='english'):\n",
    "        self.stopwords = set(nltk.corpus.stopwords.words(language))\n",
    "        self.lemmatizer = nltk.WordNetLemmatizer()\n",
    "\n",
    "    def is_stopword(self, token):\n",
    "        return token.lower() in self.stopwords\n",
    "\n",
    "    def normalize(self, document):\n",
    "        return [\n",
    "            self.lemmatize(token, tag).lower()\n",
    "            for paragraph in document\n",
    "            for sentence in paragraph\n",
    "            for (token, tag) in sentence\n",
    "            if not self.is_punct(token) and not self.is_stopword(token)\n",
    "        ]\n",
    "\n",
    "    def lemmatize(self, token, tag):\n",
    "        pass\n",
    "\n",
    "    def is_punct(self, token):\n",
    "        pass\n",
    "\n",
    "    def transform(self, documents):\n",
    "        return [' '.join(self.normalize(doc)) for doc in documents]\n",
    "\n",
    "\n",
    "class KMeansClusters(BaseEstimator, TransformerMixin):\n",
    "    distance: Callable[[Any, Any], ndarray]\n",
    "\n",
    "    def __init__(self, k=7):\n",
    "        \"\"\"\n",
    "        k is the number of clusters\n",
    "        model is the implementation of Kmeans\n",
    "        \"\"\"\n",
    "        self.k = k\n",
    "        self.distance = nltk.cluster.util.cosine_distance\n",
    "        self.model = KMeansClusterer(self.k, self.distance, avoid_empty_clusters=True)\n",
    "\n",
    "\n",
    "class OneHotVectorizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.vectorizer = CountVectorizer(binary=True)\n",
    "\n",
    "    def fit(self):\n",
    "        return self\n",
    "\n",
    "    def transform(self, documents):\n",
    "        freqs = self.vectorizer.fit_transform(documents)\n",
    "        return [freq.toarray()[0] for freq in freqs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'nltk.corpus.reader.plaintext.PlaintextCorpusReader'>\n"
     ]
    }
   ],
   "source": [
    "corpusdir = './cache2019/cacheTXT/'\n",
    "\n",
    "newcorpus = PlaintextCorpusReader(corpusdir, '.*') #newcorpus est objet nltk lemmatisé concentrant tous les textes des abstracts\n",
    "\n",
    "model = Pipeline([\n",
    "    ('norm', TextNormalizer()),\n",
    "    ('vect', OneHotVectorizer()),\n",
    "    ('clusters', KMeansClusters(k=4)),\n",
    "])\n",
    "clusters = model.fit_transform(newcorpus)\n",
    "pickles = list(newcorpus.fileids)\n",
    "\n",
    "for idx, cluster in enumerate(clusters):\n",
    "    print(\"Document '{}' assigned to cluster {}.\".format(pickles[idx],cluster))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
