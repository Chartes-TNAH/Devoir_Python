{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEI Conference 2019 #\n",
    "\n",
    "## Etape 3 : L'Analyse ##\n",
    "\n",
    "### Préambule ###\n",
    "\n",
    "L'objectif présent est d'analyser les abstracts afin de les regrouper en cluster, permettant de dégager des thématiques, de comparer les différentes conférences ou bien d'étudier si les distinctions faites par les organisateurs de la conférence 2019 entre les différentes interventions est bien pertinente.\n",
    "Nous pourrons également critiquer les choix de l'ordinateur.\n",
    "\n",
    "Nous allons faire ici de la classification non supervisée grâce au package SKLearn, librairie dédiée au Machine Learning notamment.\n",
    "\n",
    "### Les packages ### \n",
    "\n",
    "Il faut lancer la cellule ci-dessous une seule fois afin de télécharger la librairie sklearn. Cela doit être fait une seule fois, au premier lancement. Ensuite, il ne faudra plus jamais le lancer. Il est possible qu'il faille redémarrer le noyau après."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-52-c99697bebcaa>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-52-c99697bebcaa>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    pip install sklearn\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "pip install sklearn\n",
    "pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il faut lancer la cellule ci-dessous à chaque lancement de ce notebook-ci."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tout d'abord, il faut ' stemmer ' les abstracts, c'est à dire qu'il faut prendre chaque mot de chaque abstract et le remplacer par son lemme. Cela permettra à la machine de ne pas être faussée par l'usage divers d'un seul et même mot racine (les mots \"buy\", \"buys\", \"bought\" et \"buying\" ne deviennent plus qu'un seul et même mot). Ainsi, l'analyse n'en sera que plus précise et plus juste.\n",
    "\n",
    "Ce texte stemmé est stocké dans le cache, dans un répertoire propre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path = \"./cache2019/cacheTXT/\" #chemin permettant d'accès aux abstracts\n",
    "Path_output = \"./cache2019/cacheSTEM/\" #chemin de sortie des abstracts stemmés\n",
    "filelist = os.listdir(Path) #filelist est une liste regroupant tous les chemins vers les différents abstracts.\n",
    "if not os.path.exists(\"./cache2019/cacheSTEM/\"): #permet de créer un dossier dans le cache s'il est supprimé.\n",
    "    os.makedirs(\"./cache2019/cacheSTEM/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 4035)\t0.8714423053310905\n",
      "  (0, 2838)\t0.333745611880824\n",
      "  (0, 142)\t0.35944704064650296\n",
      "  (1, 4035)\t0.6849882856854673\n",
      "  (1, 2838)\t0.26233731497627444\n",
      "  (1, 148)\t0.6181759640183353\n",
      "  (1, 142)\t0.2825396594369106\n",
      "  (2, 4035)\t0.6569841659774607\n",
      "  (2, 2838)\t0.251612276715042\n",
      "  (2, 186)\t0.6569841659774607\n",
      "  (2, 142)\t0.2709886962883761\n",
      "  (3, 4035)\t0.8714423053310905\n",
      "  (3, 2838)\t0.333745611880824\n",
      "  (3, 142)\t0.35944704064650296\n",
      "  (4, 4035)\t0.6849882856854673\n",
      "  (4, 2838)\t0.26233731497627444\n",
      "  (4, 145)\t0.6181759640183353\n",
      "  (4, 142)\t0.2825396594369106\n",
      "  (5, 4035)\t0.7051478776999994\n",
      "  (5, 2838)\t0.2700580502802617\n",
      "  (5, 142)\t0.29085496114527937\n",
      "  (5, 115)\t0.5875700057279885\n",
      "  (6, 4035)\t0.7208338508359468\n",
      "  (6, 2838)\t0.2760654757520075\n",
      "  (6, 165)\t0.561946838609136\n",
      "  :\t:\n",
      "  (39, 170)\t0.6569841659774607\n",
      "  (39, 142)\t0.2709886962883761\n",
      "  (40, 4035)\t0.6569841659774607\n",
      "  (40, 2838)\t0.251612276715042\n",
      "  (40, 142)\t0.2709886962883761\n",
      "  (40, 47)\t0.6569841659774607\n",
      "  (41, 4035)\t0.7051478776999994\n",
      "  (41, 2838)\t0.2700580502802617\n",
      "  (41, 142)\t0.29085496114527937\n",
      "  (41, 34)\t0.5875700057279885\n",
      "  (42, 4035)\t0.6849882856854673\n",
      "  (42, 2838)\t0.26233731497627444\n",
      "  (42, 159)\t0.6181759640183353\n",
      "  (42, 142)\t0.2825396594369106\n",
      "  (43, 4035)\t0.8714423053310905\n",
      "  (43, 2838)\t0.333745611880824\n",
      "  (43, 142)\t0.35944704064650296\n",
      "  (44, 4035)\t0.6849882856854673\n",
      "  (44, 2838)\t0.26233731497627444\n",
      "  (44, 142)\t0.2825396594369106\n",
      "  (44, 31)\t0.6181759640183353\n",
      "  (45, 4035)\t0.6569841659774607\n",
      "  (45, 2838)\t0.251612276715042\n",
      "  (45, 179)\t0.6569841659774607\n",
      "  (45, 142)\t0.2709886962883761\n"
     ]
    }
   ],
   "source": [
    "stemmer = PorterStemmer() #J'importe une méthode acceptant une chaîne de caractère à une variable.\n",
    "documents = [] #j'instancie une liste qui regroupera l'ensemble des chemins vers les documents\n",
    "for abstract in filelist:\n",
    "    with open(Path + abstract, \"r\", encoding=\"UTF-8\") as y:\n",
    "        documents.append(Path_output + abstract)\n",
    "        texte = y.read()\n",
    "        liste_mots_tokenise = [] # j'instancie une liste qui accueillera chaque lemme (le lemme étant une str)\n",
    "        liste_mots_a_tokenise = texte.split() #je crée une liste sur laquelle je vais pouvoir boucler\n",
    "        for elem in liste_mots_a_tokenise:\n",
    "            mot_tokenise = stemmer.stem(elem) #mot_tokenise est une str, et est le lemme du mot sur lequel je boucle\n",
    "            liste_mots_tokenise.append(mot_tokenise) #j'ajoute chaque mot à une liste, qu'ensuite je join pour recréer l'abstract sous la forme d'une str\n",
    "        resultat = ' '.join(liste_mots_tokenise)\n",
    "        with open(Path_output + abstract, \"w\", encoding=\"UTF-8\") as z:\n",
    "            z.write(resultat) #resultat est une str\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "documents = vectorizer.transform(documents)\n",
    "print(documents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici, on associe à chaque mot de tous les abstracts une coordonnée unique à chaque mot utilisé, pour ensuite pouvoir les placer sur un plan en 2D et ainsi relever des ressemblances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['use machin learn for the autom classif of stage direct in tei-encod drama corpora author daria maximova (nation research univers higher school of economics, moscow, ru) frank fischer (dariah-eu and nation research univers higher school of economics, moscow, ru) abstract the <stage> tag is a core element for the encod of drama. the tei guidelin suggest nine valu for it type attribute, which is wide use in larg corpora such as the french théâtre classique, the shakespear folger librari or the swedish dramawebben. thi paper introduc an approach to automat assign stage-direct type to the tei-p5-encod russian drama corpus, rusdracor (https://dracor.org/). the corpu current featur 144 play rang from mid-18th to mid-20th centuri which make for 32 753 stage direct with 144,525 tokens. We select 18 play compris 6,569 stage direct to repres the breadth of the corpus. for the manual annot we establish a clear set of rule to identifi the stage-direct type propos by the tei guidelin (https://www.tei-c.org/release/doc/tei-p5-doc/en/html/ref-stage.html). follow the annot of our subcorpus, we develop a tool for the classif of the remain play without human interference. for the convers of stage direct into featur vectors, we use morpholog and semant data. our tool in it current state is abl to classifi differ type with an F1 score of approx. 0.75, which mean that 3 out of 4 stage direct of ani given type are assign correctly. our work will inform a dedic analysi of stage directions, which after preliminari studi by sperantov (1998) and detken (2009) will be base on larger corpora allow for a descript of the evolv of stage direct over 200 years.', 'tei xml and delta format interchang tei xml and delta format interchang author nichola cole, univers of oxford; david De roure, univers of oxford; pip willcox, the nation archiv abstract thi paper will interrog the close link between tei and xml, and examin whether the inform encod by tei could be more easili process (for certain applications) by express it in altern formats. In particular, thi paper will examin the rise of variou socal ‘delta’ format in the javascript ecosystem, that are particularli popular with project develop multi-user, collabor editors, and which express the structur and featur of text by separ the text itself from the list of attribut associ with each block, line, or character. thi offer sever advantag for processing, and allow such document to take advantag of an emerg ecosystem. As thi paper examines, for certain applications, such as the represent of negoti texts, it offer an abil to repres and manipul rich-text, annot data in a way that is not possibl in tradit tei format becaus algorithm to manipul delta format exist that have no compar (implemented) analogu for xml documents. thi paper will lay out the current state of compet ‘delta’ formats, the advantag they offer in separ text from the attribut that describ it, the problem that some of these popular but poorly-defin format would have encount in tri to express a tei document in a lossless fashion, whether a translat layer between tei xml and such a format would be possibl to achieve, and whether the academic, text-process ecosystem would benefit from an altern format that draw on such ideas, particularli as a transient format (with import/export to/from xml) for edit and data-process within particular applications. given the poorli defin state of some other use but evolv formats, thi paper will examin whether effort by the DH commun to standard version for academ purpos and link them to the current tei standard would bear fruit.', 'correspsearch v2 – new way of explor correspond correspsearch v2 – new way of explor correspond the webservic correspsearch ha been develop sinc 2014 to aggreg correspond metadata and offer it to the scientif commun for research and retrieval. the data is obtain in the tei-xml-bas \"correspond metadata interchang format\" (cmif) - develop by the tei correspond sig. A prototyp wa present at the tei confer in lyon in 2015. sinc 2017, the web servic ha been further develop in a project fund by the german research foundat (dfg). At the same time, the data quantiti increas from around 25,000 to over 52,000 letter - mani edit offer letter metadata in cmif by now. In order to enabl even small edit project to deliv data in cmif and to simplifi the captur of letter metadata from print editions, the cmif creator wa develop in 2018 to allow a conveni browser-bas input and process of metadata into cmif. over time, the develop of the web servic focus on both, the system architectur and the improv of the search, which now - in accord with the ongo develop of cmif - for the first time doe captur letter content as well. In addition, sever differ edit of one letter can be link to each other or connect to associ archives. To add on that, correspsearch doe offer a map-based, geograph search for write and receiv locat by now. the api interfac and network possibl of correspsearch have also been extended. with cslink, a javascript-bas widget to present correspond network is now avail on github as open sourc softwar that can be integr into ani digit edition. the articl will present and discuss the further develop of the web service, as well as the community’ experi with the aggreg of metadata. dure the presentation, the second version of correspsearch will be releas as public beta. bibliographi dumont, stefan: „correspsearch – connect scholarli edit of letters“, journal of the text encod initi issu 10 (2016), http://journals.openedition.org/jtei/1742 tei correspond sig (hrsg.): „correspond metadata interchang format (cmif)“, 2018.2015, https://github.com/tei-correspondence-sig/cmif', 'get along with relat databas background relat databas (rdbs) and xml are both matur technolog that have been in common use for decades. It is arguabl that they aris out of the same roots. earli work on data storag and model in the 1960 gave rise to ibm’ mainfram databas manag system ims, which repres data in the form of hierarch trees. c.j. date’ (1991) classic An introduct to databas system ha an appendix devot to im which describ it in terminolog that would be familiar to ani xml encoder. im even address the perenni issu of overlap hierarchies, by allow A secondari data structur which is still a hierarchy, but a hierarchi in which particip segment have been rearranged, possibl drastically; in other words, it allow for multipl hierarchi over the same dataset. however, begin with the work of e.f. codd in the 1970 and the rise of sql, the relat databas model familiar today becam dominant, and remain so until the rel recent popular of nosql approaches. both relat databas and xml have strength and weak as data storag and model systems. most research work with human histor and literari data would argu for the superior of xml, sinc it allow unlimit nesting, linking, and complexity. rdb propon claim superior integr constraints, querying, and process speed, although recent advanc in xml languages, databas engin and tool have somewhat erod those advantages. nevertheless, rdb remain popular, and mani research seem instinct to prefer them. most digit human programm have encount research who know littl about databas or data modelling, but are nevertheless convinc that what they need and must have for their project is a database. databas are somehow compel and attract in a way that xml is not. perhap the familiar of tabular data represent is comforting; mayb forc data into constrain represent seem to constitut master it somehow; or perhap the tendenc to gather initi data in the earli stage of a project use spreadsheets, for want of a better tool, encourag concept of data (especi metadata) in term of column and rows. whatev the reason, in one way or another, sometim against our better judgement or advice, a project may end up with both an rdb and an xml document collection, and programm must then integr these distinct form of data when build project outputs. approach to integr rdb and xml data have normal taken the form of store xml data into rdb fields, and then provid some level of richer access to that data through the use of xpath or xqueri (see bertino and catania for a use overview). thi is the approach taken by the remetca team (gonzález-blanco and rodríguez 2015): xml fragment repres vers (not full documents) are store in text field in a relat database, and the relationship between them are model use the rdb schema. however, such an approach is far from ideal; gonzález-blanco and rodríguez describ some of the limit and frustrat they encount in model the poetic structur of the vers in their database; they struggl with a complex model of relationship among those compon which are veri difficult to repres in a database, and they conclud that the e-r model is inappropri for thi purpos due to it center-bas structure, with the entiti of poem, line, and stanza in the middl of it referenti domain of studi (para 8). gibson (2012) describ a similar scenario with mix rdb and xml data, and how he use saxonʼ sql extens function to overcom the problem. however, store xml data in rdb field is suboptimal. most seriou encod project make use of version-control system such as git or subversion, for veri good reasons: in a project with mani transcrib and encoders, where multipl wave of encod and annot may be appli to each document, it is essenti to maintain a detail revis histori which make it possibl to recov ani previou incarn of ani document, and to track the revis made to specif part of the document by specif encoders. the digit victorian period poetri project thi present will focu on the integr of rdb and xml data in the digit victorian period poetri project. thi project began life mani year ago as a pure-metadata project, captur inform about ten of thousand of poem that appear in british period dure the nineteenth century. At that time, an rdb system seem a natur and suffici tool for the job, so a mysql database, along with a data-entri interface, wa set up for the researchers, and data collect proceed rapidli (figur 1). however, after some year the project gain an addit research focus, and, more recently, fund from the social scienc and research council of canada, to transcrib and encod a subset of these poems; we are focus primarili on the decad year (1820, 1830, 1840 and so on through to 1900), and we expect to encod around 2,000 poems. meanwhile, index of the much larger dataset continues. A record in the relat database. our long-term plan is for the entir dataset to be in the form of tei xml files, but for the next few months, data will continu to be ad to the rdb system, sinc we have good method and protocol for this, as well as train research assist who are use to work with it. We are now also well into our encod process, and for that we need to gener individu tei file for each poem, and store them in subversion. In thi hybrid project, the canon sourc of metadata for the poem is the rdb, while the canon sourc of textual data is the tei xml files. To build and test the project outputs, we need to gener tei file for everi poem, whether or not it has, or will have, an encod transcription. the metadata store in the tei file must be period updat base on the rdb, without disturb ani of the textual encod or the addit metadata in the tei header relat specif to the encod (respons statements, rendit elements, categori references, and so on). chang to the rdb data may result in chang to the id and filenam of the relat tei file, so ani exist tei data must be migrat to a new file, and the svn repositori must be appropri updated. the present will describ how thi process is accomplish safe without loss of data, use a system base primarili on apach ant and xslt (). A simpl represent of the metadata integr process. normally, we run the databas integr process onli on a small subset of the data at one time; for example, we may refresh the metadata in all the poem from a specif period in a specif year, in prepar for the transcription/encod team start work on that year. By the end of 2019, we plan to elimin the relat databas entirely. although it is a conveni tool for collect metadata while work through larg number of periodicals, it limit are constantli frustrating; everi day we encount situat in which someth rel trivial to encod in tei would requir substanti modif to the structur and complex of the database. for example, degre of uncertainti about the ident of an author, or about whether two pseudonym repres the same person, can easili be express in tei, but requir addit join tabl in the database. similarly, some poem claim to be translat but are probabl not, and their translat are probabl their authors. ambigu such as thi are difficult to handl in a categor system such as an rdb, but they are the bread and butter of tei encoding. use of the hashtag system for more flexibl databas entries. meanwhile, we live with the database, and devis new cun strategi to make it more bearable. the constantly-shift requir of the metadata team, as they encount new featur and unexpect except in the incom data, have led to the develop of an ad-hoc system base on hashtag fields, where research can use twitter-styl hashtag in free text note field to captur thing that would otherwis requir databas extens or modification. hashtag are defin in a separ table, and new one can be ad at will, but their use is polic by a diagnost process that identifi ani instanc of hashtag in the note field which are not in the hashtag table, therebi avoid the prolifer of typograph error or the use of undefin hashtags. the hashtag are themselv transform into a taxonomi in the tei data, and from there they are integr into the project schema as valu for target attribut on catref elements. show the hashtag table, along with an instanc of a hashtag in use in a pooem record. the note field abov it show more evid of data that realli need rich encod for titles, dates, name and so on, but must, for the moment, be handl with plain text.', 'introduc objectification: when is an <object> a <place>? introduc objectification: when is an <object> a <place>? the tei guidelin recent (a of tei P5 version 3.5.0, januari 2019) ad element for describ object and encod the name of objects. these element include: objectname, object, listobject objectidentifier, as well as chang to mani other element to loosen their descript slightly. thi paper will introduc these new element to tei user who mani not have had the chanc to use them yet, as well as introduc potenti use for the encod of object descript in tei files. the paper will not, however, mere introduc these elements, but will also look at chang still to be done in the tei guidelin to fulli support the descript of objects. for example, much of the object descript content model is taken wholesal from that for describ manuscripts. the objectidentifi wa base on the msidentifi (with some import changes), and object still ha element like mscontent in it content model. there are mani chang that are still need and thi paper activ seek to involv the commun in design these changes. chang are not onli requir to exist content model of elements, but also to clarifi the semant of the distinct between elements. use exampl of larg object such as the central librari of the nation autonom univers of mexico (unam) (c.f. https://en.wikipedia.org/wiki/central_library_(unam)), which ha substanti and import mosaic on it extern walls, thi paper will ask where the border line are between what one might consid an object and what might instead be encod as a place. conclud that the differ lie in the markup intent (a with so much in the tei), thi paper draw a distinct between encod for object description, and geo-polit entities.', 'analyz and visual uncertain knowledge: introduc the providedh open scienc platform underli uncertainti in DH research data affect decision-mak and persist dure the project’ lifecycle. thi uncertainti will alway be present. thus, effort in provid technic support for humanist research should focu on manag and make it more transparent, rather than remov it. locat and trace (certain type of) uncertainti through the evolut of a textual corpu can be done with the use of tei tag [tei consortium 2019]. however, the use of these method is not a common practice. the motiv of thi paper is to address one possibl barrier to wider use of these tag by provid a user-friendli interfac to collabor annot text with uncertainty. We propos some minor extens of the tei specif that follow from our metric of uncertainty. the first extens is ad the new categori attribut to the certainti element, requir to indic the sourc of uncertainty. the second extens is to chang the close list of valu of the locu attribut to an open list, in order to be abl to explicitli indic the attribut to which uncertainti refers. additionally, the author detect a need to describ the natur and type of uncertainti as well as evalu the degre of uncertainti the piec of data introduc [therón et al. 2019]. our tool on the platform were develop against the background of human-cent design with the focu onto eas uncertainti annot and visualization, promot the use of tei standard and make uncertainti play a more activ role in the research process. the platform fulfil the common need of a complet research lifecycl by provid well-known technolog for basic tasks, such as versioning, file management, text editing, and refer resourc management. screenshot of the collabor tei editor on the providedh open scienc platform the author aim to get feedback from the tei commun to improv their tool in a gener way and fulfil further need of the audience.', 'referenc an editori ontolog from the tei: An attempt to overcom inform typolog referenc an editori ontolog from the tei: An attempt to overcom inform typolog jakub šimek (heidelberg univers library) the introduct of tei P5 in 2007 wa accompani by effort of map content of tei document to high level conceptu model like cidoc crm. they focus on prosopograph inform connect the textual content with index metadata. moreover, a flexibl use of the <taxonomy> element wa implemented, allow for ontology-lik thesauri which can be refer to by pointer from an edition. while these mechan for name entiti and term enabl power indexing, littl attent so far ha been given to formal the way of deal with editori and documentari typolog which are use in attribut like e.g. @type, @function and @reason. these typolog refer to document types, textual and editori phenomena, the process of text product and text redact and similar categori of concept which character the text itself rather than extern entiti refer to by the textual content. attribut like @type do not permit the use of pointer to formal conceptu definit as their expect data type is teidata.enumerated, not uri pointer (although the <equiv> element in odd specif could map differ xml compon to formal uri externally). thi paper present the attempt made at the heidelberg univers librari to enabl in tei document pointer to definit of editori phenomena administr in a owl ontolog (“heiedit concepts”) in order to replac teidata.enumer attribut with uri pointer mechanisms. thi strategi make use of a few tei attribut like @ana whose data type is teidata.point and some addit pointer attribut provid by a schema extension. A “privat uri scheme” state in the tei header allow the use of abbrevi uri forms. the goal of thi institut strategi is not onli a standard of the tei encod adopt by in-hous edit project and cooper endeavour but also a transpar in documentari and editori terminolog use in tei code. scholarli edition, ontology, pointer, uri, odd', \"case studi tei customization: A restrict tei format for edit open access (eoa) edit open access (eoa) , thi project is current fund by the german feder ministri of educ and research, grant number 16oa061. is a publish platform for scholarli monograph and edit volum that provid author and publish with the mean to distribut their public in a varieti of formats, e.g. onlin as html, download as pdf and epub, or as a print book. In the new version of the platform, which is current under development, tei-xml is use as the central file format. use the odd format we are abl to specifi exactli which part of tei-xml we want to use. We can then use the exist tei infrastructur to creat document and schemata for our custom tei format. for small simpl chang creat an odd custom is simpl and feasible: there are some nice tutori and introductori materi and ad or remov some element or modul is easili be done by ad a few line into an odd file. however, becaus of the subsequ workflow steps, the eoa publish infrastructur expect tei document to have a veri specif structure. use odd as a central format for describ and enforc thi structur seem to reveal certain shortcomings: the semant of odd are sometim slightli vagu and hard to figur out from the document (how do I know what my odd defines?) some tool in the tei infrastructur are slightli incomplet or contain bug in odd, exactli one content model for everi xml element can be given. tei provid some gener purpos element (e.g. div or p) which can appear in mani differ contexts. In mani case we want to restrict their content in differ way depend on their context, e.g. their posit in the xml tree or the valu of their type attribut (e.g. div[@typ = 'chapter'], div[@typ = 'section'], …). the onli way to achiev thi with the current state of odd is to add schematron rules, which are cumbersom and repetit to write manually. In our paper, we give a roadmap of how to custom tei with odd consid the specif of edit open access. We will present a few stategi and tool that address some of the problem mention above, e.g. an experiment script to automat gener an odd includ complex schematron rule from a relax NG schema. We hope to initi a discuss about possibl improv of the odd format and the tei infrastructure. our roadmap includ the follow waypoints: creat a strict relax NG compact schema that cover all the textual phenomena of a scholarli public in thi domain. learning, understand and use odd (tei modules, the class system, complex restrict with schematron). discov the problem and limit of odd. find a solut for eoa that involv the develop of tool to gener an odd file base on the manual creat rnc schema in step 1. exampl file and script are avail at . discussion: possibl improv on odd and tei.\", 'In search of comity: tei for distant read introduct comiti is a term from theolog or polit studies, where it is use to describ the formal recognit by differ religions, nation states, or cultur that other such entiti have as much right to exist as themselves. In appli linguistics, the term ha also been use by such writer as widdowson or aston [aston 1988] seek to demonstr how the establish of comiti can facilit success inter-cultur communication, even in the absenc of linguist compet those particip in convers encount have to have a care for the preserv of good relat by promot the other\\' posit self-image, by avoid offence, encourag comity, and so on. the negoti of mean is also a negoti of social relations. widdowson, 1990, p. 110 . We appropri the term here in thi latter sense, as a mean of re-assert the inter-disciplinari root of the tei. recent histori of the tei (e.g. gavin, 2019) have a tendenc to under-emphas the multipl of disciplin gather at it birth, prefer to focu on those disciplin which can be plausibl frame as prefigur our current configur of the digit human in some way. yet the poughkeepsi confer and the process of design the guidelin which follow alik were kickstart by input from corpu linguist and comput scientist as well as tradit philologically-mind editor and source-driven historians. the tei belong to a multipl of research communities, date as it doe from a period when comput linguist and tradit philologist alik were begin to wake up to the implic of the advent of massiv amount of digit text for their disciplines. the steer committe which oversaw it develop and the tei editor alik conscienti attempt to ensur that the guidelin should reflect a view of text which wa gener share and generic, rather than specif to ani disciplin or to ani particular usag model. the tei\\' radic proposit that there wa such a thing as a singl abstract model of textual components, which might use be consid independ of it express in a particular sourc or output, or it use in ani particular discipline, wa necessarili at odd with at least two prevail orthodoxies: on the one hand, the view that a text is no less and no more than the physic document which instanti it, and can be adequ describ and repres by it salient visual properti alone; on the other hand, the view that a text is sole a linguist phenomenon, compris a bag of words, the statist properti of which are adequ to describ it. but the tei tri veri hard to prefer comiti over conflict, not onli in it organization, which brought togeth an extraordinarili heterogen group of experts, but also in it chief outputs: a set of encod guidelin which while support special did not requir ani particular specialis to prevail. old orthodoxi do not die easily, and it is interest to hear how some of the same argument are still be play out in the somewhat differ context of today\\' DH theorizers. but in our present paper, we simpli want to explor the extent to which the tei\\' model of text can be adapt to conform to the model of text characteris such field as stylometry, stylistics, textual analytics, or (to use the current term) distant reading. We hope also to explor the claim that by so do we may facilit the enrich of that model, and thu facilit more sophist research into textual phenomena across differ corpora. and we hope to demonstr that thi is best done by cultiv mutual respect for the wide differ scientific, cultural, and linguist tradit characteris thi cross-european and cross-disciplinari project, that is, by acknowledg a comiti of method as well as languages. the cost action “distant read for european literari history” the context for thi work is the eu-fund cost action “distant read for european literari history” (ca 16204) a princip deliver of which will be the european literari text collect (eltec). thi is a set of compar corpora for each of at least a dozen european languages, each corpu be a balanc select of 100 novel from the 19th century, togeth with metadata situat them in their context of product and of reception. It is hope that the eltec will becom a reliabl basi for compar work in cross-linguist data-driven textual analytics, eventu provid an access benchmark for a particular written genr of consider cultur import across europ dure the period between 1840 and 1920. two signific decis made earli on in the plan of the cost action underli the work report here. firstly, it wa agre that the eltec should be deliv in a tei-encod format, use a schema develop specif for the project. secondly, the design of that encod scheme, in particular the textual featur it make explicit by mean of markup, should be defin as far as possibl by the need of the distant read research community, rather than ani pre-exist notion of textual ontology, to the extent that the need of that commun could be determined. the target audienc envisag includ expert in comput stylistics, in corpu linguistics, and in tradit literari studi as well as more gener digit humanists, but is probabl best character as have major enthusiasm and expertis in the applic of statist method to literari and linguist analysis, and onli minor interest in the kind of textual featur most tei project have tend to focu on. the work of the action further inform about the action is avail from it websit at https://www.distant-reading.net is carri out in four work group (wgs), whose activ are subject to endors and accept by a manag committee, compos of two nation repres from each of the 29 countri current particip in the action. the work group head are also member of a smaller core group respons for day to day manag of the action. wg1 scholarli resourc is respons for the work describ in thi paper; wg2 method and tool is concern with text analyt techniqu and tools; wg3 literari theori and histori is concern with applic and implic of those method and for literari theori ; wg4 dissemin is respons for outreach and communication. the design and construct of the eltec is the respons of wg1, as note above. initially, thi work wa split into three distinct tasks: first, defin select criteria (corpu design); second, develop basic encod method (both for data and for metadata); and third, defin a suitabl workflow for prepar of the corpus. work paper on each of these topic plu a fourth on theoret issu of sampl and balanc were prepar for discuss and approv by the 30 member of wg1, and remain avail from the work group\\' website. their propos were ratifi by the manag committe after discuss at two meet dure 2018. the eltec encod scheme/ the encod requir for eltec were perceiv by wg1 to be somewhat differ from those of mani other tei projects. distant read method cover a wide rang of comput approach to literari text analysis, such as authorship attribution, topic modelling, charact network analysis, or stylist analysi but they are rare concern with editori matter such as textual variation, the establish of an authorit text, or product of print or onlin version of a text. consequently, the eltec encod scheme wa deliber not intend to repres sourc document in all their origin complex of structur or appearance, but rather to make it as simpl as possibl to access the word of which text are compos in an inform and predict way. the goal wa neither to duplic the work of scholarli editor nor to produc (yet another) digit edit of a specif sourc document. rather, the encod scheme wa design in such a way as to ensur that eltec text could be process by simpl mind (but xml-aware) system primarili concern with lexi and to make life easier for the develop of such systems. An import principl follow from thi latter goal is that eltec markup should offer the encod veri littl choice, and the softwar develop veri few surprises: the number of tag avail is greatli reduced, and their applic is tightli constrained. It facilit process greatli if access to each part of the xml tree can be provid in a uniform and consist way across multipl eltec corpora. By default, the tei provid a veri rich vocabulary, and mani subtli differ way of do more or less the same thing. tei encod have frequent taken full advantag of that to produc text which vari enormously, both in the subset of xml tag use and in the rang of attribut valu associ with them. It is tempting, but entir mistaken, to assum that the allegedli tei-conform deliver from project A will necessarili be mark up in the same way as the allegedli tei-conform deliver from project B A large-scal project call monk (metadata offer new knowledge) demonstr some of the technic consequ of thi for integr search of tei resources: see further http://monk.library.illinois.edu . On the contrary, all that tei conform realli guarante is that the intend semant of the markup use by the two project should be recover by refer to a publish standard, and are not entir ad hoc or sui generis. (thi may not seem much of an advance, though it is: see further [burnard 2019]). follow thi No surpris principle, the simplest eltec schema (the level zero schema) provid the bare minimum of tag need to mark up the typic structur and content of a nineteenth centuri novel. all preliminari matter other than the title-pag and ani authori prefac or introduct is discarded; the remaind is mark as a div of type titlepag or liminal, within a front element. within the bodi of a text, the div element is also use to make explicit it structur organization, with type attribut valu part, chapter, or letter onlyan except is made for epistolari novel which contain onli the represent of a sequenc of letters, with no other signific content: these may be mark as div type=\"letter\" . for our purposes, a chapter is consid to be the smallest subsect of a novel within which paragraph of text appear directly. further subdivis within a chapter (often indic convent by ellipses, dashes, star etc.) are mark use the mileston element; larger group of div element are indic by div elements, alway of type part, whatev their hierarch level. headings, at whatev level, are alway mark use the head element when appear at the start of a div, and the trailer element when appear at the end. within the div element, onli a veri limit number of element is permitted: specifically, in addit to those alreadi mentioned, p or l (vers line). within these element we find either plain text, hi (highlighted), pb (page break) or mileston elements. after some debate, the action\\' manag committe agre that it would be practic to requir onli thi tini subset of the tei for all eltec texts. It should be note that the text includ in an eltec corpu may come from differ kind of source. for some languag collections, no digit text of ani kind exist: the encod must start from page images, manual transcrib or put them through ocr, and introduc eltec markup from scratch. for others, exist digit text may alreadi be available: the encod must research the format use and find a way of convert it to eltec. In some cases, a tei version may alreadi exist; in other a project gutenberg html version; in yet other the text may be store in a databas of some kind. whichev is the case, if it is possibl to retain distinct which the eltec scheme permits, thi is clearli desirable; perhap less obviously, it is also necessari to remov distinct made by the origin format which the eltec scheme doe not permit. thi divers of sourc materi wa one motiv for permit multipl encod level in the eltec scheme: at level zero, onli a bare minimum of markup is requir or permitted, while at level 1 a slightli richer (but still minimalist) encod is also defined. At level 2, further tag again are introduc to support linguist process of variou kinds, as discuss further below. down-convers from a higher to a lower level is alway automat possible, but up-convers from a lower to a higher level gener requir human intervent or addit processing. At level 1, the follow addit distinct may be made in an encoding: the label element may be use for heading-lik titl appear in the middl of a division; the quot element may be use to distinguish passag such as quotations, epigraphs, stretch of verse, letter etc. which seem to float within the run text; the corr element may be use to indic a passag (typic a word or phrase) which is clearli erron in the origin and which ha been editori corrected; the element foreign, emph, or titl are avail and should be use in prefer to hi for passag render in a differ font or otherwis made visual salient in the source, where an encod can do so with confidence; the element gap may be use to indic where some compon of a sourc (typic an illustration) ha been left out of the encoding; the element note and ref may be use to captur the locat and content of authori suppli footnot or end-notes; wherev they occur in the source, note must be collect togeth in a div type=\"notes\" within a back element. for those alreadi familiar with the tei, thi list of element may seem distressingli small. It lack entir some element which everi tei introductori cours regard as indispens (no list or item; no choic or abbr; no name or date...) and toler some practic border on tag abuse. for example, all the compon of a titl page are mark as p sinc no specialis element (titlepage, docimprint etc.) are available. In the absenc of specialis but culture-specif featur (for example, publish name, imprint, imprimatur, etc.) the encod identifi onli fundament textual featur common to everi kind of text. nevertheless, we believ that the set of concept it support overlap well with the set of textual featur which almost ani exist digit transcript will seek to preserv in some form or another. thi may explain both whi the major of the text so far collect in the eltec have been encod at level 1 rather than level0, and also the speed with which the collect is growing. eltec level 1 is intend to facilit a richer and better-inform distant read of a text than a transcript of it verbal content alon would permit. eltec level 2 is partli intend to provid a consist and tei-conform way of repres the result of such readings, in partiocular those concern with linguist annotation. it primari goal is to repres in a standard way addit layer of annot of particular import to distant read applic such as stylometri or topic modelling. enrich of each lexic token to indic it morpho-syntact categori (pos) or it lemma, and identif of token which refer to name entiti are both well within the scope of exist text process techniques, and are also routin use in distant read applications. the challeng is that the input and the output format typic use by such tool are rare xml-based, and seem superfici to have a model of text quit differ from that of the order hierarchi of content object in term of which the tei commun tradit operates. for mani in the distant read commun (it seems) a text is littl more than a sequenc of tokens, mostli correspond with orthographically-defin words, though there is some variabl in the principl underli the process of tokenisation, for exampl in the model of clitics, compound forms, etc. each token ha a number of properties, which might includ such attribut as it part of speech, it lemma, or it posit in the sequenc of token make up the document. inform about a token which in an xml model would be properti of some higher level construct such as it statu as dialogue, quot matter, emphasis, etc. is occasion consid as well, but is typic model as an addit properti of the token. If a commun is defin by it tools, it would appear therefor that the distant read commun ha not fulli embrac the notion of xml as anyth other than a rather verbos archiv format. however, commun are not defin sole by their tool : by seek a way of reconcil these differ view of what text realli is in a spirit of comiti we hope to demonstr that there are advantag both for the distant reader or stylometrician and for the literari analyst or textual editor. At eltec level2, all exist element are retain and two new element s and w are introduc to support segment of run text into sentence-lik and word-lik sequenc respectively. individu token are mark use the w element, and decor with one or more of the tei-defin linguist attribut pos, lemma, and join. both word and punctuat mark are consid to be token in thi sense, although the tei suggest distinguish the two case use w and pc respectively. the s (segment) element is use to provid an end-to-end tessel segment of the whole sequenc of w elements, base on orthograph form. thi provid a conveni extens of the exist text-body-div hierarchi within which token are located. the element p, head, and l (which contain just text at level 0 and 1) at level 2 can contain a sequenc of s elements. empti element gap, milestone, pb or ref are also permit within text content at ani point, but these are disregard when segment is carri out. each s element can contain a sequenc of w elements, either directly, or wrap in one of the sub-paragraph element corr, emph, foreign, hi, label, title. To thi list we might add the element rs (refer string), provid by the tei for the encod of ani form of entiti name, such as a name entiti recognit procedur might produce. thi approach impli that w element may appear at two level in the hierarchi which may upset some software; it also impli that w element must be properli contain within one of these elements, without overlap. If either issu prove to be a major stumbl block, an altern would be to remov the tag demarc these sub-paragraph elements, indic their semant instead by addit attribut valu on the w element they contain. thi tei xml format is equal applic to the product of train data for applic use machin learn techniqu and to the output of such systems. however, sinc such machin learn applic typic oper on text content in a tabular format only, xslt filter which transform (or generate) the xml markup discuss here from such tabular format without loss of inform are envisaged. At the time of writing, however, work group 2 ha yet to put thi propos architectur to the test. eltec metadata and corpu design like everi other tei document, everi eltec text ha a tei header, though it organ and content alik are constrain much more tightli than is common tei praxis, for the reason alreadi mentioned. the structur of an eltec header is the same no matter what level of encod appli to the text. It provid minim bibliograph inform about the encod text and it source, suffici to identifi the text and it author, in a fix and consist format. It is assum that if more detail bibliograph inform is required, for exampl about the author or work encoded, thi is better obtain from standard author files; to that end a viaf code may be associ with them. As note above, eltec text may be deriv from mani sources, each of which should be document correctli in the header\\' sourcedesc element. after some debate, a common set of practic ha been identifi to distinguish (for example) eltec text deriv directli from a print sourc from those deriv from a digit source, itself deriv from a known print source, and to provid inform about each source. In the follow example, the sourc of the eltec version is a pre-exist digit edit provid by project gutenberg , but the sourc descript also provid inform about the first print edit of the work concerned. project gutenberg ebook A engomadeira de almada negreiro A engomadeira josé de almada negreiro typographia monteiro & cardoso 1917 In most cases, the eltec text will correspond with the first edit of a work in book form; but even where thi is not the case, or where inform about the precis sourc use is not available, minim inform about that first edit should also be provid in order to place the work in it origin tempor context. As with other tei conform documents, besid the mandatori file description, the tei header of everi eltec text contain a public statement which specifi it licens condit (all eltec document are licens cc-by); an encod statement specifi the level of encod used; and a revis descript contain version information. the tei header is also use to provid metadata describ the associ text in a standard form; thi is held in the profiledesc element which must specifi the languag use by the text, may option includ a textclass element contain ani culture-specif keyword consid use to describ the text, and must contain a textdesc element which document the text\\' statu with respect to select criteria discuss below. one of the knottier problem or (to be positive) more distinct featur of an eltec languag collect is that it is not intend to be an ad hoc accident construct corpu but a design one. it composit is determin not by the happenst of whatev we can get our hand on, but is instead defensible, at least in theory, as a principl and repres selection. the big question is, of course, repres of what. It would be nice to say that it repres the product of novel in a specif languag in 19th centuri europe. wg1 ha work definit for both \"novels\" and \"europe\" which we do not discuss further here, though both are clearli problemat terms. It is hope that the eltec will provid data for an empir discuss of such terms, feed into the work of wg3 on literari theori and terminology. but we cannot make that claim without ani data about the popul we are claim to repres -- which is hard to come by for mani of the languag concerned. We know about the novel which we know about, which tend to be the one that nation librari or equival cultur heritag institut have chosen to preserve, which publish over time have been abl to sell, and which lectur in literari studi have chosen to teach. more ephemer titl may have been collect (for exampl by a copyright library); but equal well may have been discard or even suppress as unworthi of inclus in the nation patrimony. titl and author alik can go in and out of fashion. but how can we express opinion about chang in the natur of the publish novel if the sampl on which we base those opinion is wildli differ in composit from the actual population? If our data lead us to assert that novel in a given languag are never written by women, or are never of fewer than 100,000 word is thi simpli becaus no femal author happen to have been preserved, or becaus short novel were routin discard from the collection? or, on the other hand, doe thi actual indic someth fundamental, a characterist of the popul we are investigating? thi matter particularli for eltec, one of the goal of which is precis to facilit cross-languag comparisons. thi problem of repres is of cours one which everi corpu linguist ha to face, and discuss of it implic are easi to find in the literatur some notabl exampl includ biber, 1993; lüdeling, 2011; bode, 2018 our approach is to sidestep the imposs of repres an unknown (and sometim unknowable) popul by attempt instead to repres the rang of possibl variat in the valu of a predefin set of variables, each correspond with a more or less object categori of inform avail for all member of the population. To take a trivial example, everi novel can be characteris as short, medium, or long; there is no possibl fourth valu for thi categori unless we revis our definit of length (elastic? unknown? instantaneous?). so, as a work hypothesis, we might say that a corpu in which roughli a third of the titl are short, a third are long, and a third are medium will repres the variat possibl for thi category. If we appli thi principl uniformli across all our corpora, we can reliabl investig (for example) cross languag variat in some other observ phenomenon (say a fond for syntact complex sentences) with respect to length. but note that we have made absolut no claim about whether novel length in the underli popul is also divid in thi way. the decad in which a novel first appear in book form is a similarli object characteristic, which in principl we can determin for everi member of the population. We can also classifi everi titl accord to the actual sex of their author (with valu such as female, male, mixed, unknown). and we can likewis classifi a titl in term of it stay power or persist by look at the number of time it ha been reprint sinc it first appearance. We suggest that text which have been frequent reprint over a long period may reason be consid canon in some sens of that vex term. the goal of our corpu balanc exercis is to ensur more or less equal time for each possibl valu for each of these four categori -- size, decade, authorsex, and canonicity. ideally, each corpu should have equival number not just for each value, but for each combin of values: so, for example, look at the third of all titl which are characteris as \"short\", there should be roughli equal number for each decad of first appearance, roughli equal number by male and femal authors, and so on. thi may howev be a council of perfection. It is alreadi appar that for some languages, it is veri difficult to find ani text at all within some time periods, or by femal authors. similarly, our definit of \"short\" (10-50 thousand words), \"medium\" (50-100 thousand words) and \"long\" (over 100 thousand words) though object and easi to validate, assum that there will be enough novel of a given length in the underli popul for us to extract a balanc sample; but in some languag it may be that the distribut of length across the popul is entir different. We cannot tell whether (for example) the absenc of ani \"long\" novel at all in czech, serbian, or norwegian is characterist of those languages, or an artefact of the select process so far. anoth difficulti is that our corpu design deliber seek to includ some forgotten or margin work along with well-known canon texts: thi is rel easi for tradit such as english, french, or german where copyright law have led to the mainten and document of larg nation collections, but less so for other less well document languages. (for some initi data, see the summari page at https://distantreading.github.io/eltec/) To encod these balanc criteria in the tei header in as direct and access a manner as possible, we have chosen to re-purpos the little-us textdesc element, origin provid by the tei as a wrapper for a set of so-cal situat paramet propos by corpu linguist as a way of object character linguist product the textdesc element is discuss in section 15.2.1 of the tei guidelin (https://tei-c.org/release/doc/tei-p5-doc/en/html/cc.html#ccahtd). In our case, we replac the tei\\' suggest vocabulari for these paramet with a vocabulari repres our four criteria, express as new non-tei element in the eltec namespace. these element (eltec:sex, eltec:size, eltec:canonicity, and eltec:timeslot) are requir by the eltec schema and have an attribut key which suppli a code valu for the criterion concern taken from a predefin close list. so, for example, a long (over 100,000 words) novel by a femal author first publish between 1881 and 1900 but onli infrequ reprint thereaft might have a text descript like the following: when complete, thi inform can be use to select subcorpora from the collect as a whole, thu permit more delic cross-linguist comparisons: for exampl between the lexi of male and femal writers, or between the stylist featur typic associ with long or short texts. dure the construct phase, these code valu also make it easi to monitor the emerg composit of the corpus, for exampl to detect whether or not the ratio of male to femal writer is consist across differ time periods, by mean of a simpl visualis like the follow thi mosaic plot for the current state of the english corpu (90 texts) show that there are roughli as mani femal (blue) as male (pink) writer across the board, but that there is a preponder of long text and of titl publish in time slot 3. for comparison, the same plot for the current state of the hungarian corpu (100 texts) show significantli fewer femal writers, and a higher proport of short texts. whether these variat are an artefact of the sampl process or repres differ in the underli popul is precis one of the research question which our approach requir us to address. chain odd the tei\\' odd (one document doe it all) system [rahtz and burnard, 2013] is wide use as a mean of custom the tei and document the custom in a standard way. when onli a singl odd custom is use across a project, there is a natur tendenc to produc broadli permiss schemas, to allow for the inevit variat of requir when materi of differ kind are to be process in an integr collection. but thi prevent the encod from take full advantag of the abil of an xml schema to check that particular document conform to predefin rules, unless they are will greatli to increas the complex of their work flow. A better approach, pioneer by the deutsch textarchiv [haaf and thoma 2016], ha been the use of a techniqu known as odd chain [burnard 2016] here, a project first defin a base odd which select all the tei compon consid to be use anywher and then use thi as the basi for smaller, more constraining, odd which select from the base onli the compon (or other rules) specif to a subset of the project\\' documentari universe. for example, an archiv may have identifi a common set of metadata it wish to document across all of it hold but also have particular metadata requir for print and manuscript sourc respectively. simpli defin two differ odds, one for print and one for manuscript when mani other compon appli to either kind of sourc open the door to redund duplic and the risk of inconsistency. the odd chain approach requir definit of a base odd which contain the union of the compon need for these two differ odds, construct as an appropri select from the full rang of tei components. the odd for print and manuscript are then defin as further specialis or custom of the base, ensur therebi that the common compon are use in a consist manner, but preserv comiti by allow equal statu to the two specialis schemas. In the eltec project, we begin by defin an odd which select from the tei all the compon use by ani eltec schema at ani level. thi odd also contain document and specifi usag constraint applic across everi schema. thi base odd is then process use the tei standard odd2odd stylesheet to produc a standalon set of tei specif which we call eltec-library. three differ odds, eltec-0, eltec-1, and eltec-2 then deriv specif schema and document for each of the three eltec levels, use thi librari of specif as a base rather than the whole of the tei. thi enabl us to custom the tei across the whole project, while at the same time respect three differ view of the result encod standard. As with other odds, we are then abl to produc document and formal schema which reflect exactli the scope of each encod level. the odd sourc and their output are maintain on github and are also be publish on zenodo along with the eltec texts. the github repositori for the eltec collect is found at https://github.com/cost-eltec/ : the zenodo commun within which it is be publish live at: https://zenodo.org/communities/eltec, state of play and futur work the eltec is still veri much a work in progress, and henc we cannot report that our design goal have been achiev with ani plausibility. An initi releas of the collect is due on zenodo in septemb 2019, and we expect sever futur releas befor the target of 100 text per languag is reached. the corpora are also maintain as a collect of publicli visibl github repositories, as note above. As well as continu to expand the collection, and continu to fine-tun it composition, we hope to improv the consist and reliabl of the metadata associ with each text, as far as possibl automatically. for example, we have develop two complementari method of automat count the number of reprint for each title, one by screen scrape from worldcat, and the other by process data from a z39.50 server where thi is available. these method should provid more reliabl data than ha hitherto been avail for the canon criterion mention above. the main area of futur work we anticip is howev in the test of the propos eltec level 2 encod and an evalu of it usefulness. At a technic level, thi may necessit some chang in the exist markup scheme, but of perhap more interest is the extent to which it avail will exemplifi the virtu of strive for comiti amongst the mani way in which tei xml markup can be applied.', 'reconceiv tei model of theatric perform text with refer to promptbook jennif roberts-smith and joey takeda with janel jenstad, mark kaethler, and tobi malon reconceiv tei model of theatric perform text with refer to promptbook thi paper explor and suggest a revis to the conceptu model of “dramat text” current evidenc in tei discourse. our critiqu aris out of the challeng our research team ha encount develop an encod protocol for promptbook in the collect of the canadian stratford shakespear festiv archives. We have argu elsewher that promptbooks—bi which we mean the promptbook defin by charl H. shattuck (1965) as the “book[s] actual use by prompter or stage manag in conduct performances” (1965, 5)—are ontolog distinct from other form of theatric perform text in the sens that “a promptbook record a seri of utter to which intend perform event are map in a rel tempor sequence” (roberts-smith et al. 2019). In thi paper, we work through the implic of that claim for the tei guidelines, and suggest a reconceptu of tei’ discurs model of theatric perform text base on: long-acknowledg limit of the exist guidelin to address the complex ontolog of a “dramat work” (mylona and lavagnino 1995); a summari of more recent theoret work on the ontolog of dramat and theatrical/perform text (includ framework propos by osborn 1996; clopper 2001; ern 2003; schafer 2006; kidni 2009; worthen 2005, 2010, 2011; werstin 2012; roberts-smith et al. 2013; griffin 2018); and a survey of exist tei guidelin for perform text (tei 7), tei listserv discuss (with a focu on the 2001 debat among tobin nellhaus, syd bauman, sperberg-mcqueen, and other note that full thread is not avail via the tei listserv as it appear some of the discuss happen “off-list”. see nellhaus’ initi post https://listserv.brown.edu/cgi-bin/wa?a2=tei-l;efa95b0.0103 and bauman’ reply: https://listserv.brown.edu/cgi-bin/wa?a2=tei-l;2973394d.0104 ), and other tei extensions, includ the music encod initi (mei). our central argument is that the tension between event and text that mylona and lavaigno see as problemat in encod “dramat works” (1995) appli neither to perform text tradit understood as such nor to promptbooks. A more function conceptu in tei would accommod dramatic, theatrical/performance, and prompt text as three distinct ontologies. We offer a short list of sampl modif to the tei guidelin for discussion. refer bauman, syd. (2001 april 12). “re: tei and drama”. tei electron listserv. https://listserv.brown.edu/archives/cgi-bin/wa?a2=tei-l;2973394d.0104 clopper, lawrenc (2001). drama, play, and game: english festiv cultur in the mediev and earli modern period. chicago: univers of chicago press. erne, luka (2003). shakespear as literari dramatist. cambridge: cambridg univers press. griffin, andrew (2018). “text, performance, and multidisciplinarity: On a digit edit of king leir.” In shakespeare’ languag in digit media: old word new tools, ed. janel jenstad, mark kaethler, and jennif roberts-smith. 84-104. abingdon: routledge. kidnie, M. J. (2009). shakespear and the problem of adaptation. abingdon: routledge. lavagnino, john, and elli mylonas. (1995). “the show must Go on: problem of tag perform texts.” comput and the human 29: 113-121. music encod initiative: guidelines. 4.0.1. april 12, 2019. music encod initiative. https://music-encoding.org/guidelines/v4/content/ . osborne, lauri E. (1996). “rethink the perform editions: theatric and textual product of shakespeare.” In shakespeare, theori and performance, ed. jame bulman, 168-86. abingdon: routledge. roberts-smith, jennifer, mark kaethler, tobi malone, liza giffen, janel jenstad, martin holmes, and joseph takeda (2019). “tag time and space: tei and the canadian stratford festiv promptbooks.” digit studies/l champ numériqu 9.1: https://doi.org/10.1017/cbo9781139103978 roberts-smith, jennifer, et al. (2014). “visual theatric text: from watch the script to the simul environ for theatr (set).” digit human quarterli 7.3. schafer, elizabeth (2006). “perform editions, editing, and editors.” shakespear survey 59: 198-212. tei consortium, eds. tei p5: guidelin for electron text encod and interchange. 3.5.0. januari 29, 2019. tei consortium. http://www.tei-c.org/guidelines/p5/. werstine, paul. 2012. earli modern playhous manuscript and the edit of shakespeare. cambridge: cambridg univers press. worthen, W. B. (2005). print, and the poetic of modern drama. cambridge: cambridg univers press. worthen, w.b. (2010). drama: between poetri and performance. chichester: wiley-blackwell. worthen, w.b. (2011). intox rhythms: or, shakespeare, literari drama, and perform (studies). shakespear quarterli 62.3 (fall): 303-339.', 'model frbr entiti and their relationship with tei: A look at hallernet bibliograph descript model frbr entiti and their relationship with tei: A look at hallernet bibliograph descript the aim of thi paper is to discuss the map between frbr (function requir for bibliograph records) and tei carri out to creat bibliograph record and their relationships. although some work ha been done in thi area (hawkins, 2008), on the differ about databas model and markup model (eide, 2015) or xml and link data (ciotti and tomasi, 2016), thi paper will argu that the tei guidelin may be suitabl for creat bibliograph record and their relationships. frbr (ifla, 2008) defin four bibliograph entiti (group 1: work, expression, manifest and item) amongst other entiti such as person, concept, event, place, etc. while the tei allow to encod “structur bibliograph citation[s]” with <biblstruct> and “a loosely-structur bibliograph citation[s]” with <bibl>. for illustr purposes, thi paper will use hallernet portal devot to albrecht von haller (bern, 1708-1777), a key figur of the swiss enlightenment, and hi circl of friend and collaborators. the websit compris both digit edit and a larg collect of 35.600 bibliograph record amongst other type of objects. hallernet ha simplifi and adapt the frbr abstract model to defin onli two bibliograph entiti — due to the current state of the bibliograph record — and their relationships: a work (a distinct intellectu or artist creation) and it manifest (the physic embodiment) use the tei element <bibl> and <biblstruct> respect and four potenti relationship (embodimentof, ispartof, isareviewof and isasuccessorof) encod with <relateditem>. sinc the tei guidelin cover both metadata and data, it vocabulari and syntax can go beyond the represent of text and facilit the creation of bibliograph catalogu that group record into “families” base upon some share characterist – e.g. same content in differ languag or differ edit of the same work. although the abstract model discuss here is a preliminari work that need to be implement in the near future, we expect that it will enabl further brows and discoveri of records. bibliograph refer ciotti, fabio and francesca tomasi, «formal ontologies, link data, and tei semant », journal of the text encod initi [online], issu 9 | septemb 2016 - decemb 2017. url: http://journals.openedition.org/jtei/1480 eide, øyvind, «ontologies, data modeling, and tei», journal of the text encod initi [online], issu 8 | decemb 2014 - decemb 2015. http://jtei.revues.org/1191 hawkins, kevin S, «frbr group 1 entiti and the tei guidelines», 2008 tei annual member meeting, held novemb 6–8 2008, in london, england, unit kingdom. ifla studi group on the function requir for bibliograph records, function requir for bibliograph records: final report. As amend and correct through februari 2008 (http://www.ifla.org/vii/s13/frbr/frbr_2008.pdf).', 'scale up automat structur of manuscript sale catalogu', 'recreat histori through event recreat histori through event event are a featur of life in space and time and henc are frequent encount in wit of the human culture. come from divers background within the tei base realm of scholarli editing, the author identifi a common practic in digit edit for encod events: dateabl event are commonli record and mark up use e.g. simpl date when=\"*\" structures. To date, there is no easi singl solut to model event that includ basic other event features, mainli refer back to the question of what happen when and with whom/what as subject and object of ani given event. there are numer exampl from gener and disciplinarili vari edit where a unifi way of describ event could provid ad value. our showcas exampl featur materi as divers as a) a mediev itinerari and mediev calendars, b) an austrian author’ diari from the 1950s, c) a corpu of government minut from the habsburg empir and d) 16th centuri polit correspondence. We will propos a common strategi on how to encod event in a way that can be easili pars and extract from tei sourc files. after care consideration, we propos minor chang to the event and listev elements. To allow for multipl level of report on events, we will discuss a nest and type way of distinguish between describ and describ events. merg data on histor event from variou sourc will provid for a closer link of text editions. It will also provid data for an enrich histor background by link event across editions. We hope to enabl the tei commun to gener what ha late been success adopt for correspond under the correspsearch label – includ a web servic –, while stay in line with the gener norm and the specif need that aris from other type of sources.', 'highlight our examples: encod xml exampl in pedagog context highlight our examples: encod xml exampl in pedagog context the tei guidelin use the egxml element throughout the prose and refer page for contain xml examples. however, mani tei user know littl about thi element, and most don’t even realis that it is not even in the usual tei namespace, but instead in a tei exampl namespac (http://www.tei-c.org/ns/examples). follow on from my paper at tei2018 (in which I propos more detail way that the tei guidelin might handl exampl more generally), thi paper will look at possibl improv to the egxml element, specif design for modern pedagog uses. when creat tei odd customis as local encod manuals, user sometim use egxml to show how encod should mark up particular textual phenomena, similar to the use in the guidelin themselves. expand thi element’ function could benefit not onli the tei guidelines, but also all those who includ snippet of xml markup in encod manuals, slides, tutorials, exercises, or anyth els possibl deriv from (or export to) a tei sourc and beyond. build on the kind of syntax highlight we are familiar with in xml editor and code snippet online, thi paper examin the need to highlight arbitrari portion of xml store in an egxml element. whether encod exist resourc contain highlight of xml or want to render modern born-digit pedagog materials, the tei guidelin current recommend no specif way to do this. thi paper look at a number of possibl option for enabl the highlight of egxml markup, includ embed namespac elements, out-of-lin markup, and byte-offset coding. all of these are summarised, with the problem that they each face, not onli in processing, but also in provid flexibl method to enabl user to express exist or desir output rendition.', 'grow collect of tei texts: some lesson from sarit grow collect of tei texts: some lesson from sarit patrick mcallist (patrick.mcallister@oeaw.ac.at) there is no certainti as to the size that a corpu of indic text would have, but it is certain that thi is due, mainly, to it sheer extent, rather than to other factor like the frequent reappear of lost work or of import new wit to exist ones. ani fanci about the constitut of a corpu of indic texts, howev narrowli defined, is quickli sober by the recognit that such a project lie far beyond the capac of a singl gener of scholars. nevertheless, some attempt toward it realiz do exist, as do veri differ idea of how such a grow collect should be design and how it could be maintained. the problem that all such attempt must overcom are not onli technical, but often practical. for example, the common expect that text in a collect should be consist at least in their formal characterist can easili conflict with the highli special and sometim chang interest of the scholar produc an edit of a work, or also with the fact that individu section of a singl work are often edit by differ scholar work with vari methods. thi talk will reflect on the choic made in sarit (http://sarit.indology.info), which attempt to provid an environ for an expand collect of indic texts. the propos solut was, and with some modif still is, to design a repositori that, over the veri long term, can increas and deepen in such a way that new contribut either improv the exist material, or expand the collect with new materi without disrupt the gener integr and basic standard of such a collection.', 'how we tripl our encod speed in the introduct the digit victorian period poetri (dvpp) project is a sshrc-fund digit human project base at the univers of victoria. with the guidanc of princip investig dr. alison chapman, the dvpp team is creat a digit index of british period poetri from the long nineteenth century. In addit to uncov period poems, write descript metadata, and compil prosopograph research, we are current use tei and css to encod a statistically-repres sampl of index poems, look for quantit evid of literari chang over time. such an endeavour requir a larg and robust dataset cover a rang of period throughout the period. At the time of writing, there are over 13,000 poem in the database, and we expect that total to reach 20,000. Of these, around 2,000 will be encoded, focus on the decad year (1820, 1830, 1840 and so on). when we initi receiv fund for thi project, we were quit confid that we would be abl to accomplish the collect of page-imag and metadata across the period, becaus we had been do that work for some year and had a clear idea of the time and resourc required. however, a rel small number of poem had been encoded, and the encod time requir had prove to be significant. our first pass of encod goe beyond simpl transcript and tag of line and line-group to includ rhyme-schem and rhyme types, refrain and similar devic such as anaphora and epistrophe, and detail descript of typograph style and layout use css. these are all featur that will form part of the analyt work to be done when the encod is complete. encod speed improv (yellow line with triangl markers). initi project from our project diagnost suggest that we would have some difficulti in accomplish the tag task within the time available, so begin toward the end of 2018, the encod team (fralick, fukushima, holmes, and karlson) began a concert effort to develop tool and techniqu to make our encod faster and more effective. the result can be seen in the graph in , taken from our project diagnostics. In thi presentation, we will describ and demonstr some of the techniqu we deploy to make thi improv possible. We hope these examples, illustrations, and suggest will be use to other project endeavour to make their encod process more accur and efficient. automat header and ocr first of all, we elimin all the work of set up the xml file, complet the core metadata, and do the transcription. our larg databas index of poetri includ metadata on all the poems, along with link to the page-imag in which they appear. A build process is abl to export the sql databas as xml, process it into tei files, and initi an ocr process on each file, store the result in the form of a comment in the bodi of the tei file. thi remov the bulk of the transcript work, and reliev the encod of ani respons for the metadata. the qualiti of the ocr is variable, but in almost all cases, proof and correct it is much faster than transcrib manual ( and ). the tei header gener automat from the database. the result of ocr, store as a comment in the xml. auto-tag of an entir poem after correct the result of the ocr against the origin page-image, the next task for the encod is to tag the structur of the poem. take advantag of the support for schematron quickfix in the oxygen xml editor, we are abl to complet autom the tag of line and line-groups; the encod simpli ha to copy/past the text version of the poem into a div, right-click, and choos a quick-fix, and the poem is automat tag with lg and l element ( and ). A schematron quickfix for auto-tag an entir poem. the result of appli the schematron quickfix. keystrok shortcut with special sauc among the more straightforward trick are obvious keystrok shortcut for insert special charact and xml tags. In addit to the basic (alt + quot to insert a curli apostrophe, alt + M to insert an em dash) we have also been abl to take advantag of the fact that oxygen allow the use of xpath in keystrok shortcut to provid a use time-sav when tag rhyme. control + alt + 0 will wrap a rhyme tag around select text, but it doe more than that; it use xpath regular expressionsse . the xpath express is rel crude, but it gener correct result almost all the time. to analyz the content of the tag to determin whether the rhyme be tag is most like masculin (one syllable) or feminin (two or more syllables), and appli the appropri attribut valu to the rhyme element. although there are of cours other varieti of rhyme, in thi collect masculin rhyme constitut 84% of the total and feminin rhyme more than 9%. thi littl trick therefor provid an accur rhyme tag more than 90% of the time ( and ). the encod select text, hit the shortcut and provid a label attribute. the shortcut automat detect the rhyme type as feminine. ${selection}${caret} An oxygen code templat for tag rhyme. rhyme-label tool the rhyme-label protocol in our project are a littl unusual. first, we use both the rhyme attribut on lg to specifi the rhyme pattern for a stanza, and also the label attribut on individu rhyme element to re-iter those labels. thi duplic ha two functions: first, it is easier to analyz the rhyme-schem of a stanza while it encod is rel uncluttered, so we do thi befor ad the rhyme element to individu lines. secondly, the duplic act as a check on the accuraci of the line-level labelling. If an encod assign rhyme=cbcb to the stanza, but then inadvert give the third line label=b, our diagnost render (of which more below) will catch thi and report the error (). diagnost report catch rhyme label inconsistency. the second respect in which our rhyme-label differ from mani convent approach is that we do not restart our rhyme label from a with everi new stanza, as is exemplifi for exampl in the tei guidelin chapter on vers (tei 2019). instead, we maintain a consist use of the same label for the same sound throughout a poem, no matter how long it is. If love is label as a in stanza one, then dove will be label a in stanza 57. the reason for thi is that we are interest in the effect of rhyme pattern across stanza as well as within them. It is certainli arguabl that there is no plausibl effect on the reader of the repetit of a sound dozen of stanza apart, but it is actual more difficult to specifi an arbitrari distanc beyond which rhyme recurr is not relevant, and use such a determin in our encoding, than it is to follow a singl sequenc throughout the poem.for longer poems, when the 26 letter of the alphabet are exhausted, we restart with a1, b1, c1 etc., follow by a2, b2, c2 as necessary. however, thi can make rhyme-label veri difficult in a long poem; it is hard to remember, when encount night at line 300, that sight and flight were actual rhyme 200 line earlier and have the label b. therefor we have devis a tool base on an xslt transform that assist in thi process. the encod specifi the rhyme they are about to tag (say night), and the follow process is run: everi instanc of the word night tag as a rhyme throughout the entir collect is collected. thi might collect (for example) 40 instanc of night. for each of those instances, all the rhyme tag in the same poem with the same label as night are retrieved. thi might collect 250 word rhyme with night. for each of those rhymes, all instanc of those form tag as rhyme throughout the collect are retrieved. thi might garner 300 word plausibl rhyme with night. for each of those rhymes, everi form tag in the same poem with the same label is retrieved. thi might yield a total of (say) 500 form which probabl rhyme with night. Of these, there might be 300 distinct form (becaus of cours there will be mani duplicates). finally, each of the distinct valu in the list is check against previously-tag rhyme in the poem the encod is work on. If ani match, a report is gener as shown in . potenti rhyme for night found IN poem 478600 night (label = b) a | right (label = b) night (label = b;) blight (label = b) flight (label = b;) report of previou line in the same poem which are potenti rhyme for the select text. the encod is thu abl to see that earlier in their poem, night, aright, night, blight and flight have all been tag with label=b, so that is the appropri label to use. sinc our current collect of tag rhyme is now well over 30,000, thi tool realli facilit the encod of longer poems. sometimes, however, a previou instanc of a rhyme may be missed, and instead of be given label=b, a rhyme may be assign a new label by mistake. when thi is noticed, anoth transform can be use to fix the problem. the user run the transformation, and suppli the label which wa erron assigned, along with the correct label; the transform then re-label the entir poem appropriately. css style with rendition/@selector throughout the nineteenth century, period publish poetri in a wide varieti of format and for mani differ reasons. sometim poem were mere filler, insert to avoid whitespac at the end of a prose article; in other cases, they form part of a prose articl on the theme of poetry; sometim they serv as embellish for illustrations; and sometim they are appear as standalon work on otherwis empti pages. our research are interest in connect between form and function, and the relationship of poem to the surround material, so it is essenti that we captur key aspect of the appear of poems, includ alignment, font styles/vari and size, margins, and indents, and in our html render of the poem (see below) we tri to reproduc thi as close as possible. In our earli encoding, we captur these featur use inlin @style attributes, but the effect of thi is to clutter the poem text with complex property-valu pair which make the rest of the encod harder to work with. As a result, we have switch to use rendit element in the header, and in particular to the rel rarely-us selector attribute, which enabl us to describ the layout of a poem extrem efficiently, especi if it follow a regular pattern. for instance: margin: 0.5em auto 0.5em 6em; margin-left: 1em; describ the layout of the exampl poem use in thi articl fully, posit the line-group within the page, and indent the second and fourth line of everi stanza. where initi encod were spend signific time insert style attribut in multipl locations, they are now skill at abstract the layout of a poem into a coupl of rendit element in a few seconds. instant feedback and error report time diagnost check of encod is extrem import for ani seriou encod project (see holm and takeda 2019). In thi project, we have found signific benefit in provid a detail render and report () for encod to use as they work, access through a transform scenario avail from a singl button click in the oxygen editor. render poem view with feedback report. thi view allow the encod to see at a glanc the layout of the poem (a they have describ it in css), check the rhyme (through colour code and report on inconsist found, as shown in ), as well as provid addit hint for featur not yet encoded, as describ in the next section. auto-detect of anaphora, epistroph and other refrain-lik devic In addit to rhyme, we are encod other featur that involv repetit form and structures, such as full-scal refrains, anaphora (repeat a word or word at the begin of a line), and epistroph (repetit at the end of a line). It is easi for an encod to miss such featur in the midst of encod rhyme and layout, so the render process includ a compon which use a simpl similar metric to identifi potenti instanc of these features. show the output of thi function as appli to the exampl poem. potenti untag refrain devic identifi by the diagnost rendering. here, line 1 and 9 are similar enough to be tag (awak to the sound of my sigh / wake then to the sound of my sighing), as are line 2 and 10 (breeze, gentl breez of the night / breeze! quiet breez of the night). similarly, the phrase breez of the night occur again in line 14. the autom process enabl us to ensur that we catch such echo even when they are distant from each other in a long poem, and it also give us a focus, through the (albeit crude) score calculation, to discuss and compar what might or might not constitut a real instanc of a refrain device. conclus through a determin focu on develop tool and techniqu to improv accuraci and efficiency, we have been abl to make a signific improv in our encod performance. thi ha been achiev through continu dialog between the encod team and the programmer. We encourag encod to express their difficulti and concerns, identifi awkward and time-consum part of the process, and suggest imagin solutions. We have also been abl to leverag the grow dataset of encod alreadi complet to help with rhyme encod in particular. other signific factor includ the instant feedback avail as a diagnost render of the poem while encoding; a project-level diagnost process which continu measur and report on our progress through the dataset; and the fact that the programm is also requir to do signific amount of encod himself, so he encount first-hand the problem the team is facing, and the frustrat and inadequaci of the solut provid so far.', \"A tei custom for the descript of paper and watermark summari tei offer a larg set of tool to describ materi featur of written documents. however, these tool are not yet suffici to produc comprehensive, structur descript of paper, and do not reflect the standard of paper historians. the present contribut consist of a tei-p5 custom for the descript of paper and watermarks. thi custom is design to let user choos the level of detail they wish to achiev in such descript while maintain standard terminology, criteria for data recording, and structure. scope and purpos be abl to record standard paper and watermark data in tei would not onli be use to project that focu on the studi of paper. It would also be a signific asset for project that use tei to describ paper document in general. indeed, watermark can be use to date manuscript and print book (see for instanc stevenson, 1967). moreover, watermark motiv are indic of paper quality, and can thu be use to assess the financi invest repres by the product of a given manuscript or print book (see for instanc busonero et al., 2001). finally, they bear wit to the geograph origin of paper, and can thu inform where document were made or the commerci rout through which the paper use to produc them wa obtain (see for instanc churchill, 1935). therefore, they are key to determin the historical, geograph and socio-econom context in which document were produced. includ thi inform in tei descript of primari sourc would facilit collabor between specialist - for instanc manuscript scholars, book historian and paper scholar - and thu benefit our knowledg of the origin and product context of paper manuscript and earli print books. the present tei custom is model on the intern standard for the descript of paper, watermark and paper mold in relat databases, iphn 2.1.1 (iph, 2013). the purpos of iphn is to standard the record of data concern histor and modern papers, with or without watermarks, in order to ensur the compat of the differ digit databas of paper and watermarks. It offer a tagset and a databas structur for the registr of all the featur of paper and watermark that contribut to their identification, and a standard terminolog for watermark motives. At the present stage, iphn is the onli exist standard for paper and watermark registr in digit databases, and it is not yet wide applied. most digit databas of watermark use differ digit methods, differ criteria for the registr of data, and differ terminology. for these reasons, their interoper is limited, despit the effort to have been made in thi direct (see especi bernstein project, 2019). adapt the iphn standard to tei-p5 significantli contribut in fix thi problem in three ways: - first of all, sinc tei is use in a major of digit catalogu cover paper documents, it allow a larger number of profession - scholar of manuscript and earli print texts, archivists, librarian etc. - to record paper and watermark data. - for the same reason, it make paper and watermark data more access and allow scholar to better local paper type and watermark in collect of paper documents. - finally, sinc tei is design to be highli stabl and interoperable, use it for paper and watermark data ensur that thi data is record and encod in a standard manner. structur and content of the custom the present custom combin new element base on the paramet list in the iphn standard with the offici tei modul msdescription, which is suitabl for the physic descript of all text-bear objects, namesdates, which provid the necessari element for the descript of persons, place and organizations, and can therefor be use to enter inform about paper maker and paper mills, and linking, in order to link the inform together. It consist of two modules: - one modul for the descript of paper and watermark themselves. - one modul for the descript of paper mold and the registr of inform concern paper mill and paper makers. the first custom modul allow user to nest descript of one or more paper type and/or watermark in the support and/or watermark element of the msdescript module. these descript contain the follow information: - the locu of a given paper type within the quir structur of the primari sourc that is described. - the physic featur of thi paper type (aspect, dimensions, state of conservation) and, when one can assess it, it composit and mode of production. - the number of chain line (i.e. the vertic line that are imprint into the paper by the mold) and the distanc between them. - the densiti of laid line (i.e. the horizont line that are imprint into the paper by the mold). - the posit of the watermark, in relat to the paper sheet and to the chain lines. - the watermark motive, use the typolog defin in iphn 2.1.1. the second custom modul allow user to enter descript inform about a paper mold (i.e. chain lines, laid line and watermark posit and motive), geograph and histor inform about the paper mill that use it, and prosopograph inform about the paper maker. It combin the customization' new element for paper and watermark descript and the element provid in the namesd modul for the record of histor and biograph information. the descript of paper and watermark within primari sourc and the descript of paper mold are meant to be regist in differ files. both custom modul includ the offici link modul so that user can link them together. thi entail that file contain paper mold descript are store in a separ database. however, sinc identifi paper mold can onli be achiev by record paper and watermark data in a veri larg number of manuscript or print books, it cannot be done for small corpora or collections. therefore, make paper mold descript file is optional, and the custom can simpli be use to includ paper and watermark data in the descript of primari sourc without necessarili link them together. cost and time invest the custom is design to give project that wish to use it maximum freedom in term of the time and money they wish to invest in the record of paper and watermark data. for thi purpose, it ha the two follow features: - the two custom modul are flexibl enough to allow user to record paper and watermark data at the level of detail that they wish to achiev while preserv standard terminolog and structure. the onli new element that are requir are those that repres featur of paper type and watermark that are both absolut necessari to their identif and observ with minim mean (see below). thi allow project to decid how much time they wish to invest in the record of paper and watermark data, and to determin if they need to hire staff for thi purpose. - the custom requir that user indic the method use to collect paper and watermark data, so that scholar who consult the descript are awar of how much inform could be collect with the method used, and how accur thi inform can be. veri cheap and minimalist method can be use to collect such data with satisfi results. nowadays, the most common of them is the use of a fibr optic light sheet to observ chain lines, laid line and watermarks. such a devic can be found in the vast major of conserv workshop of manuscript and rare book collections, which mean that use it doe not entail addit materi cost. the inform collect with thi method is, in mani cases, alreadi suffici for the identif of watermarks. much more precis method current exist - for instance, radiographi - and project that wish to record paper and watermark data may want to invest in them in order to perform more detail and accur observations. thi is, however, entir up to them, and not requir for the use of the present customization. outcom the descript of paper types, watermarks, and paper mold creat with the present custom can be use for variou purposes, depend on how detail they are and for how mani item (paper manuscript and/or print books) they have been made. At a minim level of detail and for a minim number of describ items, they allow user of digit catalogu to local watermark within collect and within collect items, know the basic featur of the paper that contain them, and compar them with the item record in other paper and watermark database, with the object of identifi them. the more item are described, even at a minim level of detail, the more like it is to identifi paper type and watermark by mine the database, and to be abl to produc paper mold descriptions. the more detail the descript are, the more accur the identif of paper type and watermark will be. finally, onc paper type and watermark have been identified, they can be use to date and contextu the item in which they were found. bibliographi bernstein project (2019). bernstein. the memori of paper. [online] http://www.memoryofpaper.eu [access septemb 1, 2019]. busonero, p., munafò, P. f., storace, M. s., ornato, E. (2001). La carta occidental nel tardo medioevo. 2 vol.. rome: ist. central per la patologia del libro. churchill, w.a. (1935). watermark in paper in holland, england, france, etc. in the xvii and xviii centuri and their interconnection. amsterdam: menno hertzberg and co. intern associ of paper historian (2013). intern standard for the registr of paper with or without watermarks. version 2.1.1 [pdf] avail at http://www.paperhistory.org/standard [access septemb 1, 2019]. stevenson, allan h.. (1967). the problem of the missal speciale. london: bibliograph society.\", 'referenc annot as a core concept of the hallernet edit and research platform may 2019 saw the launch of hallernet, a platform revolv around promin actor of the enlighten and natur research in eightteenth centuri switzerland. hallernet aim to illumin the transform of the earli modern républiqu de lettr into the modern scientif commun and it discipliniari differenti by combin digit sourc edit with a veri rich bodi of prosopograph and bibliograph research data. wherea the onlin platform is brand new in it current shape, the underli metadata wa compil over a span of almost three decades. from the outset, the main focu of the data collect wa on haller’ correspondence, the actor relat to it and bibliograph inform – pertain to haller’ works, hi librari but also a vast amount of secondari literatur of note –, and result in encompass print public such as repertorium zu albrecht von haller korrespondenz 1724–1777 (boschung et al. 2002) and bibliographia halleriana (steink and profo 2004). these endeavor led to a volumin research databas with consider depth (steink 2003), built up between 1991 and 2016 and subsequ transform into tei (cf. recker-hamm and stuber 2015, stuber, daeng and forney 2019). with the onset of a larg project on albrecht von haller’ review and letters, fund by the swiss nation scienc foundat (2018–2023), all ca. nine thousand extant review by haller will be edit in conjunct with some eight thousand themat relat letter (on their relationship cf. stuber 2004). thi undertaking, again, reli heavili on earlier research and more specif on a seri of print edit of haller’ correspondence, which provid the basi for the encod of more than half of the select letters. the propos contribut will discuss the process of the digitis and re-work of such print predecessors. specifically, the fate of the footnot shall be ponder and the chosen solut in the context of hallernet edit presented, both on the level of the tei encod and the present rendering. when develop the data model of these re-edit it quickli show that port exist annot from footnot in print to footnot in the digit edit would not leverag the full potenti of the new environment. instead, as much inform as possibl is attach to refer to databas object (persons, institutions, publications, plant and so on). onli critic (philological) annot and histor inform that cannot be relat to a databas object is retain in footnotes. the implement model for annot refer is straightforward and basic consist of note in referenc strings. In the cours of the re-edition, inform on, e.g., the social posit or the place of activ of an actor as it may be given in a legaci footnot is brought over to the respect databas object, from where it may be queri also from other occurrences. consequently, thi interweav of textual data with extens metadata make it possibl to evalu thi kind of inform not onli for a singl letter, but also for a correspond as a whole and in do so to deriv and compar social profil of specif correspond (sonntag, stuber and forney 2019). A guid principl of the migrat from the (relatively) privat databas to tei wa to allow for more openness. for one, both the transcrib document but also the gist of the databas object will be made avail in public in a fair repository. In addit to this, the data will also be retriev directli from the platform in a programmat manner. To thi end the databas object are relat to author file wherev possibl so that the inform may be share with other project and resources. besid use exist interfac such as correspsearch and integr the data with, e.g., histhub and metagrid, it will be veri interest to provid access to specif bit of the valuabl knowledg contain within the hallernet platform through nascent interfac such as prosopographi.', 'make linkabl data from account books: bookkeep ontolog in the digit edit publish cooper for histor account make linkabl data from account books: bookkeep ontolog in the digit edit publish cooper for histor account christoph pollin and kathryn tomasek tei2019, univers of graz histor account records, a genr of manuscript document that can be encod in tei/xml, can be a rich sourc about local social and econom relationship in the past as well for compar purposes. with the digit edit public cooper for histor account (depcha), documentari editor and develop in the unit state collabor with digit humanist at the zentrum für informationsmodellierung (centr for inform modeling) at the univers of graz to appli a bookkeep ontolog to accounts. the main use case for thi project are the financi paper of georg washington (stertzer), account record from a store on the stagvil plantat in north carolina (brumfield and agbe-davies), and a day book kept by laban morey wheaton, a businessman who kept a dri good store in norton, massachusetts, between 1828 and 1859 (tomasek and bauman). thi paper focus on a workflow for creat linkabl data use the tei @ana attribute. while the wheaton edit wa creat use tei-xml, the data for the washington financi paper and for the stagvil store were creat use drupal and fromthepage, respectively. transform to tei-xml for these record is an interim step for creat the rdf for link on the web of data (pollin 2019). exampl will be drawn from the stagville/fromthepag and wheaton/tei-xml data. upcom work on build a bridg from drupal to tei-xml is part of the project’ ongo research agenda. part of the workflow for creat rdf includ use of wikidata and openrefine. sinc the currency, goods, and servic exchang had contextu mean ground in both place and time, ongo work also focus on map inform from taxonomi base on the digit edit of the primari sourc to histor statist of the unit states, a long-stand project in u.s. econom history. exampl will includ an alpha-vers tei taxonomi drawn from thi source. futur work will involv map taxonomi from the washington financi paper onto thi tei taxonomi and expand it accordingly. refer bicentenni edition: histor statist of the unit states, coloni time to 1970. https://www.census.gov/library/publications/1975/compendia/hist_stats_colonial-1970.html brumfield, ben, and anna agbe-davies. encod account book relat to slaveri in the u.s. south. 2015. https://medea.hypotheses.org/182 depcha prototype: https://gams.uni-graz.at/depcha histor statist of the unit states. milleni edit online. https://hsus.cambridge.org/hsusweb/toc/hsushome.do pollin, christopher. digit edit publish cooper for histor account and the bookkeep ontolog . forthcom 2019. stertzer, jennifer. work with the financi record of georg washington: document vs. data. digit studies/l champ numérique. 2014. doi: http://doi.org/10.16995/dscn.57 tomasek, kathryn, and syd bauman. encod financi record for histor research. journal of the text encod initiative. 6. 2013. https://journals.openedition.org/jtei/895', 'the prefabr website: who need a server anyway? introduct project end , a collabor between digit humanist and librarians, is devis principl (https://raw.githubusercontent.com/projectendings/endings/master/principles.txt) for build DH project in way that ensur that they remain viable, functional, and archiv into the distant future. end principl cover five compon of project design: data product process document releas manag previou end work ha focus on data and product (holm 2017; arneil & holm 2017) and diagnost tool for monitor project progress (holm & takeda 2018 and 2019). thi present will deal with the mechan of processing, focus in particular on build larg static site which are resili becaus they have no requir for server-sid technolog at all. We will use the map of earli modern london project (moeml), one of the flagship end project, as a case study. compris of 2,000 tei sourc file and 15,000 distinct entities, moeml is a dens interlink project that requir a sophist build process to creat it websit structure, the histor aga map interface, edit of primari sourc documents, variou index and gazetteers, and encyclopedia entries. As a flagship end project, moeml ha been a testb for the scalabl of the end principles. the moeml site ha 9,000 html files, 26,000 xml files, and over 5,000 images, and is around 2gb in size. our present will cover a number of key techniqu in the build process, including: validation, validation, validation: xml, html, css, and tei egxml exampl code is valid at everi stage of the build process. diagnost to check all link and targets. uniqu query-fre url for all entiti gener the gazetteer, which includ everi variant spell of everi placename. pre-gener html fragment for ajax retriev for everi entity. process and ration rendit element and style attributes. use document type taxonomi to build sitemap and breadcrumb trails. filter of imag to includ onli those actual used. whi build a static site? when in earli 2019 the server which wa host the tei-c.org websit died, the wordpress-bas main site disappear from the internet for a consider time. sinc wordpress is a database-depend system, a singl central databas host is requir to run it, and until a new server could be brought up host that database, the site remain unavailable. however, there wa no such problem with the tei guidelines, which are statically-built and avail in multipl locat as a matter of course.on of the perspect holm brought to the end project wa hi familiar with the static build process for the tei guidelines, for which we are primarili indebt to the late sebastian rahtz, a wise and clever man who realiz all thi a long time ago. A static site, however, can be replic endlessly. all digit human project will eventu end (kirschenbaum 2009; rockwel et al 2014), and their product will transit into minimally-cur archiv hosting; static site have much more chanc of survival, avail and replic if they have no server dependencies. the world of large-scal softwar develop is also come to similar conclus for slightli differ reasons. the jamstack initi (jamstack.org) is also champion modern web develop architectur base on client-sid javascript, reusabl apis, and prebuilt markup, in the interest of better performance, cheaper, easier scaling, higher security, and a better develop experience. long-term archiv is not a primari goal of jamstack; instead, one of their motiv is that a static site is far more easili deploy across content deliveri network such as akamai becaus it ha no relianc on central back-end data sourc such as a databases. like jamstack, end advoc product base on pure client-sid html5, css, and javascript. the build process moeml’ static build process, which is manag by apach ant, take the densely-encoded, tightly-link xml collect creat by our team and build from it a massive, loosely-coupl collect of web resourc compris everyth we can possibl imagin an end-us might want to see. befor we start, though, we first check whether the current state of the collect is worth build into anyth at all. We valid (relax ng), we valid again (schematron), and we check coher (doe everi link point to someth meaningful?), consist (doe everyth conform to the encod guidelin and the editori guidelines?), and complet (doe everyth mention actual exist?) via our diagnost process (holm & takeda 2019). If a singl file is invalid, or a singl link is broken, or an id is use for two differ entities, the build fail and the process stops. A websit with error is not worth building. It is worth contrast thi rigor suit of valid process with the pre-end form of the moeml website, which wa base on an exist xml databas and to which project staff upload new and chang materi as they finish them (or thought they had finish them), when it occur to them, or (sometimes) accidentally, while upload other materials. articl were publish contain link to other articl not yet written, or person record not yet ad to the personography. one encod would add an item to the bibliographi with a new id, while anoth happen to use the same id for a location; both document would be uploaded, and, at best, link would break and, at worst, the process would fail to handl the error and break the site. such issu were not rampant, but they were omnipresent. We will have no more of that. It should also be note that these sort of error were not onli caus by encoders; develop of the moeml site had to be veri care that ani code commit to the exist server wa error-free. test code chang requir a parallel host environment, which wa an addit burden to maintain, and to keep synchron with the live site. but by check the valid of our output in the static build, we also necessarili ensur that our process work: if everyth in the build is valid, then, at the veri least, the code itself can be compil and it produc valid documents. Of course, thi doe not ensur that the code function precis the way we want it to, but, as we discuss later, the static build process give the project time to ensur that the process work as expected. assum all the valid test pass, the first stage in build the websit is to make more tei xml. lot of it, in fact. We build five differ version of our xml collect (see ). holm (2017b) provid a full descript of the rational behind thi process, but the main justif is that we want to ensur that ani futur user who come to our project look for an xml document can like find one that is tune as close as possibl to their needs. We provid xml design to best repres the praxi of our own project (origin xml), xml design to be less esoter and that align more with standard tei practic (standard xml), xml design to be most amen to gener extern processor (tei lite, tei simpleprint), and xml design to be detach entir from the rest of the collection, free of extern link and depend (standalon xml). thi is how we end up with 26,000 xml files, from a start collect of onli 2,000. As soon as each new version of the xml is created, you may easili guess what we do with it. We valid it. If ani file fail validation, the build stops. thi is also when we creat a wealth of new file that did not exist before, includ the project gazett and a rang of compil index and similar materi whose inform is inher to the origin xml, but which can now be made explicit and tangible. more of thi in the next section. finally, we begin to gener web products. A number of core principl govern the structur and organ of those products: everi entiti (location, person, bibliographi entry, organization, article, etc.) ha a uniqu id, and everi uniqu id get it own individu page on the site. No url is ever abandoned. If an id is chang or an entiti is remov from the collection, a page is still generated, redirect to the new version of the id, or to an explan of what ha happened. link open data requir stabl identifiers, so we have a respons to maintain them indefinitely. everi page stand alon and complet in term of content; everyth referenc in the bodi of the page (people, places, bibliographi item etc.) is includ into the page itself, so that if the page becom detach from it context (if, for example, someon save a local copi to use while disconnect from the internet), it will continu to work. thi of cours mean that there is massiv duplic of data across the site, but we don’t care. the entir site still come in smaller than an HD movie. all page live togeth in the same folder. thi make for a veri larg folder, but it mean that link is trivial and reliable, and url are easili rememb and typed. finally, after the websit content is generated, all it page and associ css file are valid with the w3c’ vnu validator. As always, ani invalid file caus the build to fail. gener of multipl tei output formats. advantages: you can build anyth A major advantag of build the entir site offlin is that we can run process across the entir dataset to build ani resourc we like, no matter how time- or cycle-consum it may be. A simpl exampl is the a-z index, which list all 9,000 xml:id use in the project and provid inform about the entiti to which each id refers. thi is an essenti resourc for moeml encoders, who are often creat new global uniqu xml:id for entiti in the project; have a list of all xml:id not onli prevent duplic ids, but also ensur that encod can check whether or not the person or place that they are creat alreadi exist in the project. just-in-tim creation of the index is not a feasibl option: the server-sid construct of the page, which is nearli 10mb, would be slow, even on a power server. but, by creat thi page ahead of time, the page download and render reason rapidly. We also produc a plain-text list of all the id for faster access and search by encod who may be on a slow internet connection. similarly, it would be impract to gener the gazett of earli modern london, which aggreg and group the thousand of variant placenam across the project, from the sourc data on a live server. befor implement the static build, thi resourc wa manual compil in a semi-autom process. now we creat these document with the rest of the project, which ensur that these document not onli reflect the current state of the data, but are also complet valid html befor they are published. the offlin build also allow us to take advantag of multi-step process that would be veri difficult to manag in a just-in-tim render scenario. We make great use of the tei rendition/selector mechanism, which use css selector syntax to specifi tei element to which rendit descript appli when encod present aspect of the input. In our build process, we use a two step process dure the creation of the standard xml to resolv these css selectors. for each document that ha a //rendition[@selector], we creat a temporari xslt ident transform that convert the css selector into xpath statements, which are then use as the match valu for a sequenc of xsl:templat elements. We run that transform against the sourc document to creat the standard version of the xml, ad rendit that correspond with the predefin rendit in the header. In our standalon process, we then take all style attribut on element and abstract them into rendit pointer to rendit element in the header. then, in our html creation, all of the rendit element are turn into class selector in the header of the html and, accordingly, all rendit attribut are convert into class values. disadvantag the primari disadvantag of thi approach is of cours that it involv defer gratification. build take a long time, and they often fail to complet due to invalid or other errors. It may be hour befor an encod or author can see the result of their work in the context of the built site, and thi is particularli frustrat for those who are encod primari sourc document and tri to captur for reproduct render featur of the origin text. however, patienc is a virtu and cultiv it is no bad thing. instant gratif is not a featur of scholarli discourse; compar with wait for a journal articl to be published, wait a coupl of hour to see the latest draft of your document in all it glori is scarc a hardship. thi virtu also extend to the disciplin around the public releas of complet new version of a site. rather than a roll releas public model, where on ani given day, the state of the site is inconsistent, incoherent, and unpredictable, a static build process demand a fix releas process, akin to the model of edit of a print text; each edit (delimit by project-specif milestones) is clearli label and identified, and alway coherent, consistent, and complete. As we have learn from the tei’ increment releas of the guidelines, thi is a far superior approach, as such releas are much easier to maintain and archiv over the long term. In addition, we do provid shortcut through the build process for local test of individu files. our build can be parameter by suppli one or two specif document id as input, and in that case, the entir build run for onli those document and the result are visibl within a minut or so. conclus We will conclud by summar the intent and principl govern our build process: everyth that can be pre-fabr should be pre-fabricated. everyth that could conceiv be use should be creat and included. redund is beneficial; in fact it is elegant. If the same personographi entri is replic in fifti page that mention that person, then good; ani of those page can now be use outsid the context of the collect without loss. patienc is a virtue: let your build take a long time; let your releas be well-separated.', 'refin the current teach methodolog of the tei through the analysi of server log tei 2019 abstract suggest for refin the current teach methodolog of the tei through the analysi of server log lui meneses, electron textual cultur lab, univers of victoria, ldmm@uvic.ca jonathan martin, king’ colleg london, jonathan.d.martin@kcl.ac.uk lui menes and jonathan martin keywords: server logs, teach methodology, tei guidelin great emphasi ha been place late on the pedagogi and practic of the tei guidelin [1]. In teach and learn the tei guidelin we engag scholar in real research partnerships: allow them to produc digit edit for their own studi and explor differ aspect of their text through differ perspectives. We believ that the next step in the evolut of the tei is develop train materials, which align with greatth emphasi that ha been place late on the pedagogi and practic of the tei guidelin –which wa the main theme of the 2017 confer (“tei victoria 2017” 2019)[1].. the materi for learn the tei guidelin are still in earli stage –consist primarili of past project documentation, the tei’ own introductori materials, onlin tutorials, and collect exampl – which lead to skill be acquir in unsystemat way . additionally, the guidelin have evolv and becom more rigor and theoret –make some of these train materi overwhelm and unpract for newcom who might not be familiar with text encoding. As propon of the tei guidelines, In teach and learn the tei guidelin we engag scholar in real research partnerships: allow them to produc digit edit for their own studi and explor differ aspect of their text through differ perspectives.a the guidelin have matur and becom more rigor and theoretical, we have an disciplinari oblig to develop equal rigor adequ and standard appropri train materi for new learners. the current tei infrastructur consist of a set of server and servic , allow the guidelin to be primarili access online. A server log is a file automat creat and maintain by a server consist of a list of activ it performed. for our purposes, a statist analysi of server log can be use to examin web traffic patterns. In thi abstract, we propos to analyz the tei server log in order to propos offer suggest to refin the teach methodolog base on what part of the guidelin are more frequent access –while also consid what is not access often. additionally, we are awar that custom of the guidelin exist that aim to meet the major of the need of tei user commun –(for example, tei lite ). however, formal justif for which element are includ and exclud in them do not exist. thi propos interrog and theoriz how we might present the tei guidelin as better teach materi and aim to foster the develop of skill and activ of futur scholars. refer cayless, hugh, jame cummings, martin holmes, peter stadler, and magdalena turska. 2018. “tei technic infrastructure.” present at the tei 2018: 18th annual confer and member meet of the text encod initi consortium, tokyo, japan, septemb 9. jakacki, diane. 2016. “how We teach? digit human pedagogi in an imperfect world.” dian jakacki . june 5. http://dianejakacki.net/how-we-teach-digital-humanities-pedagogy-in-an-imperfect-world/. “tei lite – tei: text encod initiative.” 2019. access april 22. https://tei-c.org/guidelines/customization/lite/. “tei victoria 2017.” 2019. access april 22. https://hcmc.uvic.ca/tei2017/. [1] “tei victoria 2017.” [online]. available: https://hcmc.uvic.ca/tei2017/. [accessed: 22-apr-2019]. [2] D. jakacki, “how we teach? digit human pedagogi in an imperfect world,” dian jakacki , 05-jun-2016. [online]. available: http://dianejakacki.net/how-we-teach-digital-humanities-pedagogy-in-an-imperfect-world/. [accessed: 22-apr-2019]. [3] H. cayless, J. cummings, M. holmes, P. stadler, and M. turska, “tei technic infrastructure,” present at the tei 2018: 18th annual confer and member meet of the text encod initi consortium, tokyo, japan, 09-sep-2018. [4] “tei lite – tei: text encod initiative.” [online]. available: https://tei-c.org/guidelines/customization/lite/. [accessed: 22-apr-2019].', 'explor tei structur to find distinct featur of text type explor tei structur to find distinct featur of text type susann haaf abstract speaker deal with text type (e.g. newspaper, letter, leaflet) success everi day: they are abl to appli the proper text type in a given context, for a certain commun purpose, accord to specif social constraints. however, extens linguist discuss on the factor that substanti constitut text type have not come to an end, yet. among the key distinct featur of text type the textual structur ha been regularli count in though it import compar to other factor could not be final resolved. today, with larg tei corpora at hand carri inform on (logic and layout) text structures, it becom possibl to automat evalu the relev of textual structur for the differenti of text types. In addition, tei structur can be includ in the recognit of other featur whose distribut depend on certain text structures. hence, next to other criteria it seem straightforward to take a closer look at tei structur for the extract of distinct featur for text types. the current paper present an approach to identifi distinct featur of devot text types. three examples, where tei structur is considered, are discussed, name (1) intertextu as indic by bibliograph references, (2) repetit of word and phrase in certain structur contexts, and (3) the level text structured in general. the featur evalu here were mention in previou (predominantli not corpus-based) studi on distinct featur of devot literatur and of text type in general. the studi is base on three 17th centuri corpora: manual of devot (4,057,497 tokens), funer sermon (6,910,357 tokens), and a refer corpu of divers text type (21,862,811 tokens). text are taken from the deutsch textarchiv corpu and are all tag accord to the tei format dtabf. It will be shown to what extent tei tag can help to safe extract these featur and to achiev their more sophist interpretation. relat public (selection) haaf, S. (2019). art und funktion von typographischen mitteln zur textgliederung in erbaulichen textsorten de 17. jahrhunderts. automatisch analys im korpusvergleich und qualit einordnung. In F. simmler and G. baeva, eds. textgliederungsprinzipien. ihr kennzeichnungsformen und funktionen vom 8. bi 18. jahrhundert. berlin: weidler (berlin sprachwissenschaftlich studien 34), pp. 383–410. haaf, S. (2016). corpu analysi base on structur phenomena in texts: exploit tei encod for linguist research. proceed of the 10th edit of the languag resourc and evalu confer (lrec), 23–28 may 2016, portorož (si). kesselheim, W. (2011). sprachlich oberflächen: musterhinweise. In S. habscheid, ed., textsorten, handlungsmuster, oberflächen. linguistisch typologien der kommunikation. berlin/new york: de gruyter, pp. 337–366. simmler, F. (1996). teil und ganz in texten. zum verhältni von textexemplar, textteilen, teiltexten, textauszügen und makrostrukturen. daphni 25 (1996), pp. 597–625. stein, S. (2003). textgliederung. einheitenbildung im geschriebenen und gesprochenen deutsch. theori und empirie. berlin/new york: de gruyter (studia linguistica germanica 69).', 'advantag and challeng of token tei advantag and challeng of token tei tei offer the option to split a text into words/tokens. however, exist token corpora in tei, such as the tei version of the bnc corpus, (almost) never realli use tei, but rather are tei base version of tradit vertic texts, without use too much of the tei markup. when combin full-fledg tei document with tokenization, sever issu and advantag occur. In thi paper, I will discuss the solut implement in teitok, a tei base corpu tool, which combin a searchabl cwb corpu with edit tei/xml files. where in a token set-up, a corpu is defin by one word per line, with option some semi-xml annotations, in teitok a corpu is defin by the token insid the xml document itself, defin by the xpath queri //w. the attribut for each token are then read from the token nodes. one set of problem that aris from token full tei is that element such as <hi> can break tokens. these problem can be solv by split such element into segments, where the new element result from thi can be explicitli mark as repetitions. thi is best illustr with an example: say we have the follow tei fragment: <hi rend=\"underlined\">som under</hi>lining. now we would need to token thi into two word: <w>some</w> <w>underlining</w>, but the <hi> element get in the way of do this. sinc we cannot use unari tag for <hi>, nor can we break up the tokens, the onli avail solut is to break up the <hi> while mark the second one as a repetit of the one befor to keep the inform that there wa in fact onli one underlining: <hi rend=\"underlined\"><w>some</w> </hi><w><hi rend=\"underlined\" rpt=\"1\">under</hi>lining</w>. conceptually, thi solut is straightforward, although to token automat can becom complicated. anoth set of problem ha to do with unari xml elements: the cwb queri languag is both more powerful, sinc in cwb all inform is relat to tokens, mean search for token is much easier, but relev inform ha to be directli connect to tokens. and tei doe not, for instance, explicitli indic which page a token belong to sinc <pb> are unari elements. however, thi is easili overcom in a index corpu by attach each token to the last preced <pb>. In order to establish which page a token appear on, one use the xpath queri ./preceeding:pb[1] to obtain the <pb/> node, where the page number and other relev inform can be obtained. there are variou advantag of token tei, apart from the primari object to allow annot over words. An exampl is the fact that it void the need for a @break=\"no\", sinc word-break <lb> are simpli those insid a token. and token can be associ with sever forms, which void the need for <ex> and <choice>, sinc altern form such as expand and regular form can be model over token directly. An inlin token tei document with linguist annot can be turn into a fulli searchabl corpus, and the grow number of corpora of teitok show the use of thi approach. however, in order to be properli searchable, the teihead should be treat as structur inform and not mere machin readabl data: in order to be abl to onli search in document with a certain type of encoding, that type ha to be in a fixed-valu field, and not just in a textual descript in the encodingdesc.', 'manuscripta - the editor from past to futur manuscripta – the editor from past to futur abstract In thi paper we will introduc a web-bas editor for tei-encod manuscript descript in manuscripta – A digit catalogu of manuscript in sweden (https://www.manuscripta.se). catalogu is done use an interfac which doe not requir ani knowledg of tei and therefor simplifi and reduc the time requir for the catalogu process. previously, it ha been necessari to use an xml editor which had a steep learn curv and wa time-consum as well as error prone, even with schema valid and detail catalogu guidelines. the interfac in the editor is divid into two windows. the left window consist of forms, arrang in tab accord to the structur of the manuscript description: header, binding, provenance, bibliography, facsimile, and codicolog unit(s). these tab form are further divid into tabs, in order to minim the scroll need to reach a specif section in the description. the right window consist of two views, a preview and an imag viewer, which can be switch via tabs. the preview show how the descript will look when publish and the view is updat in real time when the descript is changed. the imag viewer show the digitis manuscript, serv with the iiif api: use an iipimag server and mirador imag viewer. It is also possibl to creat and edit an iiif manifest file in the editor. thi is done by firstli import a comma separ file contain filenames, page descriptions, height, and width. the editor then creat a facsimil element where each imag ha a surfac element with graphic and desc elements. It is possibl to rearrang both the sequenc of the imag and edit the page descriptions. It is also possibl to bulk renam the descript by page or folio numbers. the facsimil data, togeth with a select of data from the manuscript descript is then use to creat the iiif manifest. some part of the manuscript descript consist of run text, e.g. provenance. It is possibl to tag word and phrase by select the text and then right click to get a contextu menu which list the avail elements. the persname, orgname, placename, and bibl element can furthermor be link to author files, which, in turn, can be edited, or created, in the editor. the schema valid is implement to be tie to contexts. each compon can have it own context as well as whole document for each of the differ type of documents, e.g. author document can have their own contexts. the valid is perform on store and ani valid error is commun back to the client where you can click on the error descript to go to the affect element in the right context. the manuscripta-editor is an applic packag (webapp) on top of exist-db, an xml databas which offer advanc full stack app develop with index and search functionality, and built-in function for convert tei to html. In the implement we use reactjs, the declar javascript framework, for creat reusabl view components. By get closer to schema-driven design of UI compon the catalogu editor’ work is eas if the level of detail is chang in the schema, which would mean configur rather than programming. with reactjs’ jsx, the xml-ish syntax, it is easi for an editor to combin alreadi gener compon if need for a more complex component. An addit benefit with reactj is that there is onli a direct one-way dataflow (parent>child) in the compon which make debug much easier. while implement like the tei publish is cover the tei process model (tei-pm) with complex text transform for output differ media types, navigation, pagination, search, and facsimil display by utilis web components, the manuscripta-editor also cover other workflow job like author databas lookups, advanc templating, editor sign-off, and schematron rule validation, in addit to schema-bas validation. share mani goals, like the tei publisher, the manuscripta editor is all about standards, modularity, reusability, and sustainability!', 'native-tei dialect dictionari for bavarian dialect in austria: data structure, softwar and workflow native-tei dialect dictionari for bavarian dialect in austria: data structure, softwar and workflow 1,2 jack bowers, 1 philipp stöckle, 1 ludwig maximilian breuer, 1 han christian breuer 1austrian center for digit humanities, austria 2inria - almanaach, franc thi paper discuss the use of tei in the creation of dualli born-digit and print dictionari as part of the dictionari of bavarian dialect in austria (wörterbuch der bairischen mundarten in österreich ‘wbö’). also we discuss the creation of a lexicograph editor tool that allow the non-tei expert lexicograph to creat tei articl in background of a user-friendli interface. thi work be carri out is a continu of a legaci project which began in 1913 when data began to be gather throughout the bavarian dialect area of the austrian empire. the sourc materi be use for the creation of the new articl wa collect and elicit use questionnair and record on paper slips. vocabulari continu to be collect until the 1990’ when the analogu record were convert to a tustep database. recent the databas of more than 2.4 million entri ha been convert to tei (bower & stöckle 2018). At the core of thi project are sever issu which are particularli signific in the tei, notably: a) the use of tei as primari data format for the creation of both a print and digit resource; b) the lexicograph editor tool which provid a user-friendli and open-sourc altern to oxygen xml editor in the creation of systemat and standard tei articl use odd and yaml formatter; c) the structur approach to dialect dictionari entri in tei (an under-established/peripher usag of the module). In our talk we describ the specif of each of these compon of the project and expand upon what ha been previous present about thi work in bower et al. (2018a,b), focus particularli on the tei articl structur and the editor tool. work cite bowers, j., & stöckle, P. (2018a). tei and bavarian dialect resourc in austria: updat from the dbö and wbö. In A. U. frank, C. ivanovic, F. mambrini, M. passarotti, & C. sporled (eds.), second workshop on corpus-bas research in the human (crh-2). vienna, austria: gerastre proceedings, gtp 1. bowers, j., stöckle, p., breuer, L. m., & breuer, H. C. (2018b). tei born articl on bavarian dialect - updat from the wbö. digit human austria 2018. present at the salzburg. austria.', 'archiv a tei project fairli archiv a tei project fairli A. creamer [https://orcid.org/0000-0002-5286], G. lembi [https://orcid.org/0000-0001-8962], E. mylona [https://orcid.org/0000-0002-0215], M. satlow [https://orcid.org/0000-0001-7692] organization(s): brown university, unit state of america the inscript of israel palestin project is an onlin corpu of inscript from israel and palestine, written in hebrew, greek, latin and aramaic, date roughli from the persian period to the arab conquest. As of spring 2019, it ha collect and encod more than 4000 inscriptions, out of some 10000 relev texts: we aim to creat an exhaust and easili access collect and to enabl user to carri out a varieti of search and extens textual analysis. the fair principl aim to enhanc the abil of machin to automat find and use digit objects, in addit to support their reus by individuals. the principl are organ under four area intend to ensur digit object are findable, accessible, interoperable, and re-usable. follow epigraphy.info’ mission statement we are appli the fair principl to guid our develop of archiv format and process for our corpus. As iip prepar to deposit file in the brown digit repository, we defin format for ensur that our file will be as informative, self-docu and re-us as possible. each inscript is contain in a single, xml file, encod in the well-docu epidoc subset of the tei. these files, however, link to extern maintain control vocabulari (use the xi:includ feature) and bibliographi (use zotero), in order to facilit the work of our encod and ensur consistency. one of our challeng wa to incorpor these extern data into the robust , stand-alone, archiv format. the archiv format of the iip file is the result of a transform that write all the applic <classifications> directli into the <profiledesc> and also put complet bibliograph entri deriv from zotero into each file. We will continu to encod use extern refer file to ensur consistency, but the archiv format should not reli on ani extern sources. We will introduc the fair guid principl and fair metric as they appli to epigraph corpora and tei encoding, discuss the roadmap for implementation, and look at archiv practic beyond fair when it come to preserv of data as well as re-use. while the first step to make a digit corpu findabl and access seem straightforward—iip text have been ingest into the brown digit repository, have uniqu and persist identifiers, rich metadata, and are freeli available, we can still improv on both facets. simpl interoper and re-us are avail through the iip api in both the product and the archiv version of the corpus, however, it will be import to do further work on control vocabularies, share concepts, and encod practic in order to enhanc both of these facets. bibliographi --, 2014. guid principl for findable, accessible, interoper and re-us data publish version b1.0 [www document]. force11. url https://www.force11.org/fairprincipl (access 5.10.19). --, epigraphy.info [www document], n.d. url http://epigraphy.info/ (access 7.31.19). feraudi-gruénais, f., grieshaber, f., 2016. digit epigraphi am scheideweg? / digit epigraphi at a crossroads? present at the nachnutzung und nachnutzbarkeit der forschung im akademienprogramm workshop der nordrhein-westfälischen akademi der wissenschaften und der künste und der union der deutschen akademien der wissenschaften AG „ehumanities“, düsseldorf. https://doi.org/doi:10.11588/heidok.00022141 implement fair data principles: the role of libraries, 2017. . liber. url https://libereurope.eu/blog/2017/12/08/implementing-fair-data-principles-role-libraries/ (access 5.10.19). satlow, m., 2002. inscript of israel/palestin [www document]. url https://library.brown.edu/iip/index/ (access 7.31.19). https://doi.org/10.26300/pz1d-st89 wilkinson, m.d., dumontier, m., aalbersberg, ij.j., et al., 2016. the fair guid principl for scientif data manag and stewardship. scientif data 3, 160018. https://doi.org/10.1038/sdata.2016.18', 'An encod strateg propos of “ruby” texts: exampl from japanes text In japanes texts, rubi text is a run of text in phonet script attach to a particular portion of the main bodi of the line (gener chines characters) in order to repres the read of the portion (w3c, 2001, inter alia). It is name after the bodi size name of it in the moveabl type printing. It is also call furigana as it is alloc to the main texts, which origin develop in east asian vernacular gloss cultur to the classic chines texts. rubi today is not, however, a mere annot to the line: it is more stuck to the particular charact than simpl interlinear gloss and, more importantly, it should be read in parallel with the main line, or even as if it were the main line itself. thus, rubi can be use not onli as a simpl guid of pronunci but also as an altern to the main text. histor speaking, rubi wa also attach to both side of the main text in order to denot both read and gloss, especi for borrow words: even in thi case, reader may read ani side of rubi in accord with their preferences. besides, each side can correspond to differ span of text. these rubi function are not confin to a simpl gloss, which can be encod within the current tei framework, and they deserv to be encod in it own way. one can find such a two-sid exampl where a word 打球場, or billiard hall, is attach two rubi (): ビリヤード (phonet transcript of the word billiard in japanes katakana) to the whole, and ダキウ (phonet transcript of the word 打球 in japanes katakana) to the first two characters. from f. 28v of niwa (1878) and it modern rendering. public domain. with thi case, we will propos to encod thi as follows: 打球 ダキウ 場 ビリヤード An encod exampl of the figur 1. here, follow exist conventions, rubi element denot the whole rubi environments; rb element denot the bodi texts, where nest rubi element are allow insid to encod two-sid rubies; and rt element denot the rubi texts, where type describ whether the rubi text in question is primari (p) or secondari (s). primari and secondari rubi are determin accord to the text direction: in vertic right-to-left writing, primari rubi are on the right, wherea horizont left-to-right writing, they will be on the top. thus, our encod strategi encompass histor varieties. sinc rubi ha long been one of the major concern in japanes texts, it encod strategi ha natur been propos from it earli days, includ w3c’ rubi specif (w3c, 2001; 2013a; 2013b, hara & yasunaga, 2002, whatwg, 2019). their consider are quit broad and informative, regret limit to modern usag or either confus on the inclus of interlinear gloss as rubi sole base on layout similarities.thi is becaus the exist rubi encod have focus in realiz the contemporari rubi layouts, especi present as ji (2004) or more access w3c (2012). see kawabata (2014) for more references. our propos differenti interlinear gloss to be encod by the gloss element, which are attach to a particular place, rather than a particular part of the bodi (see estill, 2016 for complex in encod of interlinear glosses). thi distingu is of necessari to an encod to be more proper and simpler.', 'tei encod of correspondence: A commun effort tei encod of correspondence: A commun effort stefan dumont, susann haaf, sabin seifert As conven of the tei correspond sig and repres for the dta base format, we were repeatedli ask about the tei encod of correspondence-specif phenomena. although the tei guidelin contain suggestions, there are still open question on how to deal with sever structur and textual occurrences. thi situat led to the idea of hold a workshop to discuss problemat case of correspond encod in tei, and to develop solut as well as potenti extens to the tei. the workshop wa fund by clarin-d and wa held in octob 2018 in berlin (germany). We invit early-carri research who deal with tei encod and/or correspond encod in the cours of their daili work, as well as one member of the tei council for advic on propos for tei extensions. from the participants, we gather exampl of insecur or problem with appli tei to correspond text beforehand and dealt with these in the workshop. all problem discuss concern aspect of letter coding: from letter-specif text structur to correspond metadata. To name but a few, problem with <postscript> or <salute> were treat as well as the use of <correspdesc> for specif correspond situations. the correspond metadata interchang format wa also develop further. follow the workshop, the problems, discussions, and solut were summar as handbook-lik articl by the workshop participants. the public “encod correspondence. A handbook on encod correspond in tei-xml and dtabf” will be releas step by step as open access from juli 2019 on with the possibl for the commun to review and comment. In our talk, we want to present thi initiative. We will outlin the field of correspond encod which were discuss and featur some interest case and their solutions. We will also present the cours of action appli which to us seem to evok fruit discuss on tei encoding. bibliographi stefan dumont, \"correspsearch – connect scholarli edit of letters\", journal of the text encod initi [online], issu 10 | 2016, onlin sinc 14 februari 2018, connect on 02 april 2018. url : http://journals.openedition.org/jtei/1742 ; doi : 10.4000/jtei.1742 susann haaf, christian thomas, \"enabl the encod of manuscript within the dtabf: extens and modular of the format\", journal of the text encod initi [online], issu 10 | 2016, onlin sinc 08 august 2017, connect on 27 septemb 2017. url: https://journals.openedition.org/jtei/1650; doi: 10.4000/jtei.1650. peter stadler, marcel illetschko, and sabin seifert, \"toward a model for encod correspond in the tei: develop and implement <correspdesc>\", journal of the text encod initi [online], issu 9 | septemb 2016 - decemb 2017, onlin sinc 24 septemb 2016, connect on 02 april 2018. url : http://journals.openedition.org/jtei/1433 ; doi : 10.4000/jtei.1433', 'An attempt of dissemin of tei in a tei-underdevelop country: activ of the sig eaj one of the mission of special interest group for east asian/japanes (hereinaft sig eaj) is the dissemin of tei in japan. A characterist of japan in comparison with tei-advanc countri in north america and europ is that the cultur which util xml-relat technolog such as xslt and xqueri have not been widespread. In addition, methodolog of research for textual have not wide been treat among human research in japan. In thi paper, we discuss how to spread tei in such tei-underdevelop countri base on the activ of sig eaj. one of our activ is the hands-on session which handl aozora bunko texts. aozora bunko transcrib public domain work like project gutenberg, and about 20,000 text are avail in it origin format and html. thi hands-on session aim to provid not onli the opportun to learn tei by the convert practic but also the tei-compli japanes text on the web. We are also prepar a comprehens set of tutori for japanes resourc use github. section such as how to write the tei header, how to encod play have been creat so far. through these sessions, more than 20 text were encod in conform with level 3 of best practic for tei in libraries. In addition, we are develop visual tool use javascript such as ceteicean, and the program which automat convert aozora bunko text into tei in the level 2 of the practice. By coupl with compil tutori and accumul markup exampl target aozora bunko texts, we have gain in-depth knowledg of markup on modern japanes texts. through thi activity, the number of peopl who are familiar with tei is get increased. moreover, in countri where xml-relat technolog have not been widespread such as japan, javascript tool can work well to share the benefit of tei. thi point is consid to be help in the spread of tei in other tei-underdevelop countries.', \"encod histori in tei: A corpus-ori approach for investig tibetan historiographi encod histori in tei: A corpus-ori approach for investig tibetan historiographi mathia fermer (mathias.fermer@oeaw.ac.at) My present address a system for deriv histor evid from tibetan primari sourc by appli semant markup to the texts' key entiti (i.e. persons, places, literari work and artefacts). thi markup system follow the tei-p5 guidelin and ha been develop in the framework of the sakya research web-appl () which hold a larg corpu of machine-read sourc in classic tibetan, rang from mediev chronolog and histori to illustri life stori of buddhist masters. the markup appli to the digit collect ha been design in line with the historiograph natur of the texts: It captur inform about histor agents, the place they visited, as well as artefact and literari work mention in vari context along the chronolog sequenc of the individu texts. use tei-markup in thi way ha proven particularli use in my own research for depict the social, geographical, artist and doctrin context of the texts' narr subject (and their authors). It allow for track teacher-stud relationship and explor the geograph expans of those masters' region networks, to give two exampl for how the empir evid from tei-encod text can be assessed. I will address the concept behind thi markup and it potenti for a quantitative, intertextu analysi that goe beyond singl texts. what can tibetan historian gain from markup-technology, if systemat appli to a wider corpu of literature? I will argu that the data deriv from a consist annot of primari literatur on a larg scale will gradual chang our understand of tibetan history. At the same time, such a corpus-ori approach to historiographi rais sever conceptu and practic question about how, and in which form encod inform can best be stored, analysed, display and reused.\", \"text graph ontology. A semant web approach to repres genet scholarli edit text graph ontolog A semant web approach to repres genet scholarli edit peter hinkelmanns, univers of salzburg A model of text as a variant graph can support repres genet text editions. the propos model make it possibl to describ the relat between token and their rel depend in text genesis. the main focu is on the represent of intradocumentari text revisions. moreov the text-graph-ontolog enabl the referenc of genet text edit via the semant web. In addit to thi ontology, a convert from and to tei-xml and a web-bas viewer and editor will be presented. graph, ontology, semant web, genet text edit text genet edit are enjoy sustain popular in the field of scholarli edit and literari studies. repres of recent research project are the faust edit (bohnenkamp, henke, and jannidi 2016) or the edit of the work of arthur schnitzler (burch et al. 2016), both of which aim at a complet reproduct of the text genesis. these edit requir the reconstruct of complex text genet processes. which sequenc of token form a specif text state? how can differ between version be described? the extens of the model of the text encod initi by element requir for genet edit wa the subject of a work group that present it result in a draft (burnard et al. 2010). part of thi draft have been incorpor into the tei guidelin (tei consortium 2013). with tei p5, complex genet edit can be realized. however, the underli structur of the hierarch graph make it difficult to reconstruct and compar text gradients, i.e. the evolutionari stage of a text with inlin markup. It can of cours be done use stand-off and out-of-lin annotations, as jame cum ha point out (cum 2018, 13). thi concept for a text graph ontolog doe not aim to be the next paper critic the xml foundat of tei P5 nor is it tri to compet with the interoper of tei encod texts. the ontolog is design for the specif purpos of implement a text variant graph use semant web technolog for the encod of genet text editions. data model for textual represent sever project use graph to repres textual variat in the recent years. they can be divid into method base on markup and method focus on variant graphs. An approach to deal with the problem of overlap annot in xml is the propos markup languag ‘gener ordered-descend direct acycl graph’ (goddag: sperberg-mcqueen and huitfeldt 2004, esp. 158). A similar model is the ‘graph annot format’ (graf: ide and suderman 2007) which extend the linguist annot framework (laf: ide and romari 2006). In graf an underli text is segment via stand-off nodes, which use the posit of charact in the text as refer points. edg link the annot with text segments. the problem of intradocumentari and intertextu variat is address by the ‘data structur for repres multi-vers texts’ (schmidt and colomb 2009). thi critic the markup approach of model like goddag and graf (schmidt and colomb 2009, 499) and propos a variant graph where the edg contain the segment of text share by multipl variants: fig. 1: A variant graph (figur by schmidt and colomb 2009, 502) the sigil indic the differ variant of the text between differ text carriers. individu token of a specif text carrier are not represented. collat text is the focu of the stemmaweb project (“the stemmaweb project” 2012–; andrew and mace 2013). similar to schmidt and colomb 2009 a variat graph is use to repres version of a text. the text are segment into token and form the node of the graph. direct edg show the similar and dissimilar between individu text carriers. undirect edg repres variant relationship like orthograph variation, grammat variation, lexic variat etc. (andrew and mace 2013, 508). fig. 2: screenshot of the ‘text relationship mapper’ of stemmaweb, show an extract of segment 1 of the chronicl of matthew (figur by “the stemmaweb project” 2012–) efer 2016 ha comprehens describ the use of graph databas for the text-ori digit humanities. He propos ‘kadmos’, a layer graph model for textual representation. hi model includ the separ into type and tokens: fig. 3: schemat represent of instanc data set and link of a short exampl document with minimalist text data model (figur by efer 2016, 76) the ‘text as graph’ model (tag: haentjen dekker and birnbaum 2017) store the text in node of variou length (haentjen dekker and birnbaum 2017, §3). A token may be split into sever node and mark up as a word: fig. 4: A simplifi poem with word token (figur by haentjen dekker and birnbaum 2017, fig. 10) tag is at the current stage defin as a data model and not as a syntact represent (haentjen dekker and birnbaum 2017, §2.1). thi veri brief overview of select data model for textual represent ha shown, that multipl model for the represent of text as graph exist. the propos model of thi paper is far from state of a stabl model and aim to connect the idea of a variat graph with semant web technologies. text graph ontolog the semant web make inform access in a machine-read way use standard vocabulari and ontologies. A distinct is made between ‘nodes’, which can be object or atom values, and ‘edges’, which describ the relationship between nodes. A statement alway consist of a tripl ‘[subject] - [predicate] - [object]’. semant web technolog enabl easi annot and link the scholarli genet edit other resources. the text graph ontolog use the web ontolog languag (owl: w3c owl work group 2012) to specifi class and properties. the first problem which need to be address is the segment of a text. there is no agreement between the briefli present model on what the atom unit of a text graph is. for the text graph ontolog token separ by white space should be assumed, with the possibl to extend the model on a sub token level. A diplomat transcript and variou normal stage can be attach to a token as a string or generatet from a separ charact graph. the propos for the annot of text revis of the grazer editionsphilogi are tailor to the need of mediaev editions. hofmeister-wint 2016 present a categor of text revis phenomena. she distinguish between self-revisions, i.e. intervent in one' own text, and extern revisions, which describ the intervent of anoth hand (hofmeister-wint 2016, 10). self-revis can be a direct compon of text product (immedi revision) or take place at a later point in time (late revision). In their opinion, third-parti revisions, on the other hand, take place exclus in a later revis step as a late revision. furthermore, the follow typolog is establish by the graz project (hofmeister, böhm, and klug 2016, 22): erad by bleaching, deletion, blackening, expans transform resp. transform by overwriting, addition, reduct insert in all describ posit (interlinear, linear, marginal) after erad by delet or bleaching, with instruct sign in differ shapes, singl or paired, as well as gap fill after precautionari recess To repres these revisions, an edg weight direct acycl graph is used. the limit of rdf make is necessari to construct weight edg as individu nodes. the base model therefor consist of three node classes: tokens, connector and border (the empti start and end node of a graph): fig. 5: ‚hello world‘ as a weight text graph in rdf the weight repres the rel order of edg from one token to the next. A substitut would therefor be repres as follows: fig. 6: ‚hello world graz‘ as a weight text graph in rdf the path follow the lowest weight is the first, the past follow the highest weight is the last version of the text. the reconstruct of a particular text state can be describ as a path through the text. delet and addit of text can be seen in the graph accordingly: fig. 7: transformation, addit and delet in a weight text graph A convers of thi small graph to tei P5 is possible: <del>hello</del> <subst><del>world</del><add>graz</add></subst> <add>tei</add> To mark specif stage of a text, the connector are be referenc to a text stage: fig. 8: text stage realis in a variant text graph the same variant graph method can be use to describ a token further on charact level: fig. 9: graph on charact level thi short articl show that semant web technolog are suitabl for repres text variant graphs. the main benefit from use rdf is the interconnect with other semant web ressources. I. e. differ text carri of one text could be transcrib by differ project and easili be link with each other. annot on a text can directli point to author file and vice versa. tool base on the model are a convert to and from tei P5 and a viewer/editor base on flask. the ontolog and the tool will be publish on github. A challeng of thi graph approach that ha not yet been solv is rule-bas validation. peter hinkelmanns, senior scientist, univers of salzburg: middl high german conceptu database, peter.hinkelmanns@sbg.ac.at peter hinkelmann is a senior scientist at the middl high german conceptu databas of the univers of salzburg. hi research interest includ middl high german lexicography, graph technolog in the digit human and histor linguistics. refer andrews, T. l., and C. mace. 2013. “beyond the tree of texts: build an empir model of scribal variat through graph analysi of text and stemmata.” intern journal of human-comput studi 28 (4): 504–21. doi:10.1093/llc/fqt032. bohnenkamp, anne, silk henke, and foti jannidis. 2016. “johann wolfgang goethe: faust: historisch-kritisch edition.” 2. beta-version. access april 26, 2017. http://beta.faustedition.net/. burch, thomas, stefan büdenbender, kristina fink, vivien friedrich, patrick heck, wolfgang lukas, kathrin nühlen et al. 2016. “text[ge]schichten: herausforderungen textgenetischen edieren bei arthur schnitzler.” In textgenes und digital edieren: wolfgang koeppen „jugend“ Im kontext der editionsphilologie, edit by katharina krüger, 87–105. editio / beihefte, 40. berlin, boston: de gruyter. burnard, lou, foti jannidis, elena pierazzo, and malt rehbein. 2010. “an encod model for genet editions.” access march 25, 2019. http://www.tei-c.org/activities/council/working/tcw19.html. cummings, james. 2018. “A world of difference: myth and misconcept about the tei.” digit scholarship human 6:i63. doi:10.1093/llc/fqy071. efer, thomas. 2016. “graphdatenbanken für die textorientierten e-humanities.” dissertation, universität leipzig. haentjen dekker, ronald, and david J. birnbaum. 2017. “it’ more than just overlap: text as graph.” In proceed of balisage: the markup confer 2017, edit by B. T. usdin, deborah A. lapeyre, jame D. mason, C. M. sperberg-mcqueen, and norman walsh. balisag seri on markup technologies: mulberri technologies, inc.rockville, maryland. hofmeister, wernfried, astrid böhm, and helmut W. klug. 2016. “die deutschsprachigen marginaltext der grazer handschrift ub, ms. 781 al interdisziplinär prüfstein explorativ revisionsforschung und editionstechnik.” editio 30 (1): 14–33. doi:10.1515/editio-2016-0002. hofmeister-winter, andrea. 2016. “beredt verbesserungen.” editio 30 (1): 1–13. doi:10.1515/editio-2016-0001. ide, nancy, and laurent romary. 2006. “repres linguist corpora and their annotations.” In proceed of the fifth languag resourc and evalu conference: lrec 2006, edit by nicoletta calzolari, khalid choukri, aldo gangemi, bent maegaard, joseph mariani, jan odijk, and daniel tapias: genua. ide, nancy, and keith suderman. 2007. “graf: A graph-bas format for linguist annotations.” In proceed of the linguist annot workshop: held in conjunct with acl 2007, edit by associ for comput linguistics, 1–8. https://www.aclweb.org/anthology/w07-1501. access juli 30, 2019. schmidt, desmond, and robert colomb. 2009. “A data structur for repres multi-vers text online.” intern journal of human-comput studi 67 (6): 497–514. doi:10.1016/j.ijhcs.2009.02.001. sperberg-mcqueen, C. m., and clau huitfeldt. 2004. “goddag: A data structur for overlap hierarchies.” In digit documents: system and principles, edit by peter king and ethan V. munson, 139–60. berlin, heidelberg: springer. tei consortium. 2013. “tei p5: guidelin for electron text encod and interchange.” version 3.6.0. access juli 30, 2019. https://www.tei-c.org/vault/p5/3.6.0/doc/tei-p5-doc/en/html/. “the stemmaweb project: tool and techniqu for empir stemmatology.” 2012–. access juli 30, 2019. https://stemmaweb.net/. w3c owl work group. 2012. “owl 2 web ontolog language: document overview (second edition).” w3c recommend 11 decemb 2012. access june 08, 2018. https://www.w3.org/tr/owl2-overview/.\", 'genesi and variance: from letter to literatur dr. torsten roeder genesi and variance: from letter to literatur abstract: the paper examin the natur of textual genesi and textual variance, base on a letter which wa later elabor into an epistolari novel. keywords: textual variance, textual genesis, alignment, critic apparatu the studi of textual genesis, as introduc by karl lachmanncfr. plachta, B. (2013). editionswissenschaft. 3rd ed. stuttgart: reclam, 27–45. and pursu by mani others, make comprehens that text is a dynam medium. dure it composit in tempor succession, text is constantli revised, reorganized, and reshaped. within thi process, even context and genr can be subject to change: draft turn into stories, and whole novel deriv from tini notes. document are what is left of such textual dynamics. understood as physic manifest of text, document appear as stabl entities, which can rel easi be digitized, transcribed, and described. the current standard guidelin for text encod seem ideal for both machine- and human-read represent of documents, and they strongli support document-rel studi of all disciplines. however, in order to pursu the question of what “text really” might be, the encod philologist should also take a look beyond the document and refocu the attent on the examin of textual processes.cfr. zanetti, S. (2012). schreiben al kulturtechnik. berlin: suhrkamp, 10–16. which phenomena of dynam can occur in text, and which encod stategi are necessari to repres them? how could varianc be describ and classified, e. g. by distinguish formal, stylistic, paratextu and contentwis variants?roeder, T. (2018). vom authentischen brief zur durchgestalteten literatur. [blog] digit human am dhip. avail at: [30 nov. 2018]. are the current standard suffici to repres textual dynamicity, or do they favor document oriented, but less dynam concept of text? the idea of thi paper is base on an autobiograph text by friedrich rochlitz, an author of the earli 19th century, who wrote an eye-wit report of the battl of leipzig (1813) to a close friend in dresden.roeder, T. (2018). tage der gefahr. [blog] digit human am dhip. avail at: [3 oct. 2018]. dure the siege, the letter could not be dispatched. rochlitz continu write hi letter until the war wa over, and finally, hi friend receiv a veri long and impress detail letter.slub dresden, mscr.dresd.h.37,verm.2°, saxonica/sammlungen zur zeitgeschichte, tagebuch von rochlitz über die begebenheiten in leipzig (1813). without the author’ consent, the letter wa forward to other interest readers. possibl due to posit feedback from the latter, rochlitz decid to elabor the letter into a novel, which wa publish first in 1816, and as a revis edit in 1822.rochlitz, F. (1816). tage der gefahr. in: neue erzählungen, 2, leipzig/züllichau: darnmannsch buchhandlung, 149–365; rochlitz, F. (1822). tage der gefahr. in: auswahl de besten au friedrich rochlitzʼ sämmtlichen schriften, 6, züllichau: darnmannsch buchhandlung, 185–312. while the variant between the two print edit can be describ and present by mean of a classic tei inlin apparatus,cfr. tei: critic apparatus. [online] P5 guidelin for electron text encod and interchang guidelines. avail at: [16 juli 2019]. the textual differ between rochlitz’ origin letter and the first edit in print requir a standoff base align method, includ ambigu and uncertainties. but doe it make sens to combin differ encod method for one textual history? doe it make more sens to separ variant from witnesses? and which concept of textual dynam is impli by the decis for a specif markup method? outgo from recent research, the paper present markup approaches, analysi toolse.g. “teicat: tei critic apparatu toolbox”, http://teicat.huma-num.fr; and “lera: locate, explore, retrac and apprehend complex text variants”, https://lera.uzi.uni-halle.de. and present methodsa prelimin version which compar the two print edit is avail at: (password and username: “teiconf”); the origin letter will be includ until the conference. to examin the risen question on textual genesi and variance, in order to broaden the understand of textual dynam for encod editor and end user as well. author information: torsten roeder graduat in musicolog and italian studi at the humboldt univers in berlin. He work in the area of digit human sinc 2007, start hi career at the berlin-brandenburg academi of scienc and humanities. hi phd thesis, present at the univers of würzburg, is base on a tei edit of 19th centuri music criticism. current he work as digit human offic at leopoldina (german nation academy, hall an der saale). contact: torsten.roeder@leopoldina.org', 'five centuri of histori in a network how should histor research encod text that will be use as primari sourc for network analysis? moreover, how can they link togeth the variou layer of thi encod in a way that explicitli show the encoder’ methodolog and interpretations? thi present is intend to demonstr and solicit feedback on an experiment procedur applic to project produc network from historiograph texts. the project commun of knowledge: interreligi network of scholar in ibn abi usaybiʿa’ histori of the physician (https://usaybia.net)fund by the german feder ministri of educ and research. is conduct a network analysi of interact among muslim, christian, and jewish scholar across five centuri of abbasid rule in the near east (750–1258). the primari sourc text for thi endeavor is an arab biograph dictionari written by ibn abi usaybiʿa, a physician activ in cairo and damascu in the 13th century. thi histori of physician detail teacher-stud relations, workplac exchanges, correspondence, oral reports, and other interact among the scholar mentioned. the network analysi base on thi historiograph text requir a way to proceed from encod a sourc text in tei to produc a dataset for network analysis. along the way, it must document at least two layer of interpretation: (1) the primari sourc author’ portray of event and (2) the researcher’ own understand of these assertions. one way to make these interpret layer explicit is to isol individu “factoids” as analyt units. encod factoid in tei and rdf ha been recent pioneer by daniel L. schwartz for the syriaca.org project “spear: syriac persons, events, and relationships.” the current project extend thi by produc factoid directli from a primari sourc and integr them into the encod text, as well as by further refin the process of curat result factoid into node and edg for network analysis.', 'A sign of the times: mediev punctuation, it encod and it rendit in modern time digit manag punctuat in the edit of mediev manuscript is one of those issu that initi look like minor details, but later reveal themselv as a tangl web of problem span from comput scienc (how to repres punctuat signs?) to philolog (what type of sign do exist?) through epistemolog (i the process of punctuat a mere technic transform or a valuabl part of the scholarship?). the aim of thi paper is to address the theoret aspect of these question and their practic implications, provid a coupl of solut fit the paradigm and the technolog of the tei. the debat on how to deal with mediev punctuat is a long and still open one. follow contini (1992), the interpret edit of a manuscript is the translat of a histor attest system into anoth system. accordingly, the philologist should recogn the punctuat system of the manuscript and convert it into a modern one. there are, however, no establish univers method for do so; most of thi work is left to the experi (and taste) of the scholar. In fact, editor often substitut the origin punctuat with a modern one. thi improv the text readability, but often lead to a loss of textual information. In thi paper we describ how we dealt with the encod and transform of the punctuat sign in some german manuscript of marco polo’ travel account. technically, we implement a set of gener rule (a xslt templates) and variou ad-hoc except (a descript instruct in xml attributes). In addit to this, we discuss the philolog foundat of thi method and, contextually, we address the topic of the transform of a singl origin sourc into differ transcriptions: from a hyperdiplomat edit to an interpret one, go through a spectrum of intermedi level of normalization. We also reflect on the separ between transcript and analysis, as well as on the role of the editor when the edit is the output of a semi-autom process.', 'reflect the influenc of technolog on model of text in scholarli digit edit short paper abstract, tei 2019 reflect the influenc of technolog on model of text in scholarli digit edit By julia josfeld and grant leyton simpson (georg-august-universität göttingen) abstract: there are mani resourc that aim to help scholar first start out in the field of digit editing. cours on xml, xpath, and xslt, introduct to tei, tutori on how to use common softwar suites, and mani more convey a ground understand of the technolog involv in thi increasingli relev field. however, one aspect that ha receiv less attent so far is how our applic of these technolog influenc our approach to and understand of “text”. In the complex digit medium, the “text” of an edit consist not onli of a reproduct of the origin sourc or sourc and a number of scholarli apparatuses, but also includ the variou layer of infrastructur that make it access to potenti users. depend on which choic editor make regard these infrastructures, the result edit will inherit their structur possibl and limitations, which in turn will dictat constraint on the model of text that can be used. while these underli influenc can sometim go unexamined, sinc they appear simpli as immut requir of the chosen technology, they nevertheless shape the underli theoret framework that an editor will work from. It is crucial, therefore, that we use care discuss of the technolog (e.g. tei) and their intersect with the materi of our edit to elucid the underli model possibl and allow for fruit and posit decis making. To illustr thi point, we will show how the applic of tei shape the model of text we were work from in our digit edit project, the electron corpu of anonym old english homili (echoe), and how we were abl to come to a more nuanc theoret framework onc we start interrog our base assumpt inherit from thi technology.', 'valid @selector: a regular express adventur start with P3 in 1994 (i.e., over two year befor css1 wa released), the guidelin support a mechan to indic a default rendition, a way of say all persnam element were in ital in the original. you would put the name of an element on the gi attribut of a tagusag element in order to indic which element had a particular default rendition. start in 2015-10 with P5 2.9.0, tei introduc a new method for the same purpos (and then phase out the origin method). In thi new method you specifi which element a default rendit appli to use the cascad style sheet (css) select mechan — you put a css selector on the selector attribut of a rendit element. but the tei onli defin selector as teidata.text (which boil down to the relax NG string datatype). thi struck me as insufficient; formal syntact valid is in order. thu I set about write a regular express to valid css3 selectors. thi present is about both the process of creat said regular expression, and the result, which is a regular express just over 18,300 charact long which I believ correctli match valid css3 selector and correctli fail to match other strings. topic to be address includ the following. obstacl to write length and complex how do you write such a long expression? the answer is you don’t—you write a program to write the expression. I wrote such a program in perl, but plan to re-writ it in xslt befor the presentation. confus there are some aspect of the css3 specif that aren’t entir clear, at least not to me. imposs accord to sever sources, css3 is not regular, and thu it cannot be pars with a regexp. So how wa I abl to do this? I think there are three contribut factors. I wa not deal with all of css3, onli with selectors; not tri to pars the selector into their compon segments, but rather onli tri to return ye or no; unawar it wa imposs until after i’d done it. featur output format the program will gener output in either relaxng or xslt built-in test the output includ a test suit of thousand of css3 selector case becaus of limit in relaxng’ use of regular expressions, the regular express produc respect case in some place where it should be ignored. languag I did not write the portion of the regular express that test a bcp 47 languag tag, but rather download someon else’ resourc usag the regular express run veri quickli in relaxng use jing, and veri slowli in xslt use saxon.', 'use microsoft word for prepar xml tei-compli digit edit use microsoft word for prepar xml tei-compli digit edit the paper will introduc the so-cal electron edit prepar in microsoft word text processor. thi tool wa origin develop in 2000 in order to gener so-cal vertic format for a corpu manag and in 2008 it wa modifi in order to output xml tei format for the manuscriptorium project (nation librari of the czech republ 2019). for captur structur inform (headings, indic etc.) and the semant of the individu part of the edit (notes, read etc.) the editor use charact and paragraph style whose applic is describ in černá and lehečka (2016). the methodolog also includ rule of transform the individu style into xml format accord to the tei P5 guidelin (the text encod initi consortium 2019). An add-in for microsoft word wa program to help editor to appli format and other necessari part of the edition, e.g. page and line numbers. the author of the paper will focu on docx document convers to xml tei P5 format. the convers which consist of approxim 60 sequenti appli xslt transform (world wide web consortium 2009) is driven by a special applic (program in c#). In order to keep as much of “the editor’ intent” as possibl and reduc error result from the transform process (omit or duplic text) an electron tool wa creat which extract plain text (divid into basic text and annotations) from the input (docx) and target document (xml tei p5). thi can be carri out by a much smaller number of xslt transform (onli 6 stylesheets). the output text is subsequ compar via text comparison tools, e.g. winmerg (2013): differ text chunk point to problemat passages, where transform to xml format fails. with these tool approxim 240 edit of old- and middle-czech text (from 1300–1800) wa prepar dure 10 years, current attract new potenti user across institutions. refer černá, A. and lehečka, B. (2016). metodika přípravi a zpracování elektronických edic starších českých textů. (the methodolog of the prepar and process of electron edit of old czech texts.) [pdf] praha: oddělení vývoje jazyka ústavu pro jazyk český AV čr, v. v. i. avail at: <http://vokabular.ujc.cas.cz/soubory/nastroje/methodics/metodika_pripravy_a_zpracovani_elektronickych_edic_df12p01ovv028.pdf> [access 14 may 2019]. nation librari of the czech republ (2019). manuscriptorium. digit librari of written cultur heritage. [online] avail at: <http://www.manuscriptorium.com> [access 14 may 2019]. the text encod initi consortium (2019). tei p5: guidelin for electron text encod and interchange. [online] avail at <http://www.tei-c.org/guidelines/p5/> [access 14 may 2019]. winmerg 2.14.0 (2013). avail at <http://winmerge.org> [access 14 may 2019]. world wide web consortium (w3c) (2009). xsl transform (xslt) version 2.0. avail at <https://www.w3.org/tr/2009/per-xslt20-20090421/> [access 14 may 2018].', 'inscriptions, hieroglyphs, linguistics… and beyond! the corpu of classic mayan as an ontolog inform resourc inscriptions, hieroglyphs, linguistics... and beyond! the corpu of classic mayan as an ontolog inform resourc franziska diehr, sven gronemeyer, uwe sikora, christian prager, maximilian brodhun, elisabeth wagner, katja diederichs, nikolai grube answer the question “what is text, really?” may be impossible, for ‘text’ be a most complex resource, fulfil numer purposes, manifest in divers document type with uniqu characteristics. To studi ‘text’ use digit methods, some kind of represent is required. the project ‘text databas and dictionari of classic mayan’ for more inform on the project, see: compil the hieroglyph text written by the ancient maya in a machine-read corpus. We do so with an approach fit the idea of “tei and beyond”: ‘text’ is repres as separ inform resources, each describ by an ontolog model repres the specif semant and complex of the material. use differ format (rdf, xml) and standard (cidoc-crm, tei-p5), the inscript are encod in a multi-level corpus: the corpu further consist of (4) an ontological-bas rdf-schema for histor and scholarli inform and physic featur of text carriers, and (5) the ‘maya imag archive’ for photograph and archiv material, for which we use the dariah servic ‘conedakor’: . (1) a tei-al conform schema defin valu and rule for the encod of the text’ topolog and structur features, (2) a ‘sign catalogue’ for the classif of maya hieroglyph (diehr et al.), and (3) the tool ‘almah’ for linguist analys (grube et. al, 5-7). maya write is not yet fulli deciphered, not all sign are known, and we still deal with compet read hypothes and a miss unicod charact set. there are effort in thi direct (pallan gayol, anderson), but in their current form they do not meet the classif requir of the maya script. these challeng are also present in other ancient, non-alphabet write system (rossi, De santis). the interdisciplinari work group encow (encod complex write systems) wa set up in 2015 for the purpos of harmonis encodings. To repres the script, we use stand-off markup to enabl an interlink structur between distribut sources: the tei encod serv as central data source, embed other inform (fig. 1). maya glyph are group in blocks, each usual contain more than one in differ arrangements. depend on space requir and aesthetics, individu sign merge, overlap, be infixed, or rotated, depend on the sign shape and space within the block. use @rend and @corresp repres thi structur by describ the posit to the neighbor glyph. By use @corresp to refer to the neighbor glyph, we mimic a numer transliter (similar to the ‘leiden conventions’), but in a more precis way: with support of the tei semant and the xml syntax an unambigu descript of the glyph arrang is provided. the project will encod approxim 10,000 texts. the data will success be made access under a CC by-4.0 licens on our project portal (https://www.classicmayan.org/) which is current in the stage of conception. furthermore, the corpu data will also be publish in the textgrid repositori (https: //textgridrep.org/), where they can also be access by extern user via oai-pmh. the rdf data of the sign catalogu will be also retriev at the portal via a sparql endpoint and also at the TG rep. To support the workflow, we develop a parser that creat the accord tei/xml structur out of a project-specif sign number transliter (grube et. al, 2-3). In our approach, ‘text’ is understood as a multi-level inform resourc in form of an ontolog corpus, offer differ view and access point to the material, provid a holist environ for studi classic mayan. figur 1: within tei:g the valu of attribut @ref refer to the uri ofth graph record in the sign catalogue. it ontolog structur link thegraph to it linguist expression, to which a transliter valu is assigned. refer franziska diehr, sven gronemeyer, elisabeth wagner, christian prager, katja diederichs, uwe sikora, maximilian brodhun, and nikolai grube. model vagueness: A criteria-bas system for the qualit assess of read propos for the deciph of classic mayan hieroglyphs.in michael piotrowski, editor, proceed of the workshop on comput method in the human 2018, volum 2314 of workshop proceedings, page 33–44, lausanne, switzerland, 2019. ceur. nikolai grube, christian prager, katja diederichs, sven gronemeyer, antj grothe, céline tamignaux, elisabeth wagner, maximilian brodhun, and franziska diehr. textdatenbank und wörterbuch de klassischen maya annual report for 2017. textdatenbank und wörterbuch de klassischen maya, (project report 5), 2018. carlo pallan gayol and deborah anderson. achiev machine-read mayan text via unicode: blend “old world” script-encod with novel digit approaches. In élika ortega, glen worthey, isabel galina, and ernesto priani, editors, digit human 2018 - puentes-bridges: book of abstracts, page 256–261, mexico city. iren rossi and annamaria De santis. cross experi in digit epigraphy: from practic to discipline. De gruyter, berlin, 2019.', 'creat high-qual print from tei document creat high-qual print from tei document even in the age of digit edit a need for high-qual print version of tei document remains, be it a print version of a singl document or a book edit of a complet or partial digit edition. Of course, there is xsl-fo but the typeset qualiti obtain by the freeli avail apach fop doe not live up to veri high demands. obviously, one can yield to one of the commerci xsl-fo engin which do a rather impress job. however, even the best xsl-fo engin are bound by the inher limit of xsl-fo which are crucial for critic editions, e.g. for format a complex text-crit apparatus. fortunately, there ha been a freeli avail typeset system sinc the earli 1980 whose qualiti is wide acknowledged, name latex and the underli tex. the use of latex for creat scholarli edit is therefor well-establish (cf. the reledmac packag and it predecessors). likewise, latex ha been utilis befor for typeset tei documents, e.g. within the xsl stylesheet for the tei framework. however, use xslt for transform tei document to variou format ha drawbacks, especi a rather high degre of redund and the consequenti pain of maintenance. luckili enough, the tei process model and odd come to our rescue. they provid mean for ad latex to the output format without too much hassle. use the tei publish librari complex requir can be easili realis within the tei odd in a descriptive, standardis way. thi paper demonstr how the portal of the law sourc foundat of the swiss lawyer societi utilis the tei process model and latex for prepar high-qual print versions. We will also propos two simpl extens to the tei process model, pb-templat and pb-behaviour, which make it easier to gener arbitrari latex from within the odd use a straightforward templat syntax As our implement proves, with those minim extens the process model becom power and gener enough to cover complex, high-qual print. authors: meier wolfgang, ruef bernhard', 'what is a line? encod and count line in earli modern dramat text what we consid to be a line, how we count and number them, and how we encod them so that they can be analyz and count are deepli signific and yet fraught decisions. line number are a key critic metric in earli modern drama. line are the domin measur of play and role length. major critic argument depend on whether and how much a charact speak in vers or prose; at the same time, editori relin is often base on the veri argument that in turn depend on lineation. complic the matter is that charact do not alway speak in full lines. part line are hard to identifi as prose or verse. multipl short utter by charact speak in success may be set on one typograph line. complic the mise-en-pag still further is the fact that prose wa often set as vers and vice versa to fit text to page space, as well as the fact that, as in modern typeset and digit interfaces, earli modern prose line break where the page or column width demands. line number are a common output of a scholarli edition. the 1623 first folio of shakespeare’ play ha even been subject to a copyright canon number system (through line numbers). yet all editori theori sinc mcgann ha emphas the instabl of such textual features. despit their utitil for citat and analyt purposes, line number can differ wildli between editions, precis becaus of the difficulti of establish line and the typograph variat between prose in differ formats. the tei offer three way of encod lines: we can locat it begin lb, describ it as a topograph line line, or identifi it as a line of vers l. We discuss the rel merit and implicit critic claim of each encod method for describ and count lines. We discuss the tension between citabl and fluid texts, outlin how we use the @ed and @edref attribut in our lineation, and introduc our prototyp wherebi variou lineat system are interoperable. the lemdo (link earli modern drama) prototyp allow for canon line identifiers, but doe not foreclos other possibilities; in fact, lemdo prolifer @xml:id so that project within the lemdo ecosystem can make project-level decis about what constitut a line. finally, we gestur toward futur reconcili of lineat system through linked-open data.', 'toward larger corpora of indic texts: for now, minim metatext toward larger corpora of indic texts: for now, minim metatext himal trikha (himal.trikha@oeaw.ac.at) the digit corpu of vidyānandin’ work (dcvw) is an ongo collect of digit text resourc for the work of a 10th centuri sanskrit author. the resourc are assembl and maintain in the context of my indolog research specialization, i.e., the histori of an indian philosoph tradition. A web interfac () allow to access the resourc and, to some extent, modifi them. the digit resourc are xml-file that are process by a bundl of technolog in order to pursu specif research interests: search for text strings, identif of dialog or intertextu elements, differ between attest etc. In thi context, the qualiti of the result depend on the qualiti of the resourc files, which are assess by three basic criteria: (a) statu of the separ of text and metatext, (b) qualiti of the captur text and (c) complianc of the metatext to an establish terminology. for the latter I use tei markup on basic two levels: (1) markup for a precis identif of the attest of the text and it specif editori featur and (2) markup to enrich the text from the perspect of my own research interests. the present will provid exampl for the appli markup. I will argu that the use of tag set within the first categori is certainli an indispens prerequisit for long term effort to build larger and larger corpora of indic texts. the tag set within the secondari category, on the other hand, seem to be of no relev for thi goal. the energi invest in the refin of technic demand tag set is an asset for scholar who are so inclined. In the current state of digit indology, however, it is still necessari to develop standard for the disciplin as a whole befor we can start to agre on the basic ones.', 'use github and it integr to create, test, and deploy a digit edit use github and it integr to create, test, and deploy a digit edit thi paper stem from the ongo work by the winnifr eaton archiv (wea), which seek to compile, transcribe, and encod the extant archiv of chinese-canadian author winnifr eaton (1875–1954). while there are mani framework for render tei onlin (includ the tei stylesheets, tei boilerplate, and ceteicean), the wea, like mani other project hous at institut without a dedic digit human infrastructure, struggl to find a framework for testing, deploying, and publish the project as a whole; omeka and wordpress were offer as solutions, but these framework are limit in their capac to handl tei-encod xml. follow the best practic outlin by the end project (carlin 2018) and inspir by the recent turn to static site for digit edit (holm 2017; viglianti 2017), minim edit (gil 2015; sayer 2016; gil 2017), and web publish at larg (rinaldi 2015), we arriv at the follow workflow: store all content on github integr travis-ci with repositori to build and valid product use travi to deploy to a separ github repositori that deploy content use the github page environ thi paper thu forward the abov method as a wide-ranging, afford solut for creat digit project in tei (it is entir free, minu the option cost of oxygen xml editor and a domain name) that is sustain and robust as it leverag exist technolog that are ubiquit and well-documented. thi process is also highli extens and can be use in concert with exist tei publish solutions, like tei boilerplate, to creat sustain and archiv static digit project that are not beholden to the structur limit of pre-exist content manag systems. our paper explain the major benefit of thi approach, which includ affordability, sustainability, and adaptability, as well as suggest the potenti of thi approach across variou pedagog and scholarli publish workflows.', 'introduc an open, dynam and effici lexic data access for tei-encod dictionari on the internet most of the tei-encod dictionari in public data repositori are not directli access for comput processing. their use by differ applic depend on how each applic process each singl dictionary. In the last decad direct comput access to data on the internet ha been provid through applic program interfac (apis). api provid a central access to data and if design and implement properly, an effici access to it. but api develop and mainten requir technic expertise, which can be an obstacl for small and medium dictionari publish that might not have in-hous solut for thi purpose. against thi background we have develop kosh, an open-sourc framework, that process ani xml-encod dictionari and creat two api for access the underli lexic data: A rest api (field 2019) and a graphql api. the purpos of thi present is to show how to use kosh with data publicli avail on github, in order to demonstr how the edit of digit dictionari and the compil of digital-born dictionari can be support with an effici access to the underli data via apis. kosh is an open-sourc framework develop to access multipl xml-encod dictionaries. It is gener and flexible, design to handl dictionari of differ structur and size with a minim configur effort. kosh process as input data in xml format that is pars and index into an elasticsearch server. In a json (javascript object notation) configur file, the path to the element to be index are defin In xpath 1.0 as also the elasticsearch datatyp of the field to be index e.g., keyword or text. finally, a kosh data modul requir a dot file (.kosh) contain the index name, the path to the xml file to be indexed, and the path to the configur file. with thi information, kosh index one or multipl xml file into one index that is access by two apis, a graphql and a rest api. If the xml sourc file are modified, the index is updat automatically. kosh can be deploy via docker or nativ on unix system and a singl kosh instanc can provid access to multipl dictionaries. In a github repository, kosh data , there are differ dataset that show the structur of a data modul for kosh. one of them contain the diccionario geográfico-histórico de la india occidental ó américa, a five-volum dictionari compil by antonio de alcedo (de alcedo 1786, 1787, 1788a, 1788b 1789), which offer a wide descript of american toponym and also, on it fifth volume, a vocabulari with a pioneer approach to descript word usag in the spanish america (lenz 1905–1910:7f). An xml version of thi dictionari ha been employ in digit gazett project such as hgi de la india and later in pelagio common base on thi xml-encod version we creat a tei-p5 compliant version. thi data can be access in two ways: first, through kosh data, where modif to the data can be propos through pull-requests. second, via api graphiql: swagger UI (rest): provid by a kosh instanc deploy with a clone of thi repository. As data modif are done at source-fil level, and the chang track with git , the edit process is open and also reversible. In kosh the publish defin which field should be indexed. for the digit of print dictionari thi mean that direct comput access can be provid with a coarse-grain encoding. when compil born-digit dictionari a few field can be avail at an earli compil stage and as the data gain complex more field can be ad to the index. thi flexibl approach to lexic data access allow to unveil dataset that are current hidden from comput applic and thu users.', 'parla-clarin: tei guidelin for corpora of parliamentari proceed parla-clarin: tei guidelin for corpora of parliamentari proceed tomaž erjavec and andrej pančur abstract: parliamentari proceed (pp) are a rich sourc of data use by e.g. scholar in historiography, sociology, polit science, linguistics, econom and econom history. As oppos to sourc of most other languag corpora, PP are not subject to copyright or person privaci protections, and are typic avail on-lin thu make them ideal for compil into corpora and open distribution. for these reason mani countri have alreadi produc PP corpora, but each typic in their own encoding, thu limit their compar and utilis in a multilingu setting. the talk will overview current approach to encod pp, with a focu on tei and tei-lik encodings, on akoma ntoso, a standard specif design for encod PP and other legisl documents, and on rdf, also a common approach to encod pps. We then motiv and propos a tei odd (so, a schema parametris and guidelines) for such corpora, base on the tei modul for transcript of speech. the work on thi parla-clarin recommend start with the “clarin parlaformat” workshop (cf. https://www.clarin.eu/blog/clarin-parlaformat-workshop) with select particip who present their own experi with encod parliamentari corpora and gave their comment to the draft propos by the authors. these comment have been larg taken into account, and the current parla-clarin recommend is avail at https://github.com/clarin-eric/parla-clarin. the git repositori contain the odd, the deriv html guidelin and xml schemas, and exampl documents. the recommend present the encod of PP metadata, includ the speaker and polit parties, the structur of the corpus, the encod of the speech and notes, linguist annot and multimedia. the talk conclud with discuss further work, esp. the provis of a set of exampl documents, the convers of akoma ntoso and rdf encod pp into parla-clarin and vice-versa, and other transform script that would operationalis the propos encoding.', 'challeng in encod parliamentari data: between applaus and interject challeng in encod parliamentari data: between applaus and interject parliamentari data ha alway been of great interest to research in the social scienc and the humanities. there are mani initi at european and nation level for compil digit collect of parliamentari data. however, these initi use differ encod scheme to present parliamentari data rang from ad hoc ones, over specif standard for repres legisl (e.g. akoma ntoso) to tei. akoma ntoso ha been creat to make the structur and semant compon of digit parliamentari document fulli access to machine-driven process https://unsceb-hlcm.github.io/part1/index-13.html (access 11.05.2019) . however, thi encod standard wa not design to includ linguist annotation. In thi regard tei is more suitable. the austrian parliamentari record corpus, parlat (wissik & pirker 2018), which wa befor onli avail in a vertic format suitabl for analysi in a corpu queri system, is now be encod in tei. In thi contribution, we will present the encount challenges, often relat to the fact, that the austrian parliamentari record are edit shorthand record of the parliamentari session and not transcript of recordings. the austrian parliamentari record includ a lot of comment within parenthesi and format differ than the other text, name in italics. these comment rang from indic applaus or laughter to indic interjections. We decid to encod these comment in differ way use the element from the transcript of speech module: to use <incident> for comment indic applaus or laughter and <u> (utterance) for comment indic interject part of thi research were support by a clarin mobil grant allow a research visit at the jožef stefan institut as part of clarin slovenia. . In the contribution, we will discuss the solut for encod such comments, regard the austrian case but also in relat to a more gener scheme propos by tomaž erjavec and andrej pančur, the teiparla https://www.clarin.eu/event/2019/parlaformat-workshop . refer wissik, T. and pirker, H. (2018). parlat beta corpu of austrian parliamentari records. in: D. fišer, M. eskevich, F. de jong, ed., lrec2018 workshop parlaclarin: creat and use parliamentari corpora in: proceed of the eleventh intern confer on languag resourc and evalu lrec2018. miyazaki : european languag resourc association, 20-23. keyword parliamentari records, tei, transcript of speech module, encod applause, encod interject', 'A realist theori of textual and it consequ on digit text represent the theoret debat about digit textual in the last decad ha been deepli influenc by the post-modernist theory. If thi influenc wa quit appar in the debat about the hypertext, we can find some of it fundament tenet also in text encod and digit scholarli edit theori (landow 1997; mcgann 2001). As a consequence, even in these area we can find a gener and strong support to anti-realist or constructivist notion about textual and it digit represent (patrick shale 2013a and 2013b; ciula and marra 2016). these view oscil from radic ontolog stances, epitom by thi notori mcgann’ sentence: what is text? I am not so naiv as to imagin that question could ever be final settled. ask such a question is like ask how long is the coast of england? (mcgann 2002); to weaker epistemolog or pragmatist stanc that advoc for the plural of the textual re-represent (the doubl re is due to the fact that a text is in itself a representation). In thi paper I propos a weak realist theori of (digital) textual (somehow along the line or the new realism movement in the recent philosoph debat (ferraris, bilgrami, and De caro 2012)) build on the theori of notat develop by nelson goodman in it languag of art (goodman 1968) and the theori of intent system devis by daniel dennet (dennet 1987 and for an introductori recapitul dennett 2009). In brief: text are spatiotempor artifact that have causal role in our cognit understanding. As dennet explains, we as a species, tend to adopt the intent stanc to explain the rational and function (or meaning) of complex systems, that consist in attribut mean and belief to those artifacts. document are prima faci interpret as intent artifact that convey meanings. At thi level what realli count is the possibl to identifi and fix the notat natur of the text, becaus it’ primarili that notat natur that ha a causal role in the chain that start form percept and end in the cognit work of interpret (mean attribution). the gener natur of the notat system ha been analyz by anoth philosoph of the preced generation, nelson goodman. In hi influent book (for mani and divers reasons) the languag of art, he affirm that textual artifact have the distinct properti to be in a definit notation, consist of certain sign or charact that are to be combin by concaten (goodman 1968, 116); thi notat condit provid a principium individuationi for the text. hence, a (digital) represent of a text is adequ if and onli if it ha the properti of have the same of spelling: the exact correspond as sequenc of letters, spaces, punctuat marks. all the other properti are either conting (if they are material) or deriv (all the cognit object produc in the act of reading). Of course, one can or will tri to adopt the other two level of explan identifi by dennett (the design stanc and the physic stance, that can be translat in our context into variou technic approach in textual studies), but thi happen onli for limit occas and in particular conditions. move from thi gener theoret account most of the recur debat about the pro and con of one encod metalanguag or digit model strategi over anoth can be refram into a moder pluralist methodolog framework, where the uniqu central point is the correct (a far as possible) represent of the charact sequenc of the textual artifact that are gener use as text-docu of a specif text-work, to which all other properti can be attached. I tend to conceiv a full-fledg stand-off markup strategi as the most natur way of give a comput model of the textual reality, but other strategi and notat approach (like xml inlin markup) can have technical, pragmat and cultur afford that, under mani respects, are advantageous.']\n"
     ]
    }
   ],
   "source": [
    "#vectorize the text i.e. convert the strings to numeric features\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(documents)\n",
    "print(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuite, on réunit les termes vectorisés en cluster, c'est à dire en groupe de ressemblance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=100,\n",
       "       n_clusters=3, n_init=1, n_jobs=None, precompute_distances='auto',\n",
       "       random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cluster documents, ici 10 clusters\n",
    "true_k = 4\n",
    "model = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "model.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enfin, on montre les termes que l'ordinateur a pu rassembler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "Cluster 0:\n",
      " text\n",
      " parliamentari\n",
      " graph\n",
      " stage\n",
      " texts\n",
      " tei\n",
      " markup\n",
      " indic\n",
      " tibetan\n",
      " direct\n",
      "Cluster 1:\n",
      " tei\n",
      " data\n",
      " thi\n",
      " encod\n",
      " edit\n",
      " project\n",
      " text\n",
      " line\n",
      " digit\n",
      " xml\n",
      "Cluster 2:\n",
      " tei\n",
      " paper\n",
      " text\n",
      " odd\n",
      " use\n",
      " thi\n",
      " encod\n",
      " format\n",
      " xml\n",
      " element\n"
     ]
    }
   ],
   "source": [
    "#print top terms per cluster clusters\n",
    "print(\"Top terms per cluster:\")\n",
    "order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names()\n",
    "for i in range(true_k):\n",
    "    print(\"Cluster %d:\" % i)\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(' %s' % terms[ind])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
