{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEI Conference 2019 #\n",
    "\n",
    "## Etape 3 : L'Analyse ##\n",
    "\n",
    "### Préambule ###\n",
    "\n",
    "L'objectif présent est d'analyser les abstracts afin de les regrouper en cluster, permettant de dégager des thématiques, de comparer les différentes conférences ou bien d'étudier si les distinctions faites par les organisateurs de la conférence 2019 entre les différentes interventions est bien pertinente.\n",
    "Nous pourrons également critiquer les choix de l'ordinateur.\n",
    "\n",
    "Nous allons faire ici de la classification non supervisée grâce au package SKLearn, librairie dédiée au Machine Learning notamment.\n",
    "\n",
    "### Les packages ### \n",
    "\n",
    "Il faut lancer la cellule ci-dessous une seule fois afin de télécharger la librairie sklearn. Cela doit être fait une seule fois, au premier lancement. Ensuite, il ne faudra plus jamais le lancer. Il est possible qu'il faille redémarrer le noyau après."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install sklearn\n",
    "pip install nltk\n",
    "pip install pandas\n",
    "pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.0.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il faut lancer la cellule ci-dessous à chaque lancement de ce notebook-ci."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "import os\n",
    "import spacy\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import sklearn\n",
    "import sys\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import ldamodel\n",
    "import gensim.corpora\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.preprocessing import normalize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D'abord, on crée le répertoire dans le cache du contenu qui recevra les textes lemmatisés :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path = \"./cache2019/cacheTXT/\" #chemin permettant d'accès aux abstracts\n",
    "Path_output = \"./cache2019/cacheLEM/\" #chemin de sortie des abstracts stemmés\n",
    "filelist = os.listdir(Path) #filelist est une liste regroupant tous les chemins vers les différents abstracts.\n",
    "if not os.path.exists(\"./cache2019/cacheLEM/\"): #permet de créer un dossier dans le cache s'il est supprimé.\n",
    "    os.makedirs(\"./cache2019/cacheLEM/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Méthode 1 : tokenisation et lemmatisation ####\n",
    "\n",
    "Je propose ci-dessous de lemmatizer le texte avec spaCy. Ma lemmatisation va retourner une forme canonique de chaque mot. La tokenisation et la lemmatisation s'appuient sur un petit corpus que j'ai importé (en_core_web_sm) depuis la base de spaCy. En entrée se trouve le texte normalisé de chaque abstract, et en sortie nous retrouvons ce même texte, mais où chaque mot est remplacé par son lem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for abstract in filelist:\n",
    "    with open(Path + abstract, \"r\", encoding=\"UTF-8\") as y:\n",
    "        texte = y.read()\n",
    "        nlp = spacy.load(\"en_core_web_sm\") #nlp est un corpus de test permettant de tokenizer efficacement l'anglais à partir d'un petit corpus (j'ai choisi le petit corpus, pas le gros)\n",
    "        doc = nlp(texte) #doc est le texte annoté. Ce n'est pas une str pour autant\n",
    "        liste_mots_lemmatise = [] # j'instancie une liste qui accueillera chaque lemme (le lemme étant une str)\n",
    "        for token in doc:\n",
    "            liste_mots_lemmatise.append(token.lemma_) #token.lemma_ est une str. Cette méthode tokenise puis lematise. Il crée une liste qu'on va joindre ensuite.\n",
    "        resultat = ' '.join(liste_mots_lemmatise)\n",
    "        with open(Path_output + abstract, \"w\", encoding=\"UTF-8\") as z:\n",
    "            z.write(resultat) #resultat est une str\n",
    "         \n",
    "\n",
    "#je peux demander de l'aide pour comprendre une doc\n",
    "#pour demain 9h du matin, il faut que j'écrive une définition de tokenisation, lemmatisation et POS taging, leur impact et leur importance. Demain à 11h (9h métropole) je le montre à Clérice\n",
    "#Pour lundi 9h, me farcir les polycopiés (page 31 à 38 du polycop à 38 pages). Il faut aussi lire l'article que Clérice m'a envoyé.\n",
    "#Regarder ce qu'est le topic modeling (en non supervisé), essayez de trouver un tuto là dessus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Méthode 2 : tokenization + stemmisation ####\n",
    "\n",
    "Dans cette seconde méthode, le texte est tokenisé (grâce à spaCy toujours) mais il est stemmé et non pas lemmatisé. Le stemming consiste à retirer les terminaisons pour garder une forme canonique de chaque mot. Le fonctionnement est identique à celui de la lemmatisation, pour ma démarche. Le résultat est également stocké dans un cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path = \"./cache2019/cacheTXT/\" #chemin permettant d'accès aux abstracts\n",
    "Path_output = \"./cache2019/cacheSTEM/\" #chemin de sortie des abstracts stemmés\n",
    "filelist = os.listdir(Path) #filelist est une liste regroupant tous les chemins vers les différents abstracts.\n",
    "if not os.path.exists(\"./cache2019/cacheSTEM/\"): #permet de créer un dossier dans le cache s'il est supprimé.\n",
    "    os.makedirs(\"./cache2019/cacheSTEM/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer() #J'importe une méthode acceptant une chaîne de caractère à une variable.\n",
    "for abstract in filelist:\n",
    "    with open(Path + abstract, \"r\", encoding=\"UTF-8\") as y:\n",
    "        texte = y.read()\n",
    "        liste_mots_tokenise = [] # j'instancie une liste qui accueillera chaque lemme (le lemme étant une str)\n",
    "        liste_mots_a_tokenise = texte.split() #je crée une liste sur laquelle je vais pouvoir boucler\n",
    "        for elem in liste_mots_a_tokenise:\n",
    "            mot_tokenise = stemmer.stem(elem) #mot_tokenise est une str, et est le lemme du mot sur lequel je boucle\n",
    "            liste_mots_tokenise.append(mot_tokenise) #j'ajoute chaque mot à une liste, qu'ensuite je join pour recréer l'abstract sous la forme d'une str\n",
    "        resultat = ' '.join(liste_mots_tokenise)\n",
    "        with open(Path_output + abstract, \"w\", encoding=\"UTF-8\") as z:\n",
    "            z.write(resultat) #resultat est une str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuite, il faut choisir quel set sera loadé et analysé entre les abstracts lemmatisés et les abstracts loadés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pour utiliser les textes lemmatisés, c'est ici\n",
    "\n",
    "documents = []\n",
    "Path = \"./cache2019/cacheLEM/\"\n",
    "filelist = os.listdir(Path) #filelist est une liste regroupant tous les chemins vers les différents abstracts.\n",
    "\n",
    "for abstract in filelist:\n",
    "    with open(Path + abstract, \"r\", encoding=\"UTF-8\") as y:\n",
    "        texte = y.read()\n",
    "        documents.append(texte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pour utiliser les textes stemmés, c'est ici\n",
    "\n",
    "documents = []\n",
    "Path = \"./cache2019/cacheSTEM/\"\n",
    "filelist = os.listdir(Path) #filelist est une liste regroupant tous les chemins vers les différents abstracts.\n",
    "\n",
    "for abstract in filelist:\n",
    "    with open(Path + abstract, \"r\", encoding=\"UTF-8\") as y:\n",
    "        texte = y.read()\n",
    "        documents.append(texte)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "## Définir les clusters ##\n",
    "\n",
    "Après avoir tokennisé et lemmatisé/stemmé nos abstracts, nous allons maintenant les vectoriser (représenter chaque mot sous la forme d'un vecteur sur un plan ordonné en deux dimensions) puis ensuite nous allons laisser la machine définir des clusters, c'est à dire des groupes de mots qui ont un sens commun.\n",
    "\n",
    "Ici, on associe à chaque mot de tous les abstracts une coordonnée unique à chaque mot utilisé, pour ensuite pouvoir les placer sur un plan en 2D et ainsi relever des ressemblances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorize the text i.e. convert the strings to numeric features\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuite, on réunit les termes vectorisés en cluster, c'est à dire en groupe de ressemblance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=100,\n",
       "       n_clusters=8, n_init=1, n_jobs=None, precompute_distances='auto',\n",
       "       random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cluster documents, ici 8 clusters\n",
    "true_k = 8\n",
    "model = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "model.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enfin, on montre les termes que l'ordinateur a pu rassembler ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "Cluster 0:\n",
      " token\n",
      " hi\n",
      " tokenized\n",
      " tei\n",
      " tokenize\n",
      " break\n",
      " pb\n",
      " unary\n",
      " searchable\n",
      " cwb\n",
      "Cluster 1:\n",
      " correspondence\n",
      " bibliographic\n",
      " cmif\n",
      " letter\n",
      " tei\n",
      " correspsearch\n",
      " hallernet\n",
      " metadata\n",
      " pron\n",
      " jtei\n",
      "Cluster 2:\n",
      " pron\n",
      " text\n",
      " tei\n",
      " use\n",
      " paper\n",
      " encode\n",
      " corpus\n",
      " element\n",
      " odd\n",
      " edition\n",
      "Cluster 3:\n",
      " austria\n",
      " bavarian\n",
      " dialect\n",
      " bowers\n",
      " creation\n",
      " breuer\n",
      " stöckle\n",
      " dictionary\n",
      " article\n",
      " tei\n",
      "Cluster 4:\n",
      " kosh\n",
      " dictionary\n",
      " api\n",
      " access\n",
      " index\n",
      " datum\n",
      " data\n",
      " file\n",
      " xml\n",
      " path\n",
      "Cluster 5:\n",
      " pron\n",
      " tei\n",
      " line\n",
      " project\n",
      " rhyme\n",
      " poem\n",
      " use\n",
      " digital\n",
      " xml\n",
      " build\n",
      "Cluster 6:\n",
      " sales\n",
      " catalogues\n",
      " structuring\n",
      " automatic\n",
      " scale\n",
      " manuscript\n",
      " 打球場\n",
      " entities\n",
      " entire\n",
      " entirely\n",
      "Cluster 7:\n",
      " punctuation\n",
      " sign\n",
      " pron\n",
      " manuscript\n",
      " medieval\n",
      " interpretative\n",
      " modern\n",
      " transformation\n",
      " edition\n",
      " transcription\n"
     ]
    }
   ],
   "source": [
    "#print top terms per cluster clusters\n",
    "print(\"Top terms per cluster:\")\n",
    "order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names()\n",
    "for i in range(true_k):\n",
    "    print(\"Cluster %d:\" % i)\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(' %s' % terms[ind])\n",
    "        \n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster de documents ##\n",
    "\n",
    "Grâce à un système de prédiction découlant de l'apprentissage et de l'entraînement sur les termes des abstracts, nous pouvons classifier les abstracts en 8 catégories. Nous pourrons ensuite les comparer avec le classement déjà fait par les organisateurs de la conférence TEI 2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dans le groupe 1, il y a :\n",
      "\t Advantages and challenges of tokenized TEI\n",
      "\n",
      " Dans le groupe 2, il y a :\n",
      "\t correspSearch v2 – New ways of exploring correspondence\n",
      "\t Modeling FRBR Entities and their Relationships with TEI: A Look at HallerNet Bibliographic Descriptions\n",
      "\t Referencing annotations as a core concept of the hallerNet edition and research platform\n",
      "\t TEI encoding of correspondence: A community effort\n",
      "\n",
      " Dans le groupe 3, il y a :\n",
      "\t Using Machine Learning for the Automated Classification of Stage Directions in TEI-Encoded Drama Corpora\n",
      "\t TEI XML and Delta Format Interchangeability\n",
      "\t Introducing Objectification: when is an <object> a <place>?\n",
      "\t Analyzing and Visualizing Uncertain Knowledge: Introducing the PROVIDEDH Open Science Platform\n",
      "\t Referencing an editorial ontology from the TEI: An attempt to overcome informal typologies\n",
      "\t Case Study TEI Customization: A Restricted TEI Format for Edition Open Access (EOA)\n",
      "\t In search of comity: TEI for distant reading\n",
      "\t Reconceiving TEI models of theatrical performance text with reference to promptbooks\n",
      "\t Recreating history through events\n",
      "\t Highlighting Our Examples: encoding XML examples in pedagogical contexts\n",
      "\t Growing collections of TEI texts: Some lessons from SARIT\n",
      "\t A TEI customization for the description of paper and watermarks\n",
      "\t Exploring TEI structures to find distinctive features of text types\n",
      "\t An Encoding Strategic Proposal of “Ruby” Texts: Examples from Japanese Texts\n",
      "\t An Attempt of Dissemination of TEI in a TEI-underdeveloped country: Activities of the SIG EAJ\n",
      "\t Encoding history in TEI: A corpus-oriented approach for investigating Tibetan historiography\n",
      "\t Text Graph Ontology. A Semantic Web approach to represent genetic scholarly editions\n",
      "\t Genesis and Variance: From Letter to Literature\n",
      "\t Five Centuries of History in a Network\n",
      "\t Reflecting the Influence of Technology on Models of Text in Scholarly Digital Editing\n",
      "\t Using Microsoft Word for preparing XML TEI-compliant digital editions\n",
      "\t Inscriptions, Hieroglyphs, Linguistics… and Beyond! The Corpus of Classic Mayan as an Ontological Information Resource\n",
      "\t Creating high-quality print from TEI documents\n",
      "\t Towards larger corpora of Indic texts: For now, minimize metatext\n",
      "\t Parla-CLARIN: TEI guidelines for corpora of parliamentary proceedings\n",
      "\t Challenges in encoding parliamentary data: between applause and interjections\n",
      "\t A realistic theory of textuality and its consequences on digital text representation\n",
      "\n",
      " Dans le groupe 4, il y a :\n",
      "\t Native-TEI dialectal dictionary for Bavarian dialects in Austria: data structure, software and workflow\n",
      "\n",
      " Dans le groupe 5, il y a :\n",
      "\t Introducing an Open, Dynamic and Efficient Lexical Data Access for TEI-encoded Dictionaries on the Internet\n",
      "\n",
      " Dans le groupe 6, il y a :\n",
      "\t Getting Along with Relational Databases\n",
      "\t How we tripled our encoding speed in the \n",
      "\t Making Linkable Data from Account Books: Bookkeeping Ontology in the Digital Edition Publishing Cooperative for Historical Accounts\n",
      "\t The Prefabricated Website: Who Needs a Server Anyway?\n",
      "\t Refining the Current Teaching Methodology of the TEI through the Analysis of Server Logs\n",
      "\t Manuscripta - The editor from past to future\n",
      "\t Archiving a TEI project FAIRly\n",
      "\t Validating @selector: a regular expression adventure\n",
      "\t What is a Line? Encoding and Counting Lines in Early Modern Dramatic Texts\n",
      "\t Using Github and its Integrations to Create, Test, and Deploy a Digital Edition\n",
      "\n",
      " Dans le groupe 7, il y a :\n",
      "\t Scaling up Automatic Structuring of Manuscript Sales Catalogues\n",
      "\n",
      " Dans le groupe 8, il y a :\n",
      "\t A sign of the times: medieval punctuation, its encoding and its rendition in modern times\n"
     ]
    }
   ],
   "source": [
    "Path = \"./cache2019/cacheLEM/\" #On peut remplacer le cache STEM par LEM à la place, pour ainsi comparer les deux sorties.\n",
    "filelist = os.listdir(Path) #filelist est une liste regroupant tous les chemins vers les différents abstracts.\n",
    "liste_triee = [] #c'est la liste contenant les infos triées de tous les abstracts\n",
    "\n",
    "\n",
    "for abstract in filelist:\n",
    "    reference = abstract.replace('.txt', '') #je normalise le nom du texte pour le faire correpondre avec celui indiqué dans le tableau csv réunissant toutes les informations\n",
    "    with open(Path + abstract, \"r\", encoding=\"UTF-8\") as y:\n",
    "        liste_resultat_unitaire = [] #j'instancie la liste des infos propres à chaque abstract\n",
    "        texte = y.read()\n",
    "        X = vectorizer.transform([texte]) #je vectorise le texte de l'abstract\n",
    "        predicted = model.predict(X)  #j'attribue l'abstract à un cluster. predicted est de la classe numpy array : c'est un vecteur\n",
    "        with open('./cache2019/TEI2019.csv', 'r') as csvfile:\n",
    "            read = csv.reader(csvfile, delimiter = ',')\n",
    "            titre = ''\n",
    "            auteurs = ''\n",
    "            for row in read:\n",
    "                if row[4] == reference:\n",
    "                    titre = row[2]\n",
    "                    auteurs = row[0]\n",
    "        liste_resultat_unitaire.append(titre)\n",
    "        liste_resultat_unitaire.append(auteurs)\n",
    "        liste_resultat_unitaire.append(int(predicted))\n",
    "        liste_triee.append(liste_resultat_unitaire)\n",
    "        \n",
    "\n",
    "print(\"Dans le groupe 1, il y a :\")\n",
    "for elem in liste_triee:\n",
    "    if elem[2] == 0:\n",
    "        print(\"\\t\", elem[0] )\n",
    "        \n",
    "print(\"\\n Dans le groupe 2, il y a :\")\n",
    "for elem in liste_triee:\n",
    "    if elem[2] == 1:\n",
    "        print(\"\\t\", elem[0] )\n",
    "        \n",
    "print(\"\\n Dans le groupe 3, il y a :\")\n",
    "for elem in liste_triee:\n",
    "    if elem[2] == 2:\n",
    "        print(\"\\t\", elem[0] )\n",
    "\n",
    "print(\"\\n Dans le groupe 4, il y a :\")\n",
    "for elem in liste_triee:\n",
    "    if elem[2] == 3:\n",
    "        print(\"\\t\", elem[0] )\n",
    "\n",
    "print(\"\\n Dans le groupe 5, il y a :\")\n",
    "for elem in liste_triee:\n",
    "    if elem[2] == 4:\n",
    "        print(\"\\t\", elem[0] )\n",
    "        \n",
    "print(\"\\n Dans le groupe 6, il y a :\")\n",
    "for elem in liste_triee:\n",
    "    if elem[2] == 5:\n",
    "        print(\"\\t\", elem[0] )\n",
    "        \n",
    "print(\"\\n Dans le groupe 7, il y a :\")\n",
    "for elem in liste_triee:\n",
    "    if elem[2] == 6:\n",
    "        print(\"\\t\", elem[0] )\n",
    "        \n",
    "print(\"\\n Dans le groupe 8, il y a :\")\n",
    "for elem in liste_triee:\n",
    "    if elem[2] == 7:\n",
    "        print(\"\\t\", elem[0] )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voici le plan et l'agencement des conférences tel qu'il est indiqué ici : https://graz-2019.tei-c.org/wp-content/uploads/2019/09/ProgrammheftFINAL.pdf\n",
    "\n",
    "TEI, formal ontologies,controlled vocabularies and Linked OpenData :\n",
    "- Making Linkable Data from Account Books: Bookkeeping Ontology in the Digital Edition Publishing Cooperative for Historical Accounts\n",
    "- Inscriptions, Hieroglyphs, Linguistics and Beyond! The Corpus of Classic Mayan as an Ontological Information Resource\n",
    "- Modeling FRBR Entities and their Relationships with TEI: A Look at HallerNet Bibliographic Descriptions\n",
    "- Referencing an editorial ontology from the TEI: An attempt to overcome informal typologies\n",
    "- Text Graph Ontology. A Semantic Web approach to represent genetic scholarly editions\n",
    "\n",
    "TEI and models of text :\n",
    "- A realistic theory of textuality and its consequences on digital text representation\n",
    "- Genesis and Variance: From Letter to Literature\n",
    "- Between freedom and formalisation: a hypergraph model for representing the nature of text\n",
    "- Reflecting the Influence of Technology on Models of Text in Scholarly Digital Editing\n",
    "- Introducing Objectification: when is an <object> a <place> ?\n",
    "- An Encoding Strategic Proposal of “Ruby” Texts: Examples from Japanese Texts\n",
    "- Referencing annotations as a core concept of the hallerNet edition and research platform\n",
    "- Recreating history through events\n",
    "- Document Modeling with the TEI Critical Apparatus\n",
    "- Exploring TEI structures to find distinctive features of text types\n",
    "- Reconceiving TEI models of theatrical performance text with reference to promptbooks\n",
    "- What is a Line? Encoding and Counting Lines in Early Modern Dramatic Texts\n",
    "    \n",
    "TEI across corpora,languages, and cultures :\n",
    "- Growing collections of TEI texts: Some lessons from SARIT\n",
    "- Towards larger corpora of Indic texts: For now, minimize metatext\n",
    "- Encoding history in TEI: A corpus-oriented approach for investigating Tibetan historiography\n",
    "- Advantages and challenges of tokenized TEI\n",
    "- A sign of the times: medieval punctuation, its encoding and its rendition in modern times\n",
    "\n",
    "TEI annotation and publication :\n",
    "- Analyzing and Visualizing Uncertain Knowledge: Introducing the PROVIDEDH Open Science Platform\n",
    "- The Prefabricated Website: Who Needs a Server Anyway?\n",
    "- correspSearch v2 –New ways of exploring correspondence\n",
    "- Validating @selector: a regular expression adventure\n",
    "- TEI encoding of correspondence: A community effort\n",
    "\n",
    "TEI simplification and extension :\n",
    "- Opportunities and challenges of the TEI for scholarly journals in the Humanities\n",
    "- Archiving a TEI project FAIRly\n",
    "- Creating high-quality print from TEI documents\n",
    "- Native-TEI dialectal dictionary for Bavarian dialects in Austria: data structure, software and workflow\n",
    "- An Attempt of Dissemination of TEI in a TEI-underdeveloped country: Activities of the SIG EAJ\n",
    "- Refining the Current Teaching Methodology of the TEI through the Analysis of Server Logs\n",
    "- Using Github and its Integrations to Create, Test, and Deploy a Digital Edition\n",
    "\n",
    "TEI environments and infrastructures :\n",
    "- Parla-CLARIN: TEI guidelines for corpora of parliamentary proceedings\n",
    "- Challenges in encoding parliamentary data: between applause and interjections\n",
    "- A TEI customization for the description of paper and watermarks\n",
    "- How we tripled our encoding speed in the Digital Victorian Periodical Project\n",
    "- Manuscripta-The editor from past to future\n",
    "- Highlighting Our Examples: encoding XML examples in pedagogical contexts\n",
    "- Case Study TEI Customization: A Restricted TEI Format for Edition Open Access (EOA)\n",
    "- In search of comity: TEI for distant reading\n",
    "\n",
    "TEI and beyond :interactions, interchange, integrations and interoperability :\n",
    "- Using Machine Learning for the Automated Classification of Stage Directions in TEI-Encoded Drama Corpora\n",
    "- TEI XML and Delta Format Interchangeability\n",
    "\n",
    "TEI and non-XML technologies :\n",
    "- Five Centuries of History in a Network\n",
    "- Introducing an Open, Dynamic and Efficient Lexical Data Access for TEI-encoded Dictionaries on the Internet\n",
    "- Getting Along with Relational Databases\n",
    "- Using Microsoft Word for preparing XML TEI-compliant digital editions\n",
    "- Scaling up Automatic Structuring of Manuscript Sales Catalogues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ne pas le lancer, je stock ici l'exemple pour clusteriser des documents\n",
    "for abstract in documents:\n",
    "    X = vectorizer.transform([abstract])\n",
    "    predicted = model.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modelling ###\n",
    "\n",
    "Ensuite, je vais tenter de déterminer des sujets aux cluster, c'est à dire déterminer grâce aux mots de chaque cluster des thèmes propres à chaque cluster grâce à une étude de chacun des termes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/formation1/snap/jupyter/common/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:1466: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  idf = np.log(n_samples / df) + 1\n"
     ]
    }
   ],
   "source": [
    "transformer = TfidfTransformer(smooth_idf=False)\n",
    "x_tfidf = transformer.fit_transform(X)\n",
    "xtfidf_norm = normalize(x_tfidf, norm='l1', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/formation1/snap/jupyter/common/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:109: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return np.sqrt(res * 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NMF(alpha=0.0, beta_loss='frobenius', init=None, l1_ratio=0.0, max_iter=200,\n",
       "    n_components=8, random_state=None, shuffle=False, solver='cd', tol=0.0001,\n",
       "    verbose=0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_topics = 8 #définition du nombre de topics\n",
    "model = NMF(n_components=num_topics) \n",
    "model.fit(xtfidf_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic # 01</th>\n",
       "      <th>Topic # 02</th>\n",
       "      <th>Topic # 03</th>\n",
       "      <th>Topic # 04</th>\n",
       "      <th>Topic # 05</th>\n",
       "      <th>Topic # 06</th>\n",
       "      <th>Topic # 07</th>\n",
       "      <th>Topic # 08</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>stance</td>\n",
       "      <td>point</td>\n",
       "      <td>far</td>\n",
       "      <td>physical</td>\n",
       "      <td>theory</td>\n",
       "      <td>perception</td>\n",
       "      <td>facie</td>\n",
       "      <td>notational</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>artifact</td>\n",
       "      <td>notion</td>\n",
       "      <td>ask</td>\n",
       "      <td>advocate</td>\n",
       "      <td>1987</td>\n",
       "      <td>finally</td>\n",
       "      <td>theory</td>\n",
       "      <td>realist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>notational</td>\n",
       "      <td>possibility</td>\n",
       "      <td>try</td>\n",
       "      <td>sameness</td>\n",
       "      <td>act</td>\n",
       "      <td>building</td>\n",
       "      <td>2009</td>\n",
       "      <td>property</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>theory</td>\n",
       "      <td>approach</td>\n",
       "      <td>physical</td>\n",
       "      <td>notion</td>\n",
       "      <td>punctuation</td>\n",
       "      <td>methodological</td>\n",
       "      <td>inline</td>\n",
       "      <td>pron</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pron</td>\n",
       "      <td>interpret</td>\n",
       "      <td>study</td>\n",
       "      <td>notorious</td>\n",
       "      <td>notorious</td>\n",
       "      <td>notational</td>\n",
       "      <td>framework</td>\n",
       "      <td>text</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>goodman</td>\n",
       "      <td>recur</td>\n",
       "      <td>property</td>\n",
       "      <td>1987</td>\n",
       "      <td>hypertext</td>\n",
       "      <td>1968</td>\n",
       "      <td>2013b</td>\n",
       "      <td>far</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>representation</td>\n",
       "      <td>attribution</td>\n",
       "      <td>explanation</td>\n",
       "      <td>settle</td>\n",
       "      <td>recent</td>\n",
       "      <td>daniel</td>\n",
       "      <td>devise</td>\n",
       "      <td>constructivist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>textuality</td>\n",
       "      <td>brief</td>\n",
       "      <td>fundamental</td>\n",
       "      <td>reading</td>\n",
       "      <td>mcgann</td>\n",
       "      <td>anti</td>\n",
       "      <td>fix</td>\n",
       "      <td>devise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>debate</td>\n",
       "      <td>belief</td>\n",
       "      <td>language</td>\n",
       "      <td>brief</td>\n",
       "      <td>various</td>\n",
       "      <td>fix</td>\n",
       "      <td>2013a</td>\n",
       "      <td>rationale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>property</td>\n",
       "      <td>nelson</td>\n",
       "      <td>notion</td>\n",
       "      <td>encode</td>\n",
       "      <td>decade</td>\n",
       "      <td>area</td>\n",
       "      <td>complex</td>\n",
       "      <td>notorious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>cognitive</td>\n",
       "      <td>provide</td>\n",
       "      <td>notorious</td>\n",
       "      <td>realistic</td>\n",
       "      <td>representation</td>\n",
       "      <td>bilgrami</td>\n",
       "      <td>book</td>\n",
       "      <td>language</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>intentional</td>\n",
       "      <td>naive</td>\n",
       "      <td>propose</td>\n",
       "      <td>landow</td>\n",
       "      <td>derivative</td>\n",
       "      <td>tend</td>\n",
       "      <td>bilgrami</td>\n",
       "      <td>strategy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>dennet</td>\n",
       "      <td>generally</td>\n",
       "      <td>encode</td>\n",
       "      <td>work</td>\n",
       "      <td>2013a</td>\n",
       "      <td>realist</td>\n",
       "      <td>primarily</td>\n",
       "      <td>landow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>text</td>\n",
       "      <td>debate</td>\n",
       "      <td>notation</td>\n",
       "      <td>textuality</td>\n",
       "      <td>reason</td>\n",
       "      <td>fledged</td>\n",
       "      <td>sameness</td>\n",
       "      <td>really</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>mcgann</td>\n",
       "      <td>cultural</td>\n",
       "      <td>notational</td>\n",
       "      <td>way</td>\n",
       "      <td>ask</td>\n",
       "      <td>textual</td>\n",
       "      <td>condition</td>\n",
       "      <td>1968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>digital</td>\n",
       "      <td>pragmatic</td>\n",
       "      <td>reframe</td>\n",
       "      <td>notational</td>\n",
       "      <td>pron</td>\n",
       "      <td>develop</td>\n",
       "      <td>nelson</td>\n",
       "      <td>mark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>textual</td>\n",
       "      <td>definite</td>\n",
       "      <td>contingent</td>\n",
       "      <td>ferraris</td>\n",
       "      <td>reading</td>\n",
       "      <td>oscillate</td>\n",
       "      <td>account</td>\n",
       "      <td>recapitulation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>strategy</td>\n",
       "      <td>reality</td>\n",
       "      <td>support</td>\n",
       "      <td>philosophical</td>\n",
       "      <td>level</td>\n",
       "      <td>shale</td>\n",
       "      <td>recapitulation</td>\n",
       "      <td>debate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>nature</td>\n",
       "      <td>support</td>\n",
       "      <td>ciula</td>\n",
       "      <td>modernist</td>\n",
       "      <td>devise</td>\n",
       "      <td>rationale</td>\n",
       "      <td>deeply</td>\n",
       "      <td>certain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>weak</td>\n",
       "      <td>2002</td>\n",
       "      <td>epitomize</td>\n",
       "      <td>naive</td>\n",
       "      <td>encoding</td>\n",
       "      <td>modeling</td>\n",
       "      <td>mcgann</td>\n",
       "      <td>settle</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Topic # 01   Topic # 02   Topic # 03     Topic # 04      Topic # 05  \\\n",
       "0           stance        point          far       physical          theory   \n",
       "1         artifact       notion          ask       advocate            1987   \n",
       "2       notational  possibility          try       sameness             act   \n",
       "3           theory     approach     physical         notion     punctuation   \n",
       "4             pron    interpret        study      notorious       notorious   \n",
       "5          goodman        recur     property           1987       hypertext   \n",
       "6   representation  attribution  explanation         settle          recent   \n",
       "7       textuality        brief  fundamental        reading          mcgann   \n",
       "8           debate       belief     language          brief         various   \n",
       "9         property       nelson       notion         encode          decade   \n",
       "10       cognitive      provide    notorious      realistic  representation   \n",
       "11     intentional        naive      propose         landow      derivative   \n",
       "12          dennet    generally       encode           work           2013a   \n",
       "13            text       debate     notation     textuality          reason   \n",
       "14          mcgann     cultural   notational            way             ask   \n",
       "15         digital    pragmatic      reframe     notational            pron   \n",
       "16         textual     definite   contingent       ferraris         reading   \n",
       "17        strategy      reality      support  philosophical           level   \n",
       "18          nature      support        ciula      modernist          devise   \n",
       "19            weak         2002    epitomize          naive        encoding   \n",
       "\n",
       "        Topic # 06      Topic # 07      Topic # 08  \n",
       "0       perception           facie      notational  \n",
       "1          finally          theory         realist  \n",
       "2         building            2009        property  \n",
       "3   methodological          inline            pron  \n",
       "4       notational       framework            text  \n",
       "5             1968           2013b             far  \n",
       "6           daniel          devise  constructivist  \n",
       "7             anti             fix          devise  \n",
       "8              fix           2013a       rationale  \n",
       "9             area         complex       notorious  \n",
       "10        bilgrami            book        language  \n",
       "11            tend        bilgrami        strategy  \n",
       "12         realist       primarily          landow  \n",
       "13         fledged        sameness          really  \n",
       "14         textual       condition            1968  \n",
       "15         develop          nelson            mark  \n",
       "16       oscillate         account  recapitulation  \n",
       "17           shale  recapitulation          debate  \n",
       "18       rationale          deeply         certain  \n",
       "19        modeling          mcgann          settle  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_nmf_topics(model, n_top_words):\n",
    "    \n",
    "    #il faut obtenir le mot à partir de son vecteur identifiant pour afficher les termes de chaque sujet\n",
    "    feat_names = vectorizer.get_feature_names()\n",
    "    \n",
    "    word_dict = {}\n",
    "    for i in range(num_topics):\n",
    "        \n",
    "        #for each topic, obtain the largest values, and add the words they map to into the dictionary.\n",
    "        words_ids = model.components_[i].argsort()[:-20 - 1:-1]\n",
    "        words = [feat_names[key] for key in words_ids]\n",
    "        word_dict['Topic # ' + '{:02d}'.format(i+1)] = words;\n",
    "    \n",
    "    return pd.DataFrame(word_dict)\n",
    "get_nmf_topics(model, 20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
