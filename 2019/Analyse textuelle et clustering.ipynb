{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEI Conference 2019 #\n",
    "\n",
    "## Etape 3 : L'Analyse ##\n",
    "\n",
    "### Préambule ###\n",
    "\n",
    "L'objectif présent est d'analyser les abstracts afin de les regrouper en cluster, permettant de dégager des thématiques, de comparer les différentes conférences ou bien d'étudier si les distinctions faites par les organisateurs de la conférence 2019 entre les différentes interventions est bien pertinente.\n",
    "Nous pourrons également critiquer les choix de l'ordinateur.\n",
    "\n",
    "Nous allons faire ici de la classification non supervisée grâce au package SKLearn, librairie dédiée au Machine Learning notamment.\n",
    "\n",
    "### Les packages ### \n",
    "\n",
    "Il faut lancer la cellule ci-dessous une seule fois afin de télécharger la librairie sklearn. Cela doit être fait une seule fois, au premier lancement. Ensuite, il ne faudra plus jamais le lancer. Il est possible qu'il faille redémarrer le noyau après."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install sklearn\n",
    "pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il faut lancer la cellule ci-dessous à chaque lancement de ce notebook-ci."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tout d'abord, il faut ' stemmer ' les abstracts, c'est à dire qu'il faut prendre chaque mot de chaque abstract et le remplacer par son lemme. Cela permettra à la machine de ne pas être faussée par l'usage divers d'un seul et même mot racine (les mots \"buy\", \"buys\", \"bought\" et \"buying\" ne deviennent plus qu'un seul et même mot). Ainsi, l'analyse n'en sera que plus précise et plus juste.\n",
    "\n",
    "Ce texte stemmé est stocké dans le cache, dans un répertoire propre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path = \"./cache2019/cacheTXT/\" #chemin permettant d'accès aux abstracts\n",
    "Path_output = \"./cache2019/cacheSTEM/\" #chemin de sortie des abstracts stemmés\n",
    "filelist = os.listdir(Path) #filelist est une liste regroupant tous les chemins vers les différents abstracts.\n",
    "if not os.path.exists(\"./cache2019/cacheSTEM/\"): #permet de créer un dossier dans le cache s'il est supprimé.\n",
    "    os.makedirs(\"./cache2019/cacheSTEM/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer() #J'importe une méthode acceptant une chaîne de caractère à une variable.\n",
    "documents = [] #j'instancie une liste qui regroupera l'ensemble des chemins vers les documents\n",
    "for abstract in filelist:\n",
    "    with open(Path + abstract, \"r\", encoding=\"UTF-8\") as y:\n",
    "        documents.append(Path_output + abstract)\n",
    "        texte = y.read()\n",
    "        liste_mots_tokenise = [] # j'instancie une liste qui accueillera chaque lemme (le lemme étant une str)\n",
    "        liste_mots_a_tokenise = texte.split() #je crée une liste sur laquelle je vais pouvoir boucler\n",
    "        for elem in liste_mots_a_tokenise:\n",
    "            mot_tokenise = stemmer.stem(elem) #mot_tokenise est une str, et est le lemme du mot sur lequel je boucle\n",
    "            liste_mots_tokenise.append(mot_tokenise) #j'ajoute chaque mot à une liste, qu'ensuite je join pour recréer l'abstract sous la forme d'une str\n",
    "        resultat = ' '.join(liste_mots_tokenise)\n",
    "        with open(Path_output + abstract, \"w\", encoding=\"UTF-8\") as z:\n",
    "            z.write(resultat) #resultat est une str\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici, on associe à chaque mot de tous les abstracts une coordonnée unique à chaque mot utilisé, pour ensuite pouvoir les placer sur un plan en 2D et ainsi relever des ressemblances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorize the text i.e. convert the strings to numeric features\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuite, on réunit les termes vectorisés en cluster, c'est à dire en groupe de ressemblance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cluster documents, ici 10 clusters\n",
    "true_k = 4\n",
    "model = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "model.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enfin, on montre les termes que l'ordinateur a pu rassembler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print top terms per cluster clusters\n",
    "print(\"Top terms per cluster:\")\n",
    "order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names()\n",
    "for i in range(true_k):\n",
    "    print(\"Cluster %d:\" % i)\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(' %s' % terms[ind])\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En dessous, je teste des trucs, en vain. Seul le deuxième donne un truc intelligible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "documents = vectorizer.transform(documents)\n",
    "print(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = model.fit_transform(X)\n",
    "for idx, cluster in enumerate(z):\n",
    "    print(\"Document '{}' assigned to cluster {}.\".format(documents[idx],cluster))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_document_features = TfidfVectorizer().fit_transform(documents) #où chaque texte est dans la liste my_text\n",
    "\n",
    "km = KMeans(8).fit(my_document_features)\n",
    "print(km.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_document_features = vec.fit_transform('./cache2019/cacheSTEM/') #où my text files est un chemin vers un dossier où y a les textes\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
