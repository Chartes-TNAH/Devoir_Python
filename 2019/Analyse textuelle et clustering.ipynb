{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEI Conference 2019 #\n",
    "\n",
    "## Etape 3 : L'Analyse ##\n",
    "\n",
    "### Préambule ###\n",
    "\n",
    "L'objectif présent est d'analyser les abstracts afin de les regrouper en cluster, permettant de dégager des thématiques, de comparer les différentes conférences ou bien d'étudier si les distinctions faites par les organisateurs de la conférence 2019 entre les différentes interventions est bien pertinente.\n",
    "Nous pourrons également critiquer les choix de l'ordinateur.\n",
    "\n",
    "Nous allons faire ici de la classification non supervisée grâce au package SKLearn, librairie dédiée au Machine Learning notamment.\n",
    "\n",
    "### Les packages ### \n",
    "\n",
    "Il faut lancer la cellule ci-dessous une seule fois afin de télécharger la librairie sklearn. Cela doit être fait une seule fois, au premier lancement. Ensuite, il ne faudra plus jamais le lancer. Il est possible qu'il faille redémarrer le noyau après."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install sklearn\n",
    "pip install nltk\n",
    "pip install pandas\n",
    "pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.0.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il faut lancer la cellule ci-dessous à chaque lancement de ce notebook-ci."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "import os\n",
    "import spacy\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import sklearn\n",
    "import sys\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import ldamodel\n",
    "import gensim.corpora\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.preprocessing import normalize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D'abord, on crée le répertoire dans le cache du contenu qui recevra les textes lemmatisés :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path = \"./cache2019/cacheTXT/\" #chemin permettant d'accès aux abstracts\n",
    "Path_output = \"./cache2019/cacheLEM/\" #chemin de sortie des abstracts stemmés\n",
    "filelist = os.listdir(Path) #filelist est une liste regroupant tous les chemins vers les différents abstracts.\n",
    "if not os.path.exists(\"./cache2019/cacheLEM/\"): #permet de créer un dossier dans le cache s'il est supprimé.\n",
    "    os.makedirs(\"./cache2019/cacheLEM/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Méthode 1 : tokenisation et lemmatisation ####\n",
    "\n",
    "Je propose ci-dessous de lemmatizer le texte avec spaCy. Ma lemmatisation va retourner une forme canonique de chaque mot. La tokenisation et la lemmatisation s'appuient sur un petit corpus que j'ai importé (en_core_web_sm) depuis la base de spaCy. En entrée se trouve le texte normalisé de chaque abstract, et en sortie nous retrouvons ce même texte, mais où chaque mot est remplacé par son lem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for abstract in filelist:\n",
    "    with open(Path + abstract, \"r\", encoding=\"UTF-8\") as y:\n",
    "        texte = y.read()\n",
    "        nlp = spacy.load(\"en_core_web_sm\") #nlp est un corpus de test permettant de tokenizer efficacement l'anglais à partir d'un petit corpus (j'ai choisi le petit corpus, pas le gros)\n",
    "        doc = nlp(texte) #doc est le texte annoté. Ce n'est pas une str pour autant\n",
    "        liste_mots_lemmatise = [] # j'instancie une liste qui accueillera chaque lemme (le lemme étant une str)\n",
    "        for token in doc:\n",
    "            liste_mots_lemmatise.append(token.lemma_) #token.lemma_ est une str. Cette méthode tokenise puis lematise. Il crée une liste qu'on va joindre ensuite.\n",
    "        resultat = ' '.join(liste_mots_lemmatise)\n",
    "        with open(Path_output + abstract, \"w\", encoding=\"UTF-8\") as z:\n",
    "            z.write(resultat) #resultat est une str\n",
    "         \n",
    "\n",
    "#je peux demander de l'aide pour comprendre une doc\n",
    "#pour demain 9h du matin, il faut que j'écrive une définition de tokenisation, lemmatisation et POS taging, leur impact et leur importance. Demain à 11h (9h métropole) je le montre à Clérice\n",
    "#Pour lundi 9h, me farcir les polycopiés (page 31 à 38 du polycop à 38 pages). Il faut aussi lire l'article que Clérice m'a envoyé.\n",
    "#Regarder ce qu'est le topic modeling (en non supervisé), essayez de trouver un tuto là dessus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Méthode 2 : tokenization + stemmisation ####\n",
    "\n",
    "Dans cette seconde méthode, le texte est tokenisé (grâce à spaCy toujours) mais il est stemmé et non pas lemmatisé. Le stemming consiste à retirer les terminaisons pour garder une forme canonique de chaque mot. Le fonctionnement est identique à celui de la lemmatisation, pour ma démarche. Le résultat est également stocké dans un cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path = \"./cache2019/cacheTXT/\" #chemin permettant d'accès aux abstracts\n",
    "Path_output = \"./cache2019/cacheSTEM/\" #chemin de sortie des abstracts stemmés\n",
    "filelist = os.listdir(Path) #filelist est une liste regroupant tous les chemins vers les différents abstracts.\n",
    "if not os.path.exists(\"./cache2019/cacheSTEM/\"): #permet de créer un dossier dans le cache s'il est supprimé.\n",
    "    os.makedirs(\"./cache2019/cacheSTEM/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer() #J'importe une méthode acceptant une chaîne de caractère à une variable.\n",
    "for abstract in filelist:\n",
    "    with open(Path + abstract, \"r\", encoding=\"UTF-8\") as y:\n",
    "        texte = y.read()\n",
    "        liste_mots_tokenise = [] # j'instancie une liste qui accueillera chaque lemme (le lemme étant une str)\n",
    "        liste_mots_a_tokenise = texte.split() #je crée une liste sur laquelle je vais pouvoir boucler\n",
    "        for elem in liste_mots_a_tokenise:\n",
    "            mot_tokenise = stemmer.stem(elem) #mot_tokenise est une str, et est le lemme du mot sur lequel je boucle\n",
    "            liste_mots_tokenise.append(mot_tokenise) #j'ajoute chaque mot à une liste, qu'ensuite je join pour recréer l'abstract sous la forme d'une str\n",
    "        resultat = ' '.join(liste_mots_tokenise)\n",
    "        with open(Path_output + abstract, \"w\", encoding=\"UTF-8\") as z:\n",
    "            z.write(resultat) #resultat est une str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuite, il faut choisir quel set sera loadé et analysé entre les abstracts lemmatisés et les abstracts loadés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pour utiliser les textes lemmatisés, c'est ici\n",
    "\n",
    "documents = []\n",
    "Path = \"./cache2019/cacheLEM/\"\n",
    "filelist = os.listdir(Path) #filelist est une liste regroupant tous les chemins vers les différents abstracts.\n",
    "\n",
    "for abstract in filelist:\n",
    "    with open(Path + abstract, \"r\", encoding=\"UTF-8\") as y:\n",
    "        texte = y.read()\n",
    "        documents.append(texte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pour utiliser les textes stemmés, c'est ici\n",
    "\n",
    "documents = []\n",
    "Path = \"./cache2019/cacheSTEM/\"\n",
    "filelist = os.listdir(Path) #filelist est une liste regroupant tous les chemins vers les différents abstracts.\n",
    "\n",
    "for abstract in filelist:\n",
    "    with open(Path + abstract, \"r\", encoding=\"UTF-8\") as y:\n",
    "        texte = y.read()\n",
    "        documents.append(texte)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "## Définir les clusters ##\n",
    "\n",
    "Après avoir tokennisé et lemmatisé/stemmé nos abstracts, nous allons maintenant les vectoriser (représenter chaque mot sous la forme d'un vecteur sur un plan ordonné en deux dimensions) puis ensuite nous allons laisser la machine définir des clusters, c'est à dire des groupes de mots qui ont un sens commun.\n",
    "\n",
    "Ici, on associe à chaque mot de tous les abstracts une coordonnée unique à chaque mot utilisé, pour ensuite pouvoir les placer sur un plan en 2D et ainsi relever des ressemblances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorize the text i.e. convert the strings to numeric features\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuite, on réunit les termes vectorisés en cluster, c'est à dire en groupe de ressemblance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cluster documents, ici 8 clusters\n",
    "true_k = 8\n",
    "model = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "model.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enfin, on montre les termes que l'ordinateur a pu rassembler ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print top terms per cluster clusters\n",
    "print(\"Top terms per cluster:\")\n",
    "order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names()\n",
    "for i in range(true_k):\n",
    "    print(\"Cluster %d:\" % i)\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(' %s' % terms[ind])\n",
    "        \n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster de documents ##\n",
    "\n",
    "Grâce à un système de prédiction découlant de l'apprentissage et de l'entraînement sur les termes des abstracts, nous pouvons classifier les abstracts en 8 catégories. Nous pourrons ensuite les comparer avec le classement déjà fait par les organisateurs de la conférence TEI 2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path = \"./cache2019/cacheSTEM/\" #On peut remplacer le cache STEM par LEM à la place, pour ainsi comparer les deux sorties.\n",
    "filelist = os.listdir(Path) #filelist est une liste regroupant tous les chemins vers les différents abstracts.\n",
    "liste_triee = [] #c'est la liste contenant les infos triées de tous les abstracts\n",
    "\n",
    "\n",
    "for abstract in filelist:\n",
    "    reference = abstract.replace('.txt', '') #je normalise le nom du texte pour le faire correpondre avec celui indiqué dans le tableau csv réunissant toutes les informations\n",
    "    with open(Path + abstract, \"r\", encoding=\"UTF-8\") as y:\n",
    "        liste_resultat_unitaire = [] #j'instancie la liste des infos propres à chaque abstract\n",
    "        texte = y.read()\n",
    "        X = vectorizer.transform([texte]) #je vectorise le texte de l'abstract\n",
    "        predicted = model.predict(X)  #j'attribue l'abstract à un cluster. predicted est de la classe numpy array : c'est un vecteur\n",
    "        with open('./cache2019/TEI2019.csv', 'r') as csvfile:\n",
    "            read = csv.reader(csvfile, delimiter = ',')\n",
    "            titre = ''\n",
    "            auteurs = ''\n",
    "            for row in read:\n",
    "                if row[4] == reference:\n",
    "                    titre = row[2]\n",
    "                    auteurs = row[0]\n",
    "        liste_resultat_unitaire.append(titre)\n",
    "        liste_resultat_unitaire.append(auteurs)\n",
    "        liste_resultat_unitaire.append(int(predicted))\n",
    "        liste_triee.append(liste_resultat_unitaire)\n",
    "        \n",
    "\n",
    "print(\"Dans le groupe 1, il y a :\")\n",
    "for elem in liste_triee:\n",
    "    if elem[2] == 0:\n",
    "        print(\"\\t\", elem[0] )\n",
    "        \n",
    "print(\"\\n Dans le groupe 2, il y a :\")\n",
    "for elem in liste_triee:\n",
    "    if elem[2] == 1:\n",
    "        print(\"\\t\", elem[0] )\n",
    "        \n",
    "print(\"\\n Dans le groupe 3, il y a :\")\n",
    "for elem in liste_triee:\n",
    "    if elem[2] == 2:\n",
    "        print(\"\\t\", elem[0] )\n",
    "\n",
    "print(\"\\n Dans le groupe 4, il y a :\")\n",
    "for elem in liste_triee:\n",
    "    if elem[2] == 3:\n",
    "        print(\"\\t\", elem[0] )\n",
    "\n",
    "print(\"\\n Dans le groupe 5, il y a :\")\n",
    "for elem in liste_triee:\n",
    "    if elem[2] == 4:\n",
    "        print(\"\\t\", elem[0] )\n",
    "        \n",
    "print(\"\\n Dans le groupe 6, il y a :\")\n",
    "for elem in liste_triee:\n",
    "    if elem[2] == 5:\n",
    "        print(\"\\t\", elem[0] )\n",
    "        \n",
    "print(\"\\n Dans le groupe 7, il y a :\")\n",
    "for elem in liste_triee:\n",
    "    if elem[2] == 6:\n",
    "        print(\"\\t\", elem[0] )\n",
    "        \n",
    "print(\"\\n Dans le groupe 8, il y a :\")\n",
    "for elem in liste_triee:\n",
    "    if elem[2] == 7:\n",
    "        print(\"\\t\", elem[0] )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voici le plan et l'agencement des conférences tel qu'il est indiqué ici : https://graz-2019.tei-c.org/wp-content/uploads/2019/09/ProgrammheftFINAL.pdf\n",
    "\n",
    "TEI, formal ontologies,controlled vocabularies and Linked OpenData :\n",
    "- Making Linkable Data from Account Books: Bookkeeping Ontology in the Digital Edition Publishing Cooperative for Historical Accounts\n",
    "- Inscriptions, Hieroglyphs, Linguistics and Beyond! The Corpus of Classic Mayan as an Ontological Information Resource\n",
    "- Modeling FRBR Entities and their Relationships with TEI: A Look at HallerNet Bibliographic Descriptions\n",
    "- Referencing an editorial ontology from the TEI: An attempt to overcome informal typologies\n",
    "- Text Graph Ontology. A Semantic Web approach to represent genetic scholarly editions\n",
    "\n",
    "TEI and models of text :\n",
    "- A realistic theory of textuality and its consequences on digital text representation\n",
    "- Genesis and Variance: From Letter to Literature\n",
    "- Between freedom and formalisation: a hypergraph model for representing the nature of text\n",
    "- Reflecting the Influence of Technology on Models of Text in Scholarly Digital Editing\n",
    "- Introducing Objectification: when is an <object> a <place> ?\n",
    "- An Encoding Strategic Proposal of “Ruby” Texts: Examples from Japanese Texts\n",
    "- Referencing annotations as a core concept of the hallerNet edition and research platform\n",
    "- Recreating history through events\n",
    "- Document Modeling with the TEI Critical Apparatus\n",
    "- Exploring TEI structures to find distinctive features of text types\n",
    "- Reconceiving TEI models of theatrical performance text with reference to promptbooks\n",
    "- What is a Line? Encoding and Counting Lines in Early Modern Dramatic Texts\n",
    "    \n",
    "TEI across corpora,languages, and cultures :\n",
    "- Growing collections of TEI texts: Some lessons from SARIT\n",
    "- Towards larger corpora of Indic texts: For now, minimize metatext\n",
    "- Encoding history in TEI: A corpus-oriented approach for investigating Tibetan historiography\n",
    "- Advantages and challenges of tokenized TEI\n",
    "- A sign of the times: medieval punctuation, its encoding and its rendition in modern times\n",
    "\n",
    "TEI annotation and publication :\n",
    "- Analyzing and Visualizing Uncertain Knowledge: Introducing the PROVIDEDH Open Science Platform\n",
    "- The Prefabricated Website: Who Needs a Server Anyway?\n",
    "- correspSearch v2 –New ways of exploring correspondence\n",
    "- Validating @selector: a regular expression adventure\n",
    "- TEI encoding of correspondence: A community effort\n",
    "\n",
    "TEI simplification and extension :\n",
    "- Opportunities and challenges of the TEI for scholarly journals in the Humanities\n",
    "- Archiving a TEI project FAIRly\n",
    "- Creating high-quality print from TEI documents\n",
    "- Native-TEI dialectal dictionary for Bavarian dialects in Austria: data structure, software and workflow\n",
    "- An Attempt of Dissemination of TEI in a TEI-underdeveloped country: Activities of the SIG EAJ\n",
    "- Refining the Current Teaching Methodology of the TEI through the Analysis of Server Logs\n",
    "- Using Github and its Integrations to Create, Test, and Deploy a Digital Edition\n",
    "\n",
    "TEI environments and infrastructures :\n",
    "- Parla-CLARIN: TEI guidelines for corpora of parliamentary proceedings\n",
    "- Challenges in encoding parliamentary data: between applause and interjections\n",
    "- A TEI customization for the description of paper and watermarks\n",
    "- How we tripled our encoding speed in the Digital Victorian Periodical Project\n",
    "- Manuscripta-The editor from past to future\n",
    "- Highlighting Our Examples: encoding XML examples in pedagogical contexts\n",
    "- Case Study TEI Customization: A Restricted TEI Format for Edition Open Access (EOA)\n",
    "- In search of comity: TEI for distant reading\n",
    "\n",
    "TEI and beyond :interactions, interchange, integrations and interoperability :\n",
    "- Using Machine Learning for the Automated Classification of Stage Directions in TEI-Encoded Drama Corpora\n",
    "- TEI XML and Delta Format Interchangeability\n",
    "\n",
    "TEI and non-XML technologies :\n",
    "- Five Centuries of History in a Network\n",
    "- Introducing an Open, Dynamic and Efficient Lexical Data Access for TEI-encoded Dictionaries on the Internet\n",
    "- Getting Along with Relational Databases\n",
    "- Using Microsoft Word for preparing XML TEI-compliant digital editions\n",
    "- Scaling up Automatic Structuring of Manuscript Sales Catalogues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for abstract in documents:\n",
    "    X = vectorizer.transform([abstract])\n",
    "    predicted = model.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modelling ###\n",
    "\n",
    "Ensuite, je vais tenter de déterminer des sujets aux cluster, c'est à dire déterminer grâce aux mots de chaque cluster des thèmes propres à chaque cluster grâce à une étude de chacun des termes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = TfidfTransformer(smooth_idf=False)\n",
    "x_tfidf = transformer.fit_transform(X)\n",
    "xtfidf_norm = normalize(x_tfidf, norm='l1', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NMF(alpha=0.0, beta_loss='frobenius', init=None, l1_ratio=0.0, max_iter=200,\n",
       "    n_components=8, random_state=None, shuffle=False, solver='cd', tol=0.0001,\n",
       "    verbose=0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_topics = 8 #définition du nombre de topics\n",
    "model = NMF(n_components=num_topics) \n",
    "model.fit(xtfidf_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic # 01</th>\n",
       "      <th>Topic # 02</th>\n",
       "      <th>Topic # 03</th>\n",
       "      <th>Topic # 04</th>\n",
       "      <th>Topic # 05</th>\n",
       "      <th>Topic # 06</th>\n",
       "      <th>Topic # 07</th>\n",
       "      <th>Topic # 08</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sales</td>\n",
       "      <td>uncertainty</td>\n",
       "      <td>ruby</td>\n",
       "      <td>parliamentary</td>\n",
       "      <td>cmif</td>\n",
       "      <td>event</td>\n",
       "      <td>sarit</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>catalogues</td>\n",
       "      <td>pron</td>\n",
       "      <td>gloss</td>\n",
       "      <td>clarin</td>\n",
       "      <td>correspondence</td>\n",
       "      <td>recreate</td>\n",
       "      <td>indic</td>\n",
       "      <td>objectification</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>structuring</td>\n",
       "      <td>egxml</td>\n",
       "      <td>japanese</td>\n",
       "      <td>pp</td>\n",
       "      <td>correspsearch</td>\n",
       "      <td>medieval</td>\n",
       "      <td>mcallister</td>\n",
       "      <td>objectidentifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>automatic</td>\n",
       "      <td>delta</td>\n",
       "      <td>denote</td>\n",
       "      <td>parla</td>\n",
       "      <td>letter</td>\n",
       "      <td>diverse</td>\n",
       "      <td>lesson</td>\n",
       "      <td>unam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>scale</td>\n",
       "      <td>odd</td>\n",
       "      <td>interlinear</td>\n",
       "      <td>comment</td>\n",
       "      <td>metadata</td>\n",
       "      <td>unified</td>\n",
       "      <td>collection</td>\n",
       "      <td>description</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>manuscript</td>\n",
       "      <td>tei</td>\n",
       "      <td>phonetic</td>\n",
       "      <td>interjection</td>\n",
       "      <td>sig</td>\n",
       "      <td>diary</td>\n",
       "      <td>metatext</td>\n",
       "      <td>change</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>punctuation</td>\n",
       "      <td>kosh</td>\n",
       "      <td>main</td>\n",
       "      <td>applause</td>\n",
       "      <td>v2</td>\n",
       "      <td>1950</td>\n",
       "      <td>attempt</td>\n",
       "      <td>introduce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>watermark</td>\n",
       "      <td>pedagogical</td>\n",
       "      <td>w3c</td>\n",
       "      <td>ntoso</td>\n",
       "      <td>service</td>\n",
       "      <td>norm</td>\n",
       "      <td>grow</td>\n",
       "      <td>distinction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sign</td>\n",
       "      <td>watermark</td>\n",
       "      <td>打球</td>\n",
       "      <td>akoma</td>\n",
       "      <td>jtei</td>\n",
       "      <td>listevent</td>\n",
       "      <td>patrick</td>\n",
       "      <td>element</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>distinctive</td>\n",
       "      <td>highlighting</td>\n",
       "      <td>sided</td>\n",
       "      <td>recommendation</td>\n",
       "      <td>network</td>\n",
       "      <td>generically</td>\n",
       "      <td>scholar</td>\n",
       "      <td>geo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>und</td>\n",
       "      <td>regular</td>\n",
       "      <td>ビリヤード</td>\n",
       "      <td>speech</td>\n",
       "      <td>1742</td>\n",
       "      <td>commonly</td>\n",
       "      <td>indology</td>\n",
       "      <td>mscontents</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>interpretative</td>\n",
       "      <td>dictionary</td>\n",
       "      <td>ダキウ</td>\n",
       "      <td>parlaformat</td>\n",
       "      <td>dumont</td>\n",
       "      <td>calendar</td>\n",
       "      <td>oeaw</td>\n",
       "      <td>msidentifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>medieval</td>\n",
       "      <td>format</td>\n",
       "      <td>billiard</td>\n",
       "      <td>austrian</td>\n",
       "      <td>connect</td>\n",
       "      <td>realm</td>\n",
       "      <td>reappearance</td>\n",
       "      <td>actively</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>tibetan</td>\n",
       "      <td>latex</td>\n",
       "      <td>katakana</td>\n",
       "      <td>proceeding</td>\n",
       "      <td>development</td>\n",
       "      <td>generalize</td>\n",
       "      <td>frequent</td>\n",
       "      <td>wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>manuscripta</td>\n",
       "      <td>platform</td>\n",
       "      <td>body</td>\n",
       "      <td>economic</td>\n",
       "      <td>aggregation</td>\n",
       "      <td>governmental</td>\n",
       "      <td>disrupt</td>\n",
       "      <td>central_library_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>type</td>\n",
       "      <td>xml</td>\n",
       "      <td>attach</td>\n",
       "      <td>typically</td>\n",
       "      <td>lyon</td>\n",
       "      <td>dateable</td>\n",
       "      <td>expectation</td>\n",
       "      <td>clarify</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>simmler</td>\n",
       "      <td>paper</td>\n",
       "      <td>right</td>\n",
       "      <td>corpus</td>\n",
       "      <td>widget</td>\n",
       "      <td>16th</td>\n",
       "      <td>sober</td>\n",
       "      <td>wall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>york</td>\n",
       "      <td>highlight</td>\n",
       "      <td>chinese</td>\n",
       "      <td>andrej</td>\n",
       "      <td>browser</td>\n",
       "      <td>itinerary</td>\n",
       "      <td>narrowly</td>\n",
       "      <td>objectname</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>oberflächen</td>\n",
       "      <td>namespace</td>\n",
       "      <td>simple</td>\n",
       "      <td>erjavec</td>\n",
       "      <td>creator</td>\n",
       "      <td>enriched</td>\n",
       "      <td>deepen</td>\n",
       "      <td>wiki</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>textsorten</td>\n",
       "      <td>selector</td>\n",
       "      <td>particular</td>\n",
       "      <td>tomaž</td>\n",
       "      <td>dfg</td>\n",
       "      <td>linking</td>\n",
       "      <td>realization</td>\n",
       "      <td>autonomous</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Topic # 01    Topic # 02   Topic # 03      Topic # 04      Topic # 05  \\\n",
       "0            sales   uncertainty         ruby   parliamentary            cmif   \n",
       "1       catalogues          pron        gloss          clarin  correspondence   \n",
       "2      structuring         egxml     japanese              pp   correspsearch   \n",
       "3        automatic         delta       denote           parla          letter   \n",
       "4            scale           odd  interlinear         comment        metadata   \n",
       "5       manuscript           tei     phonetic    interjection             sig   \n",
       "6      punctuation          kosh         main        applause              v2   \n",
       "7        watermark   pedagogical          w3c           ntoso         service   \n",
       "8             sign     watermark           打球           akoma            jtei   \n",
       "9      distinctive  highlighting        sided  recommendation         network   \n",
       "10             und       regular        ビリヤード          speech            1742   \n",
       "11  interpretative    dictionary          ダキウ     parlaformat          dumont   \n",
       "12        medieval        format     billiard        austrian         connect   \n",
       "13         tibetan         latex     katakana      proceeding     development   \n",
       "14     manuscripta      platform         body        economic     aggregation   \n",
       "15            type           xml       attach       typically            lyon   \n",
       "16         simmler         paper        right          corpus          widget   \n",
       "17            york     highlight      chinese          andrej         browser   \n",
       "18     oberflächen     namespace       simple         erjavec         creator   \n",
       "19      textsorten      selector   particular           tomaž             dfg   \n",
       "\n",
       "      Topic # 06    Topic # 07        Topic # 08  \n",
       "0          event         sarit            object  \n",
       "1       recreate         indic   objectification  \n",
       "2       medieval    mcallister  objectidentifier  \n",
       "3        diverse        lesson              unam  \n",
       "4        unified    collection       description  \n",
       "5          diary      metatext            change  \n",
       "6           1950       attempt         introduce  \n",
       "7           norm          grow       distinction  \n",
       "8      listevent       patrick           element  \n",
       "9    generically       scholar               geo  \n",
       "10      commonly      indology        mscontents  \n",
       "11      calendar          oeaw      msidentifier  \n",
       "12         realm  reappearance          actively  \n",
       "13    generalize      frequent         wikipedia  \n",
       "14  governmental       disrupt  central_library_  \n",
       "15      dateable   expectation           clarify  \n",
       "16          16th         sober              wall  \n",
       "17     itinerary      narrowly        objectname  \n",
       "18      enriched        deepen              wiki  \n",
       "19       linking   realization        autonomous  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_nmf_topics(model, n_top_words):\n",
    "    \n",
    "    #il faut obtenir le mot à partir de son vecteur identifiant pour afficher les termes de chaque sujet\n",
    "    feat_names = vectorizer.get_feature_names()\n",
    "    \n",
    "    word_dict = {}\n",
    "    for i in range(num_topics):\n",
    "        \n",
    "        #for each topic, obtain the largest values, and add the words they map to into the dictionary.\n",
    "        words_ids = model.components_[i].argsort()[:-20 - 1:-1]\n",
    "        words = [feat_names[key] for key in words_ids]\n",
    "        word_dict['Topic # ' + '{:02d}'.format(i+1)] = words;\n",
    "    \n",
    "    return pd.DataFrame(word_dict)\n",
    "get_nmf_topics(model, 20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
