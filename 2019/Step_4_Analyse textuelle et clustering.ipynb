{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEI Conference 2019 #\n",
    "\n",
    "## Etape 4 : Classification non supervisée ##\n",
    "\n",
    "### Préambule ###\n",
    "\n",
    "Nous allons faire ici de la classification non supervisée grâce au package SKLearn, librairie dédiée au Machine Learning.\n",
    "\n",
    "L'objectif est de soumettre librement le corpus à l'ordinateur et de le laisser créer des clusters, c'est à dire des regroupements de textes. Pour cela, l'ordinateur va vectoriser chaque terme de chaque abstract du corpus, permettant d'attribuer à chaque mot une valeur unique pondérée par la récurrence du terme dans le texte. Grâce à ces calculs, nous obtenons une représentation des textes dans l'espace, que l'ordinateur peut ensuite réunir en calculant la proximité de chacun des vecteurs / textes depuis certains points (les centroids) disposés aléatoirement. La répétition de cette expérience aléatoire permet de définir une classification supposée objective.\n",
    "\n",
    "La classification que nous obtiendrons nous permettra de comparer avec le plan déterminé par le programme de la conférence TEI 2019, par exemple.\n",
    "\n",
    "### Les packages ### \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il faut lancer la cellule ci-dessous pour importer les packages dédiés.\n",
    "\n",
    "La librairie SKLearn permet, grâce à TF-IdF, de vectoriser les textes, c'est à dire d'attribuer à chaque mot un vecteur unique le caractérisant dans l'espaces. KMeans permet en outre de définir des clusters.\n",
    "\n",
    "La librairie spaCy permet de lemmatiser, tandis que la librairie PorterStemmer de NLTK permet de stemmer.\n",
    "\n",
    "Les librairies Pandas, Numpy et Scipy permettent de traiter la donnée et de produire des tableaux à partir des textes vectorisés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import sys\n",
    "\n",
    "# Vectorisation\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "#Représentation\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choisir son corpus de travail ###\n",
    "\n",
    "Ensuite, il faut choisir quel set sera loadé et analysé entre les abstracts lemmatisés et les abstracts loadés. Le résultat change entre les deux versions. Pour un premier essai, il est premier de travailler avec la version lemmatisée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pour utiliser les textes stemmés, c'est ici\n",
    "\n",
    "documents = []\n",
    "Path = \"./cache2019/cacheSTEM/\"\n",
    "filelist = os.listdir(Path) #filelist est une liste regroupant tous les chemins vers les différents abstracts.\n",
    "\n",
    "for abstract in filelist:\n",
    "    with open(Path + abstract, \"r\", encoding=\"UTF-8\") as y:\n",
    "        texte = y.read()\n",
    "        documents.append(texte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pour utiliser les textes lemmatisés, c'est ici\n",
    "\n",
    "documents = []\n",
    "Path = \"./cache2019/cacheLEM/\"\n",
    "filelist = os.listdir(Path) #filelist est une liste regroupant tous les chemins vers les différents abstracts.\n",
    "\n",
    "for abstract in filelist:\n",
    "    with open(Path + abstract, \"r\", encoding=\"UTF-8\") as y:\n",
    "        texte = y.read()\n",
    "        documents.append(texte)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "## Définir les clusters ##\n",
    "\n",
    "Après avoir tokennisé et lemmatisé/stemmé nos abstracts, nous allons maintenant les vectoriser (représenter chaque mot sous la forme d'un vecteur sur un plan ordonné en deux dimensions) puis ensuite nous allons laisser la machine définir des clusters, c'est à dire des groupes de mots qui ont un sens commun.\n",
    "\n",
    "Ici, on associe à chaque mot de tous les abstracts une coordonnée unique à chaque mot utilisé, pour ensuite pouvoir les placer sur un plan en 2D et ainsi relever des ressemblances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorise le texte, c'est à dire transforme chaque terme du texte en vecteur\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuite, on réunit les termes vectorisés en cluster, c'est à dire en groupe de ressemblance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=100,\n",
       "       n_clusters=8, n_init=1, n_jobs=None, precompute_distances='auto',\n",
       "       random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#clusterise les documents, ici 8 clusters\n",
    "true_k = 8\n",
    "model = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "model.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enfin, on montre les termes que l'ordinateur a pu rassembler ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "Cluster 0:\n",
      " ruby\n",
      " gloss\n",
      " pron\n",
      " japanese\n",
      " text\n",
      " denote\n",
      " main\n",
      " interlinear\n",
      " encode\n",
      " w3c\n",
      "Cluster 1:\n",
      " tei\n",
      " text\n",
      " performance\n",
      " guidelines\n",
      " pron\n",
      " 2019\n",
      " japan\n",
      " country\n",
      " aozora\n",
      " bunko\n",
      "Cluster 2:\n",
      " catalogues\n",
      " sales\n",
      " manuscript\n",
      " structuring\n",
      " automatic\n",
      " scale\n",
      " manuscripta\n",
      " editor\n",
      " description\n",
      " validation\n",
      "Cluster 3:\n",
      " parliamentary\n",
      " clarin\n",
      " pp\n",
      " parla\n",
      " comment\n",
      " applause\n",
      " interjection\n",
      " encode\n",
      " austrian\n",
      " corpus\n",
      "Cluster 4:\n",
      " regular\n",
      " selector\n",
      " expression\n",
      " css3\n",
      " pron\n",
      " write\n",
      " relaxng\n",
      " rendition\n",
      " default\n",
      " program\n",
      "Cluster 5:\n",
      " correspondence\n",
      " bibliographic\n",
      " cmif\n",
      " letter\n",
      " tei\n",
      " correspsearch\n",
      " hallernet\n",
      " metadata\n",
      " pron\n",
      " jtei\n",
      "Cluster 6:\n",
      " pron\n",
      " tei\n",
      " text\n",
      " use\n",
      " paper\n",
      " xml\n",
      " element\n",
      " edition\n",
      " project\n",
      " encode\n",
      "Cluster 7:\n",
      " https\n",
      " fair\n",
      " historical\n",
      " principles\n",
      " org\n",
      " digital\n",
      " financial\n",
      " der\n",
      " states\n",
      " iip\n"
     ]
    }
   ],
   "source": [
    "#montre le top 10 des mot-clés les plus représentatifs de chaque cluster\n",
    "print(\"Top terms per cluster:\")\n",
    "order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names()\n",
    "for i in range(true_k):\n",
    "    print(\"Cluster %d:\" % i)\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(' %s' % terms[ind])\n",
    "        \n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster de documents ##\n",
    "\n",
    "Grâce à un système de prédiction découlant de l'apprentissage et de l'entraînement sur les termes des abstracts, nous pouvons classifier les abstracts en 8 catégories. Nous pourrons ensuite les comparer avec le classement déjà fait par les organisateurs de la conférence TEI 2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dans le groupe 1, il y a :\n",
      "\t An Encoding Strategic Proposal of “Ruby” Texts: Examples from Japanese Texts\n",
      "\n",
      " Dans le groupe 2, il y a :\n",
      "\t Reconceiving TEI models of theatrical performance text with reference to promptbooks\n",
      "\t Refining the Current Teaching Methodology of the TEI through the Analysis of Server Logs\n",
      "\t An Attempt of Dissemination of TEI in a TEI-underdeveloped country: Activities of the SIG EAJ\n",
      "\t Using Microsoft Word for preparing XML TEI-compliant digital editions\n",
      "\n",
      " Dans le groupe 3, il y a :\n",
      "\t Scaling up Automatic Structuring of Manuscript Sales Catalogues\n",
      "\t Manuscripta - The editor from past to future\n",
      "\n",
      " Dans le groupe 4, il y a :\n",
      "\t Parla-CLARIN: TEI guidelines for corpora of parliamentary proceedings\n",
      "\t Challenges in encoding parliamentary data: between applause and interjections\n",
      "\n",
      " Dans le groupe 5, il y a :\n",
      "\t Validating @selector: a regular expression adventure\n",
      "\n",
      " Dans le groupe 6, il y a :\n",
      "\t correspSearch v2 – New ways of exploring correspondence\n",
      "\t Modeling FRBR Entities and their Relationships with TEI: A Look at HallerNet Bibliographic Descriptions\n",
      "\t Referencing annotations as a core concept of the hallerNet edition and research platform\n",
      "\t TEI encoding of correspondence: A community effort\n",
      "\n",
      " Dans le groupe 7, il y a :\n",
      "\t Using Machine Learning for the Automated Classification of Stage Directions in TEI-Encoded Drama Corpora\n",
      "\t TEI XML and Delta Format Interchangeability\n",
      "\t Getting Along with Relational Databases\n",
      "\t Introducing Objectification: when is an <object> a <place>?\n",
      "\t Analyzing and Visualizing Uncertain Knowledge: Introducing the PROVIDEDH Open Science Platform\n",
      "\t Referencing an editorial ontology from the TEI: An attempt to overcome informal typologies\n",
      "\t Case Study TEI Customization: A Restricted TEI Format for Edition Open Access (EOA)\n",
      "\t In search of comity: TEI for distant reading\n",
      "\t Recreating history through events\n",
      "\t Highlighting Our Examples: encoding XML examples in pedagogical contexts\n",
      "\t Growing collections of TEI texts: Some lessons from SARIT\n",
      "\t How we tripled our encoding speed in the \n",
      "\t A TEI customization for the description of paper and watermarks\n",
      "\t The Prefabricated Website: Who Needs a Server Anyway?\n",
      "\t Exploring TEI structures to find distinctive features of text types\n",
      "\t Advantages and challenges of tokenized TEI\n",
      "\t Native-TEI dialectal dictionary for Bavarian dialects in Austria: data structure, software and workflow\n",
      "\t Encoding history in TEI: A corpus-oriented approach for investigating Tibetan historiography\n",
      "\t Text Graph Ontology. A Semantic Web approach to represent genetic scholarly editions\n",
      "\t Genesis and Variance: From Letter to Literature\n",
      "\t Five Centuries of History in a Network\n",
      "\t A sign of the times: medieval punctuation, its encoding and its rendition in modern times\n",
      "\t Reflecting the Influence of Technology on Models of Text in Scholarly Digital Editing\n",
      "\t Inscriptions, Hieroglyphs, Linguistics… and Beyond! The Corpus of Classic Mayan as an Ontological Information Resource\n",
      "\t Creating high-quality print from TEI documents\n",
      "\t What is a Line? Encoding and Counting Lines in Early Modern Dramatic Texts\n",
      "\t Towards larger corpora of Indic texts: For now, minimize metatext\n",
      "\t Using Github and its Integrations to Create, Test, and Deploy a Digital Edition\n",
      "\t Introducing an Open, Dynamic and Efficient Lexical Data Access for TEI-encoded Dictionaries on the Internet\n",
      "\t A realistic theory of textuality and its consequences on digital text representation\n",
      "\n",
      " Dans le groupe 8, il y a :\n",
      "\t Making Linkable Data from Account Books: Bookkeeping Ontology in the Digital Edition Publishing Cooperative for Historical Accounts\n",
      "\t Archiving a TEI project FAIRly\n"
     ]
    }
   ],
   "source": [
    "Path = \"./cache2019/cacheLEM/\" #On peut remplacer le cache STEM par LEM à la place, pour ainsi comparer les deux sorties.\n",
    "filelist = os.listdir(Path) #filelist est une liste regroupant tous les chemins vers les différents abstracts.\n",
    "liste_triee = [] #c'est la liste contenant les infos triées de tous les abstracts\n",
    "\n",
    "\n",
    "for abstract in filelist:\n",
    "    reference = abstract.replace('.txt', '') #je normalise le nom du texte pour le faire correpondre avec celui indiqué dans le tableau csv réunissant toutes les informations\n",
    "    with open(Path + abstract, \"r\", encoding=\"UTF-8\") as y:\n",
    "        liste_resultat_unitaire = [] #j'instancie la liste des infos propres à chaque abstract\n",
    "        texte = y.read()\n",
    "        X = vectorizer.transform([texte]) #je vectorise le texte de l'abstract\n",
    "        predicted = model.predict(X)  #j'attribue l'abstract à un cluster. predicted est de la classe numpy array : c'est un vecteur\n",
    "        with open('./cache2019/TEI2019.csv', 'r') as csvfile:\n",
    "            read = csv.reader(csvfile, delimiter = ',')\n",
    "            titre = ''\n",
    "            auteurs = ''\n",
    "            for row in read:\n",
    "                if row[4] == reference:\n",
    "                    titre = row[2]\n",
    "                    auteurs = row[0]\n",
    "        liste_resultat_unitaire.append(titre)\n",
    "        liste_resultat_unitaire.append(auteurs)\n",
    "        liste_resultat_unitaire.append(int(predicted))\n",
    "        liste_triee.append(liste_resultat_unitaire)\n",
    "        \n",
    "\n",
    "print(\"Dans le groupe 1, il y a :\")\n",
    "for elem in liste_triee:\n",
    "    if elem[2] == 0:\n",
    "        print(\"\\t\", elem[0] )\n",
    "        \n",
    "print(\"\\n Dans le groupe 2, il y a :\")\n",
    "for elem in liste_triee:\n",
    "    if elem[2] == 1:\n",
    "        print(\"\\t\", elem[0] )\n",
    "        \n",
    "print(\"\\n Dans le groupe 3, il y a :\")\n",
    "for elem in liste_triee:\n",
    "    if elem[2] == 2:\n",
    "        print(\"\\t\", elem[0] )\n",
    "\n",
    "print(\"\\n Dans le groupe 4, il y a :\")\n",
    "for elem in liste_triee:\n",
    "    if elem[2] == 3:\n",
    "        print(\"\\t\", elem[0] )\n",
    "\n",
    "print(\"\\n Dans le groupe 5, il y a :\")\n",
    "for elem in liste_triee:\n",
    "    if elem[2] == 4:\n",
    "        print(\"\\t\", elem[0] )\n",
    "        \n",
    "print(\"\\n Dans le groupe 6, il y a :\")\n",
    "for elem in liste_triee:\n",
    "    if elem[2] == 5:\n",
    "        print(\"\\t\", elem[0] )\n",
    "        \n",
    "print(\"\\n Dans le groupe 7, il y a :\")\n",
    "for elem in liste_triee:\n",
    "    if elem[2] == 6:\n",
    "        print(\"\\t\", elem[0] )\n",
    "        \n",
    "print(\"\\n Dans le groupe 8, il y a :\")\n",
    "for elem in liste_triee:\n",
    "    if elem[2] == 7:\n",
    "        print(\"\\t\", elem[0] )\n",
    "\n",
    "\n",
    "#Des librairies python qui convertissent en table HMTL, faut que je fasse de la dataviz\n",
    "# chord visualisation est une librairie faisant des dataviz, Seaborn aussi, BOKEH aussi, Plotly (MatPlotLib est la base de tous), Parallel Plot est cool.\n",
    "# On peut utiliser XSLT via LXML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voici le plan et l'agencement des conférences tel qu'il est indiqué ici : https://graz-2019.tei-c.org/wp-content/uploads/2019/09/ProgrammheftFINAL.pdf\n",
    "\n",
    "TEI, formal ontologies,controlled vocabularies and Linked OpenData :\n",
    "- Making Linkable Data from Account Books: Bookkeeping Ontology in the Digital Edition Publishing Cooperative for Historical Accounts\n",
    "- Inscriptions, Hieroglyphs, Linguistics and Beyond! The Corpus of Classic Mayan as an Ontological Information Resource\n",
    "- Modeling FRBR Entities and their Relationships with TEI: A Look at HallerNet Bibliographic Descriptions\n",
    "- Referencing an editorial ontology from the TEI: An attempt to overcome informal typologies\n",
    "- Text Graph Ontology. A Semantic Web approach to represent genetic scholarly editions\n",
    "\n",
    "TEI and models of text :\n",
    "- A realistic theory of textuality and its consequences on digital text representation\n",
    "- Genesis and Variance: From Letter to Literature\n",
    "- Between freedom and formalisation: a hypergraph model for representing the nature of text\n",
    "- Reflecting the Influence of Technology on Models of Text in Scholarly Digital Editing\n",
    "- Introducing Objectification: when is an <object> a <place> ?\n",
    "- An Encoding Strategic Proposal of “Ruby” Texts: Examples from Japanese Texts\n",
    "- Referencing annotations as a core concept of the hallerNet edition and research platform\n",
    "- Recreating history through events\n",
    "- Document Modeling with the TEI Critical Apparatus\n",
    "- Exploring TEI structures to find distinctive features of text types\n",
    "- Reconceiving TEI models of theatrical performance text with reference to promptbooks\n",
    "- What is a Line? Encoding and Counting Lines in Early Modern Dramatic Texts\n",
    "    \n",
    "TEI across corpora,languages, and cultures :\n",
    "- Growing collections of TEI texts: Some lessons from SARIT\n",
    "- Towards larger corpora of Indic texts: For now, minimize metatext\n",
    "- Encoding history in TEI: A corpus-oriented approach for investigating Tibetan historiography\n",
    "- Advantages and challenges of tokenized TEI\n",
    "- A sign of the times: medieval punctuation, its encoding and its rendition in modern times\n",
    "\n",
    "TEI annotation and publication :\n",
    "- Analyzing and Visualizing Uncertain Knowledge: Introducing the PROVIDEDH Open Science Platform\n",
    "- The Prefabricated Website: Who Needs a Server Anyway?\n",
    "- correspSearch v2 –New ways of exploring correspondence\n",
    "- Validating @selector: a regular expression adventure\n",
    "- TEI encoding of correspondence: A community effort\n",
    "\n",
    "TEI simplification and extension :\n",
    "- Opportunities and challenges of the TEI for scholarly journals in the Humanities\n",
    "- Archiving a TEI project FAIRly\n",
    "- Creating high-quality print from TEI documents\n",
    "- Native-TEI dialectal dictionary for Bavarian dialects in Austria: data structure, software and workflow\n",
    "- An Attempt of Dissemination of TEI in a TEI-underdeveloped country: Activities of the SIG EAJ\n",
    "- Refining the Current Teaching Methodology of the TEI through the Analysis of Server Logs\n",
    "- Using Github and its Integrations to Create, Test, and Deploy a Digital Edition\n",
    "\n",
    "TEI environments and infrastructures :\n",
    "- Parla-CLARIN: TEI guidelines for corpora of parliamentary proceedings\n",
    "- Challenges in encoding parliamentary data: between applause and interjections\n",
    "- A TEI customization for the description of paper and watermarks\n",
    "- How we tripled our encoding speed in the Digital Victorian Periodical Project\n",
    "- Manuscripta-The editor from past to future\n",
    "- Highlighting Our Examples: encoding XML examples in pedagogical contexts\n",
    "- Case Study TEI Customization: A Restricted TEI Format for Edition Open Access (EOA)\n",
    "- In search of comity: TEI for distant reading\n",
    "\n",
    "TEI and beyond :interactions, interchange, integrations and interoperability :\n",
    "- Using Machine Learning for the Automated Classification of Stage Directions in TEI-Encoded Drama Corpora\n",
    "- TEI XML and Delta Format Interchangeability\n",
    "\n",
    "TEI and non-XML technologies :\n",
    "- Five Centuries of History in a Network\n",
    "- Introducing an Open, Dynamic and Efficient Lexical Data Access for TEI-encoded Dictionaries on the Internet\n",
    "- Getting Along with Relational Databases\n",
    "- Using Microsoft Word for preparing XML TEI-compliant digital editions\n",
    "- Scaling up Automatic Structuring of Manuscript Sales Catalogues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ne pas le lancer, je stock ici l'exemple pour clusteriser des documents\n",
    "for abstract in documents:\n",
    "    X = vectorizer.transform([abstract])\n",
    "    predicted = model.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modelling ###\n",
    "\n",
    "Ensuite, je vais tenter de déterminer des sujets aux cluster, c'est à dire déterminer grâce aux mots de chaque cluster des thèmes propres à chaque cluster grâce à une étude de chacun des termes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = TfidfTransformer(smooth_idf=False)\n",
    "x_tfidf = transformer.fit_transform(X)\n",
    "xtfidf_norm = normalize(x_tfidf, norm='l1', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 8 #définition du nombre de topics\n",
    "model = NMF(n_components=num_topics) \n",
    "model.fit(xtfidf_norm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
