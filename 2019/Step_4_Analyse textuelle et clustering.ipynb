{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEI Conference 2019 #\n",
    "\n",
    "## Etape 3 : L'Analyse ##\n",
    "\n",
    "### Préambule ###\n",
    "\n",
    "L'objectif présent est d'analyser les abstracts afin de les regrouper en cluster, permettant de dégager des thématiques, de comparer les différentes conférences ou bien d'étudier si les distinctions faites par les organisateurs de la conférence 2019 entre les différentes interventions est bien pertinente.\n",
    "Nous pourrons également critiquer les choix de l'ordinateur.\n",
    "\n",
    "Nous allons faire ici de la classification non supervisée grâce au package SKLearn, librairie dédiée au Machine Learning notamment.\n",
    "\n",
    "### Les packages ### \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il faut lancer la cellule ci-dessous pour importer les packages dédiés.\n",
    "\n",
    "La librairie SKLearn permet, grâce à TF-IdF, de vectoriser les textes, c'est à dire d'attribuer à chaque mot un vecteur unique le caractérisant dans l'espaces. KMeans permet en outre de définir des clusters.\n",
    "\n",
    "La librairie spaCy permet de lemmatiser, tandis que la librairie PorterStemmer de NLTK permet de stemmer.\n",
    "\n",
    "Les librairies Pandas, Numpy et Scipy permettent de traiter la donnée et de produire des tableaux à partir des textes vectorisés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import sklearn\n",
    "import sys\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "#from gensim.models import ldamodel #utiliser gensim pour du topic modeling\n",
    "#import gensim.corpora\n",
    "\n",
    "#le vrai Topic Modelling, c'est LDA pas NMF comme j'ai fait en bas\n",
    "# Au lieu d'utiliser le tf idf\n",
    "#La sémantique distributionnelle: \"You know the sense of a word with the company it have.\"\n",
    "#Pour mon mémoire, lire des trucs sur Word2Vec, voir d'où vient le gars et pourquoi ça incide sur mon travail\n",
    "#FastText aussi, l'étudier, voir de quel entreprise ça vient\n",
    "#lire l'ouvrage Ancient Manuscript in Digital Culture, lire l'article \"Qualitativ Analysis of Semantic Linguage Models\"\n",
    "#Lire le 1, le 2.1, (le 2.2 par curiosité). C'est simple et accessible, lire les concepts généraux qu'il donne et utiliser la biblio\n",
    "#ensuite, j'irai lire les articles publiés en 2019. Obligé de le lire pour le mémoire."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choisir son corpus de travail ###\n",
    "\n",
    "Ensuite, il faut choisir quel set sera loadé et analysé entre les abstracts lemmatisés et les abstracts loadés. Le résultat change entre les deux versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pour utiliser les textes lemmatisés, c'est ici\n",
    "\n",
    "documents = []\n",
    "Path = \"./cache2019/cacheLEM/\"\n",
    "filelist = os.listdir(Path) #filelist est une liste regroupant tous les chemins vers les différents abstracts.\n",
    "\n",
    "for abstract in filelist:\n",
    "    with open(Path + abstract, \"r\", encoding=\"UTF-8\") as y:\n",
    "        texte = y.read()\n",
    "        documents.append(texte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pour utiliser les textes stemmés, c'est ici\n",
    "\n",
    "documents = []\n",
    "Path = \"./cache2019/cacheSTEM/\"\n",
    "filelist = os.listdir(Path) #filelist est une liste regroupant tous les chemins vers les différents abstracts.\n",
    "\n",
    "for abstract in filelist:\n",
    "    with open(Path + abstract, \"r\", encoding=\"UTF-8\") as y:\n",
    "        texte = y.read()\n",
    "        documents.append(texte)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "## Définir les clusters ##\n",
    "\n",
    "Après avoir tokennisé et lemmatisé/stemmé nos abstracts, nous allons maintenant les vectoriser (représenter chaque mot sous la forme d'un vecteur sur un plan ordonné en deux dimensions) puis ensuite nous allons laisser la machine définir des clusters, c'est à dire des groupes de mots qui ont un sens commun.\n",
    "\n",
    "Ici, on associe à chaque mot de tous les abstracts une coordonnée unique à chaque mot utilisé, pour ensuite pouvoir les placer sur un plan en 2D et ainsi relever des ressemblances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorize the text i.e. convert the strings to numeric features\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuite, on réunit les termes vectorisés en cluster, c'est à dire en groupe de ressemblance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=100,\n",
       "       n_clusters=8, n_init=1, n_jobs=None, precompute_distances='auto',\n",
       "       random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cluster documents, ici 8 clusters\n",
    "true_k = 8\n",
    "model = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "model.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enfin, on montre les termes que l'ordinateur a pu rassembler ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "Cluster 0:\n",
      " textual\n",
      " pron\n",
      " letter\n",
      " rochlitz\n",
      " text\n",
      " stance\n",
      " artifact\n",
      " notational\n",
      " variance\n",
      " theory\n",
      "Cluster 1:\n",
      " tei\n",
      " text\n",
      " format\n",
      " pron\n",
      " edition\n",
      " graph\n",
      " xml\n",
      " performance\n",
      " 2019\n",
      " guidelines\n",
      "Cluster 2:\n",
      " pron\n",
      " tei\n",
      " use\n",
      " element\n",
      " xml\n",
      " odd\n",
      " line\n",
      " project\n",
      " datum\n",
      " edition\n",
      "Cluster 3:\n",
      " description\n",
      " watermark\n",
      " paper\n",
      " pron\n",
      " object\n",
      " punctuation\n",
      " manuscript\n",
      " element\n",
      " editor\n",
      " manuscripta\n",
      "Cluster 4:\n",
      " correspondence\n",
      " bibliographic\n",
      " cmif\n",
      " tei\n",
      " pron\n",
      " correspsearch\n",
      " metadata\n",
      " jtei\n",
      " frbr\n",
      " letter\n",
      "Cluster 5:\n",
      " text\n",
      " pron\n",
      " network\n",
      " indic\n",
      " tibetan\n",
      " collection\n",
      " markup\n",
      " corpus\n",
      " history\n",
      " sarit\n",
      "Cluster 6:\n",
      " token\n",
      " structuring\n",
      " type\n",
      " stage\n",
      " sales\n",
      " catalogues\n",
      " tei\n",
      " pron\n",
      " direction\n",
      " corpus\n",
      "Cluster 7:\n",
      " parliamentary\n",
      " clarin\n",
      " pp\n",
      " parla\n",
      " comment\n",
      " applause\n",
      " interjection\n",
      " encode\n",
      " austrian\n",
      " corpus\n"
     ]
    }
   ],
   "source": [
    "#print top terms per cluster clusters\n",
    "print(\"Top terms per cluster:\")\n",
    "order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names()\n",
    "for i in range(true_k):\n",
    "    print(\"Cluster %d:\" % i)\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(' %s' % terms[ind])\n",
    "        \n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster de documents ##\n",
    "\n",
    "Grâce à un système de prédiction découlant de l'apprentissage et de l'entraînement sur les termes des abstracts, nous pouvons classifier les abstracts en 8 catégories. Nous pourrons ensuite les comparer avec le classement déjà fait par les organisateurs de la conférence TEI 2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dans le groupe 1, il y a :\n",
      "\t Genesis and Variance: From Letter to Literature\n",
      "\t A realistic theory of textuality and its consequences on digital text representation\n",
      "\n",
      " Dans le groupe 2, il y a :\n",
      "\t TEI XML and Delta Format Interchangeability\n",
      "\t Reconceiving TEI models of theatrical performance text with reference to promptbooks\n",
      "\t Making Linkable Data from Account Books: Bookkeeping Ontology in the Digital Edition Publishing Cooperative for Historical Accounts\n",
      "\t Refining the Current Teaching Methodology of the TEI through the Analysis of Server Logs\n",
      "\t An Attempt of Dissemination of TEI in a TEI-underdeveloped country: Activities of the SIG EAJ\n",
      "\t Text Graph Ontology. A Semantic Web approach to represent genetic scholarly editions\n",
      "\t Using Microsoft Word for preparing XML TEI-compliant digital editions\n",
      "\t Creating high-quality print from TEI documents\n",
      "\n",
      " Dans le groupe 3, il y a :\n",
      "\t Getting Along with Relational Databases\n",
      "\t Analyzing and Visualizing Uncertain Knowledge: Introducing the PROVIDEDH Open Science Platform\n",
      "\t Referencing an editorial ontology from the TEI: An attempt to overcome informal typologies\n",
      "\t Case Study TEI Customization: A Restricted TEI Format for Edition Open Access (EOA)\n",
      "\t In search of comity: TEI for distant reading\n",
      "\t Recreating history through events\n",
      "\t Highlighting Our Examples: encoding XML examples in pedagogical contexts\n",
      "\t How we tripled our encoding speed in the \n",
      "\t Referencing annotations as a core concept of the hallerNet edition and research platform\n",
      "\t The Prefabricated Website: Who Needs a Server Anyway?\n",
      "\t Native-TEI dialectal dictionary for Bavarian dialects in Austria: data structure, software and workflow\n",
      "\t An Encoding Strategic Proposal of “Ruby” Texts: Examples from Japanese Texts\n",
      "\t Reflecting the Influence of Technology on Models of Text in Scholarly Digital Editing\n",
      "\t Validating @selector: a regular expression adventure\n",
      "\t What is a Line? Encoding and Counting Lines in Early Modern Dramatic Texts\n",
      "\t Using Github and its Integrations to Create, Test, and Deploy a Digital Edition\n",
      "\t Introducing an Open, Dynamic and Efficient Lexical Data Access for TEI-encoded Dictionaries on the Internet\n",
      "\n",
      " Dans le groupe 4, il y a :\n",
      "\t Introducing Objectification: when is an <object> a <place>?\n",
      "\t A TEI customization for the description of paper and watermarks\n",
      "\t Manuscripta - The editor from past to future\n",
      "\t A sign of the times: medieval punctuation, its encoding and its rendition in modern times\n",
      "\n",
      " Dans le groupe 5, il y a :\n",
      "\t correspSearch v2 – New ways of exploring correspondence\n",
      "\t Modeling FRBR Entities and their Relationships with TEI: A Look at HallerNet Bibliographic Descriptions\n",
      "\t Archiving a TEI project FAIRly\n",
      "\t TEI encoding of correspondence: A community effort\n",
      "\n",
      " Dans le groupe 6, il y a :\n",
      "\t Growing collections of TEI texts: Some lessons from SARIT\n",
      "\t Encoding history in TEI: A corpus-oriented approach for investigating Tibetan historiography\n",
      "\t Five Centuries of History in a Network\n",
      "\t Towards larger corpora of Indic texts: For now, minimize metatext\n",
      "\n",
      " Dans le groupe 7, il y a :\n",
      "\t Using Machine Learning for the Automated Classification of Stage Directions in TEI-Encoded Drama Corpora\n",
      "\t Scaling up Automatic Structuring of Manuscript Sales Catalogues\n",
      "\t Exploring TEI structures to find distinctive features of text types\n",
      "\t Advantages and challenges of tokenized TEI\n",
      "\t Inscriptions, Hieroglyphs, Linguistics… and Beyond! The Corpus of Classic Mayan as an Ontological Information Resource\n",
      "\n",
      " Dans le groupe 8, il y a :\n",
      "\t Parla-CLARIN: TEI guidelines for corpora of parliamentary proceedings\n",
      "\t Challenges in encoding parliamentary data: between applause and interjections\n"
     ]
    }
   ],
   "source": [
    "Path = \"./cache2019/cacheLEM/\" #On peut remplacer le cache STEM par LEM à la place, pour ainsi comparer les deux sorties.\n",
    "filelist = os.listdir(Path) #filelist est une liste regroupant tous les chemins vers les différents abstracts.\n",
    "liste_triee = [] #c'est la liste contenant les infos triées de tous les abstracts\n",
    "\n",
    "\n",
    "for abstract in filelist:\n",
    "    reference = abstract.replace('.txt', '') #je normalise le nom du texte pour le faire correpondre avec celui indiqué dans le tableau csv réunissant toutes les informations\n",
    "    with open(Path + abstract, \"r\", encoding=\"UTF-8\") as y:\n",
    "        liste_resultat_unitaire = [] #j'instancie la liste des infos propres à chaque abstract\n",
    "        texte = y.read()\n",
    "        X = vectorizer.transform([texte]) #je vectorise le texte de l'abstract\n",
    "        predicted = model.predict(X)  #j'attribue l'abstract à un cluster. predicted est de la classe numpy array : c'est un vecteur\n",
    "        with open('./cache2019/TEI2019.csv', 'r') as csvfile:\n",
    "            read = csv.reader(csvfile, delimiter = ',')\n",
    "            titre = ''\n",
    "            auteurs = ''\n",
    "            for row in read:\n",
    "                if row[4] == reference:\n",
    "                    titre = row[2]\n",
    "                    auteurs = row[0]\n",
    "        liste_resultat_unitaire.append(titre)\n",
    "        liste_resultat_unitaire.append(auteurs)\n",
    "        liste_resultat_unitaire.append(int(predicted))\n",
    "        liste_triee.append(liste_resultat_unitaire)\n",
    "        \n",
    "\n",
    "print(\"Dans le groupe 1, il y a :\")\n",
    "for elem in liste_triee:\n",
    "    if elem[2] == 0:\n",
    "        print(\"\\t\", elem[0] )\n",
    "        \n",
    "print(\"\\n Dans le groupe 2, il y a :\")\n",
    "for elem in liste_triee:\n",
    "    if elem[2] == 1:\n",
    "        print(\"\\t\", elem[0] )\n",
    "        \n",
    "print(\"\\n Dans le groupe 3, il y a :\")\n",
    "for elem in liste_triee:\n",
    "    if elem[2] == 2:\n",
    "        print(\"\\t\", elem[0] )\n",
    "\n",
    "print(\"\\n Dans le groupe 4, il y a :\")\n",
    "for elem in liste_triee:\n",
    "    if elem[2] == 3:\n",
    "        print(\"\\t\", elem[0] )\n",
    "\n",
    "print(\"\\n Dans le groupe 5, il y a :\")\n",
    "for elem in liste_triee:\n",
    "    if elem[2] == 4:\n",
    "        print(\"\\t\", elem[0] )\n",
    "        \n",
    "print(\"\\n Dans le groupe 6, il y a :\")\n",
    "for elem in liste_triee:\n",
    "    if elem[2] == 5:\n",
    "        print(\"\\t\", elem[0] )\n",
    "        \n",
    "print(\"\\n Dans le groupe 7, il y a :\")\n",
    "for elem in liste_triee:\n",
    "    if elem[2] == 6:\n",
    "        print(\"\\t\", elem[0] )\n",
    "        \n",
    "print(\"\\n Dans le groupe 8, il y a :\")\n",
    "for elem in liste_triee:\n",
    "    if elem[2] == 7:\n",
    "        print(\"\\t\", elem[0] )\n",
    "\n",
    "\n",
    "#Des librairies python qui convertissent en table HMTL, faut que je fasse de la dataviz\n",
    "# chord visualisation est une librairie faisant des dataviz, Seaborn aussi, BOKEH aussi, Plotly (MatPlotLib est la base de tous), Parallel Plot est cool.\n",
    "# On peut utiliser XSLT via LXML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voici le plan et l'agencement des conférences tel qu'il est indiqué ici : https://graz-2019.tei-c.org/wp-content/uploads/2019/09/ProgrammheftFINAL.pdf\n",
    "\n",
    "TEI, formal ontologies,controlled vocabularies and Linked OpenData :\n",
    "- Making Linkable Data from Account Books: Bookkeeping Ontology in the Digital Edition Publishing Cooperative for Historical Accounts\n",
    "- Inscriptions, Hieroglyphs, Linguistics and Beyond! The Corpus of Classic Mayan as an Ontological Information Resource\n",
    "- Modeling FRBR Entities and their Relationships with TEI: A Look at HallerNet Bibliographic Descriptions\n",
    "- Referencing an editorial ontology from the TEI: An attempt to overcome informal typologies\n",
    "- Text Graph Ontology. A Semantic Web approach to represent genetic scholarly editions\n",
    "\n",
    "TEI and models of text :\n",
    "- A realistic theory of textuality and its consequences on digital text representation\n",
    "- Genesis and Variance: From Letter to Literature\n",
    "- Between freedom and formalisation: a hypergraph model for representing the nature of text\n",
    "- Reflecting the Influence of Technology on Models of Text in Scholarly Digital Editing\n",
    "- Introducing Objectification: when is an <object> a <place> ?\n",
    "- An Encoding Strategic Proposal of “Ruby” Texts: Examples from Japanese Texts\n",
    "- Referencing annotations as a core concept of the hallerNet edition and research platform\n",
    "- Recreating history through events\n",
    "- Document Modeling with the TEI Critical Apparatus\n",
    "- Exploring TEI structures to find distinctive features of text types\n",
    "- Reconceiving TEI models of theatrical performance text with reference to promptbooks\n",
    "- What is a Line? Encoding and Counting Lines in Early Modern Dramatic Texts\n",
    "    \n",
    "TEI across corpora,languages, and cultures :\n",
    "- Growing collections of TEI texts: Some lessons from SARIT\n",
    "- Towards larger corpora of Indic texts: For now, minimize metatext\n",
    "- Encoding history in TEI: A corpus-oriented approach for investigating Tibetan historiography\n",
    "- Advantages and challenges of tokenized TEI\n",
    "- A sign of the times: medieval punctuation, its encoding and its rendition in modern times\n",
    "\n",
    "TEI annotation and publication :\n",
    "- Analyzing and Visualizing Uncertain Knowledge: Introducing the PROVIDEDH Open Science Platform\n",
    "- The Prefabricated Website: Who Needs a Server Anyway?\n",
    "- correspSearch v2 –New ways of exploring correspondence\n",
    "- Validating @selector: a regular expression adventure\n",
    "- TEI encoding of correspondence: A community effort\n",
    "\n",
    "TEI simplification and extension :\n",
    "- Opportunities and challenges of the TEI for scholarly journals in the Humanities\n",
    "- Archiving a TEI project FAIRly\n",
    "- Creating high-quality print from TEI documents\n",
    "- Native-TEI dialectal dictionary for Bavarian dialects in Austria: data structure, software and workflow\n",
    "- An Attempt of Dissemination of TEI in a TEI-underdeveloped country: Activities of the SIG EAJ\n",
    "- Refining the Current Teaching Methodology of the TEI through the Analysis of Server Logs\n",
    "- Using Github and its Integrations to Create, Test, and Deploy a Digital Edition\n",
    "\n",
    "TEI environments and infrastructures :\n",
    "- Parla-CLARIN: TEI guidelines for corpora of parliamentary proceedings\n",
    "- Challenges in encoding parliamentary data: between applause and interjections\n",
    "- A TEI customization for the description of paper and watermarks\n",
    "- How we tripled our encoding speed in the Digital Victorian Periodical Project\n",
    "- Manuscripta-The editor from past to future\n",
    "- Highlighting Our Examples: encoding XML examples in pedagogical contexts\n",
    "- Case Study TEI Customization: A Restricted TEI Format for Edition Open Access (EOA)\n",
    "- In search of comity: TEI for distant reading\n",
    "\n",
    "TEI and beyond :interactions, interchange, integrations and interoperability :\n",
    "- Using Machine Learning for the Automated Classification of Stage Directions in TEI-Encoded Drama Corpora\n",
    "- TEI XML and Delta Format Interchangeability\n",
    "\n",
    "TEI and non-XML technologies :\n",
    "- Five Centuries of History in a Network\n",
    "- Introducing an Open, Dynamic and Efficient Lexical Data Access for TEI-encoded Dictionaries on the Internet\n",
    "- Getting Along with Relational Databases\n",
    "- Using Microsoft Word for preparing XML TEI-compliant digital editions\n",
    "- Scaling up Automatic Structuring of Manuscript Sales Catalogues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ne pas le lancer, je stock ici l'exemple pour clusteriser des documents\n",
    "for abstract in documents:\n",
    "    X = vectorizer.transform([abstract])\n",
    "    predicted = model.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modelling ###\n",
    "\n",
    "Ensuite, je vais tenter de déterminer des sujets aux cluster, c'est à dire déterminer grâce aux mots de chaque cluster des thèmes propres à chaque cluster grâce à une étude de chacun des termes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/formation1/Documents/Cours/dossier_python/venv/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:1466: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  idf = np.log(n_samples / df) + 1\n"
     ]
    }
   ],
   "source": [
    "transformer = TfidfTransformer(smooth_idf=False)\n",
    "x_tfidf = transformer.fit_transform(X)\n",
    "xtfidf_norm = normalize(x_tfidf, norm='l1', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NMF(alpha=0.0, beta_loss='frobenius', init=None, l1_ratio=0.0, max_iter=200,\n",
       "    n_components=8, random_state=None, shuffle=False, solver='cd', tol=0.0001,\n",
       "    verbose=0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_topics = 8 #définition du nombre de topics\n",
    "model = NMF(n_components=num_topics) \n",
    "model.fit(xtfidf_norm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
