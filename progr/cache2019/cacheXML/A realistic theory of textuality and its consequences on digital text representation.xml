<body>
<p>The theoretical debate about digital textuality in the last decades has been deeply
               influenced by the post-modernist theory. If this influence was quite apparent in the
               debates about the hypertext, we can find some of its fundamental tenets also in text
               encoding and digital scholarly editing theories (Landow 1997; McGann 2001). As a
               consequence, even in these areas we can find a general and strong support to
               anti-realist or constructivist notions about textuality and its digital
               representation (Patrick Shale 2013a and 2013b; Ciula and Marras 2016). These views
               oscillate from radical ontological stances, epitomized by this notorious McGann’s
               sentence: <q>What is text? I am not so naive as to imagine that question could ever
                  be finally settled. Asking such a question is like asking <q>How long is the coast
                     of England?</q>
</q> (McGann 2002); to weaker epistemological or pragmatist
               stances that advocate for the plurality of the textual re-representations (the double
                  <mentioned>re</mentioned> is due to the fact that a text is in itself a
               representation).</p>
<p>In this paper I propose a weak realist theory of (digital) textuality (somehow along
               the lines or the <emph>new realism </emph>movement in the recent philosophical debate
               (Ferraris, Bilgrami, and De Caro 2012)) building on the theory of notation developed
               by Nelson Goodman in its <hi rend="italic">Languages of Art</hi> (Goodman 1968) and
               the theory of intentional systems devised by Daniel Dennet (Dennet 1987 and for an
               introductory recapitulation Dennett 2009). In brief: texts are spatiotemporal
               artifacts that have causal roles in our cognitive understanding. As Dennet explains,
               we as a species, tend to adopt the <hi rend="italic">intentional stance</hi> to
               explain the rationales and functions (or meaning) of complex systems, that consists
               in attributing meaning and beliefs to those artifacts. Documents are <hi rend="italic">prima facie</hi> interpreted as intentional artifacts that convey
               meanings. At this level what really counts is the possibility to identify and fix the
               notational nature of the text, because it’s primarily that notational nature that has
               a causal role in the chain that starts form perception and ends in the cognitive work
               of interpretation (meaning attribution).</p>
<p>The general nature of the notational systems has been analyzed by another philosopher
               of the preceding generation, Nelson Goodman. In his influent book (for many and
               diverse reasons) <hi rend="italic">The languages of Art, he affirms </hi>that textual
               artifacts have the distinctive property to be <q>in a definite notation, consisting
                  of certain signs or characters that are to be combined by concatenation</q>
               (Goodman 1968, 116); this <hi rend="italic">notational condition</hi> provides a <hi rend="italic">principium individuationis</hi> for the text.</p>
<p>Hence, a (digital) representation of a text is adequate if and only if it has the
               property of having the <q>sameness of spelling</q>: the exact correspondence as
               sequences of letters, spaces, punctuation marks. All the other properties are either
               contingent (if they are material) or derivative (all the cognitive objects produced
               in the act of reading). Of course, one can or will try to adopt the other two level
               of explanation identified by Dennett (the <hi rend="italic">design stance</hi> and
               the <hi rend="italic">physical stance</hi>, that can be translated in our context
               into various technical approaches in textual studies), but this happens only for
               limited occasions and in particular conditions.</p>
<p>Moving from this general theoretical account most of the recurring debates about the
               pros and cons of one encoding metalanguage or digital modeling strategy over another
               can be reframed into a moderate pluralistic methodological framework, where the
               unique central point is the correct (as far as possible) representation of the
               characters sequences of the textual artifacts that are generally used as
               text-documents of a specific text-work, to which all other properties can be
               attached. I tend to conceive a full-fledged stand-off markup strategy as the most
               natural way of giving a computational modeling of the textual reality, but other
               strategies and notational approaches (like XML inline markup) can have technical,
               pragmatic and cultural affordances that, under many respects, are advantageous.</p>
</body>