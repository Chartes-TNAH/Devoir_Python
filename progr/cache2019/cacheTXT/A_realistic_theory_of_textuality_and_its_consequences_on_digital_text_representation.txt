 The theoretical debate about digital textuality in the last decades has been deeply                influenced by the post-modernist theory. If this influence was quite apparent in the                debates about the hypertext, we can find some of its fundamental tenets also in text                encoding and digital scholarly editing theories (Landow 1997; McGann 2001). As a                consequence, even in these areas we can find a general and strong support to                anti-realist or constructivist notions about textuality and its digital                representation (Patrick Shale 2013a and 2013b; Ciula and Marras 2016). These views                oscillate from radical ontological stances, epitomized by this notorious McGann’s                sentence: What is text? I am not so naive as to imagine that question could ever                   be finally settled. Asking such a question is like asking How long is the coast                      of England?  (McGann 2002); to weaker epistemological or pragmatist                stances that advocate for the plurality of the textual re-representations (the double                   re is due to the fact that a text is in itself a                representation). In this paper I propose a weak realist theory of (digital) textuality (somehow along                the lines or the new realism movement in the recent philosophical debate                (Ferraris, Bilgrami, and De Caro 2012)) building on the theory of notation developed                by Nelson Goodman in its Languages of Art (Goodman 1968) and                the theory of intentional systems devised by Daniel Dennet (Dennet 1987 and for an                introductory recapitulation Dennett 2009). In brief: texts are spatiotemporal                artifacts that have causal roles in our cognitive understanding. As Dennet explains,                we as a species, tend to adopt the intentional stance to                explain the rationales and functions (or meaning) of complex systems, that consists                in attributing meaning and beliefs to those artifacts. Documents are prima facie interpreted as intentional artifacts that convey                meanings. At this level what really counts is the possibility to identify and fix the                notational nature of the text, because it’s primarily that notational nature that has                a causal role in the chain that starts form perception and ends in the cognitive work                of interpretation (meaning attribution). The general nature of the notational systems has been analyzed by another philosopher                of the preceding generation, Nelson Goodman. In his influent book (for many and                diverse reasons) The languages of Art, he affirms that textual                artifacts have the distinctive property to be in a definite notation, consisting                   of certain signs or characters that are to be combined by concatenation                (Goodman 1968, 116); this notational condition provides a principium individuationis for the text. Hence, a (digital) representation of a text is adequate if and only if it has the                property of having the sameness of spelling: the exact correspondence as                sequences of letters, spaces, punctuation marks. All the other properties are either                contingent (if they are material) or derivative (all the cognitive objects produced                in the act of reading). Of course, one can or will try to adopt the other two level                of explanation identified by Dennett (the design stance and                the physical stance, that can be translated in our context                into various technical approaches in textual studies), but this happens only for                limited occasions and in particular conditions. Moving from this general theoretical account most of the recurring debates about the                pros and cons of one encoding metalanguage or digital modeling strategy over another                can be reframed into a moderate pluralistic methodological framework, where the                unique central point is the correct (as far as possible) representation of the                characters sequences of the textual artifacts that are generally used as                text-documents of a specific text-work, to which all other properties can be                attached. I tend to conceive a full-fledged stand-off markup strategy as the most                natural way of giving a computational modeling of the textual reality, but other                strategies and notational approaches (like XML inline markup) can have technical,                pragmatic and cultural affordances that, under many respects, are advantageous. 