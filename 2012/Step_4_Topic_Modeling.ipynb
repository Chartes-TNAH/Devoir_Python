{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEI Conference 2012 #\n",
    "\n",
    "## Etape 4 : Topic Modeling ##\n",
    "\n",
    "Jusqu'à maintenant, nous avons fait de la classification non supervisée, où l'ordinateur distribuait librement chaque document en fonction de la répartition de ses mots.\n",
    "\n",
    "Nous allons ici développer un peu cela en faisant du topic modeling : cette fois-ci, chaque mot est assimilé à un sujet (ou plus précisément à une variété de sujets dont la proximité est donnée sous la forme d'une coefficient relatif à chacun des sujets), et l'addition de chacun de ces coefficients pour chaque mot détermine pour chaque abstract un sujet (ou plutôt une liste de thèmes dont la proximité est indiquée sous la forme d'un coefficient). Grâce à cela, on peut déterminer des clusters fondés non plus la répétitivité de certains termes ou de certaines constructions, mais sur les champs lexicaux utilisés.\n",
    "\n",
    "Nous nous appuierons pour cela sur divers librairies qu'il convient d'importer.\n",
    "\n",
    "## Les packages ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# SpaCy pour la lemmatisation\n",
    "import spacy\n",
    "\n",
    "# Librairies pour la représentation en schéma\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Active le chargement pour Gensim\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "#Permet d'utiliser LDA Mallet dans la seconde partie de l'analyse\n",
    "import git\n",
    "from gensim.test.utils import common_corpus, common_dictionary\n",
    "from gensim.models.wrappers import LdaMallet\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Les stopwords ##\n",
    "\n",
    "Je définis des stopwords, c'est à dire la liste de mots qui ne doivent pas être pris en compte, ces mots étant généralement des mots grammaticaux. Les mots grammaticaux sont généralement très nombreux mais portent un très faible poids sémantique. Il faut donc les retirer pour éviter qu'ils n'occultent les autres mots à cause de leur surreprésentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use']) #permet d'ajouter certains termes que j'ai décrété"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choisir son corpus ##\n",
    "\n",
    "Il faut activer l'une des deux cellules (au choix) pour charger en mémoire le corpus traité de la manière que vous voulez.\n",
    "\n",
    "Le meilleur choix pour un premier test est de choisir le texte lemmatisé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pour utiliser les textes stemmés, c'est ici\n",
    "import os\n",
    "documents = []\n",
    "Path = \"./cache2012/cacheSTEM/\"\n",
    "filelist = os.listdir(Path) #filelist est une liste regroupant tous les chemins vers les différents abstracts.\n",
    "\n",
    "for abstract in filelist:\n",
    "    with open(Path + abstract, \"r\", encoding=\"UTF-8\") as y:\n",
    "        texte = y.read()\n",
    "        documents.append(texte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pour utiliser les textes lemmatisés, c'est ici\n",
    "import os\n",
    "documents = []\n",
    "Path = \"./cache2012/cacheLEM/\"\n",
    "filelist = os.listdir(Path) #filelist est une liste regroupant tous les chemins vers les différents abstracts.\n",
    "\n",
    "for abstract in filelist:\n",
    "    with open(Path + abstract, \"r\", encoding=\"UTF-8\") as y:\n",
    "        texte = y.read()\n",
    "        documents.append(texte)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic modeling partie 1 : définir des topic ##\n",
    "\n",
    "### Séquençage et nettoyage ###\n",
    "\n",
    "Tout d'abord, je définis une fonction qui séquence chaque abstract en mot et retire la ponctuation. C'est l'étape du préprocessing, c'est à dire formatter le document afin de le rendre lisible par gensim. Gensim a sa propre méthode de séquençage, que nous utiliserons. Nous avons déjà lemmatisé les documents, mais il est utile de le refaire, car cette fois-ci nous utiliserons le lemmatiseur intégré à Gensim, qui permet alors de rendre un document utilisable par Gensim.\n",
    "\n",
    "Chaque document, sous la forme d'une str, devient une liste de str."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['get', 'critical', 'with', 'the', 'apparatus', 'how', 'to', 're', 'think', 'the', 'tei', 'encoding', 'of', 'critical', 'edition', 'the', 'critical', 'apparatus', 'module', 'chapter', 'of', 'the', 'tei', 'be', 'both', 'central', 'and', 'controversial', 'one', 'central', 'because', 'pron', 'be', 'mandatory', 'for', 'each', 'critical', 'editor', 'to', 'master', 'and', 'use', 'pron', 'controversial', 'because', 'most', 'editor', 'complain', 'about', 'pron', 'feature', 'the', 'module', 'have', 'not', 'be', 'revise', 'since', 'tei', 'and', 'be', 'therefore', 'in', 'need', 'of', 'rehaule', 'keeping', 'in', 'mind', 'the', 'expectation', 'and', 'need', 'of', 'user', 'in', 'continuously', 'evolve', 'digital', 'environment', 'the', 'possibility', 'in', 'term', 'of', 'processing', 'display', 'or', 'even', 'print', 'have', 'to', 'be', 'balance', 'with', 'the', 'need', 'for', 'an', 'encode', 'scheme', 'manageable', 'manually', 'by', 'scholar', 'without', 'have', 'to', 'rely', 'on', 'third', 'party', 'tool', 'to', 'handle', 'the', 'tag', 'process', 'in', 'this', 'paper', 'pron', 'will', 'discuss', 'the', 'current', 'state', 'of', 'the', 'critical', 'apparatus', 'module', 'chapter', 'with', 'pron', 'strength', 'and', 'shortcoming', 'pron', 'will', 'also', 'explain', 'the', 'step', 'that', 'have', 'be', 'take', 'so', 'far', 'towards', 'revision', 'of', 'the', 'module', 'and', 'the', 'difficulty', 'pron', 'have', 'meet', 'finally', 'pron', 'will', 'set', 'out', 'the', 'aim', 'pron', 'be', 'willing', 'to', 'reach', 'and', 'possible', 'solution', 'towards', 'that', 'end', 'module', 'that', 'would', 'answer', 'the', 'various', 'and', 'varied', 'need', 'of', 'wide', 'range', 'of', 'user', 'whether', 'pron', 'be', 'encode', 'print', 'base', 'apparatus', 'bear', 'digital', 'one', 'or', 'work', 'on', 'automatic', 'collation']]\n"
     ]
    }
   ],
   "source": [
    "def sent_to_words(documents):\n",
    "    for document in documents:\n",
    "        yield(gensim.utils.simple_preprocess(str(document), deacc=True))  # deacc=True retire la ponctuation\n",
    "\n",
    "data_words = list(sent_to_words(documents)) #je lemmatise les abstracts et les stock dans une variable\n",
    "\n",
    "print(data_words[:1]) #Voici à quoi ressemble un extrait du corpus lemmatisé"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construire les bigrammes et les trigrammes ###\n",
    "\n",
    "Nous allons maintenant construire les bigrammes et les trigrammes, c'est à dire rassembler les mots par groupe de 2 ou 3 en fonction de leur ressemblance sémantique, permettant de dégager ainsi des débuts de regroupement.\n",
    "\n",
    "Nous créons donc les modèles pour créer les bigrammes et les trigrammes, qui donnent une liste de mots assignés à chaque mot un à un. Nous testons à la fin un exemple de la liste de mot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['get', 'critical', 'with', 'the', 'apparatus', 'how', 'to', 're', 'think', 'the', 'tei', 'encoding', 'of', 'critical', 'edition', 'the', 'critical', 'apparatus', 'module', 'chapter', 'of', 'the', 'tei', 'be', 'both', 'central', 'and', 'controversial', 'one', 'central', 'because', 'pron', 'be', 'mandatory', 'for', 'each', 'critical', 'editor', 'to', 'master', 'and', 'use', 'pron', 'controversial', 'because', 'most', 'editor', 'complain', 'about', 'pron', 'feature', 'the', 'module', 'have', 'not', 'be', 'revise', 'since', 'tei', 'and', 'be', 'therefore', 'in', 'need', 'of', 'rehaule', 'keeping', 'in', 'mind', 'the', 'expectation', 'and', 'need', 'of', 'user', 'in', 'continuously', 'evolve', 'digital', 'environment', 'the', 'possibility', 'in', 'term', 'of', 'processing', 'display', 'or', 'even', 'print', 'have', 'to', 'be', 'balance', 'with', 'the', 'need', 'for', 'an', 'encode', 'scheme', 'manageable', 'manually', 'by', 'scholar', 'without', 'have', 'to', 'rely', 'on', 'third', 'party', 'tool', 'to', 'handle', 'the', 'tag', 'process', 'in', 'this', 'paper', 'pron', 'will', 'discuss', 'the', 'current', 'state', 'of', 'the', 'critical', 'apparatus', 'module', 'chapter', 'with', 'pron', 'strength', 'and', 'shortcoming', 'pron', 'will', 'also', 'explain', 'the', 'step', 'that', 'have', 'be', 'take', 'so', 'far', 'towards', 'revision', 'of', 'the', 'module', 'and', 'the', 'difficulty', 'pron', 'have', 'meet', 'finally', 'pron', 'will', 'set', 'out', 'the', 'aim', 'pron', 'be', 'willing', 'to', 'reach', 'and', 'possible', 'solution', 'towards', 'that', 'end', 'module', 'that', 'would', 'answer', 'the', 'various', 'and', 'varied', 'need', 'of', 'wide', 'range', 'of', 'user', 'whether', 'pron', 'be', 'encode', 'print', 'base', 'apparatus', 'bear', 'digital', 'one', 'or', 'work', 'on', 'automatic', 'collation']\n"
     ]
    }
   ],
   "source": [
    "# Construit les modèles bigramme et trigramme\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # plus le paramètre threshold est haut, plus il est difficile de construire des groupes de mots\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "# On applique la méthode et les modèles dans une variable\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# Un exemple d'une trigrammes avec le premier mot.\n",
    "print(trigram_mod[bigram_mod[data_words[0]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous insérons ensuite les modèles dans des fonctions :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puis ensuite nous l'appliquons à notre set de textes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['critical', 'apparatus', 'think', 'encode', 'critical', 'edition', 'critical', 'apparatus', 'chapter', 'central', 'pron', 'mandatory', 'critical', 'editor', 'editor', 'complain', 'feature', 'module', 'revise', 'therefore', 'need', 'rehaule', 'keeping', 'mind', 'expectation', 'need', 'user', 'continuously', 'evolve', 'digital', 'environment', 'possibility', 'term', 'processing', 'display', 'even', 'print', 'balance', 'encode', 'scheme', 'manageable', 'manually', 'scholar', 'rely', 'third', 'party', 'tool', 'handle', 'tag', 'process', 'paper', 'pron', 'discuss', 'current', 'state', 'critical', 'apparatus', 'chapter', 'also', 'explain', 'step', 'take', 'far', 'revision', 'module', 'meet', 'finally', 'set', 'pron', 'willing', 'reach', 'possible', 'solution', 'end', 'module', 'answer', 'various', 'varied', 'need', 'wide', 'range', 'user', 'work', 'automatic', 'collation']]\n"
     ]
    }
   ],
   "source": [
    "# On retire les stop words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# On forme les bigrammes\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# On lemmatise le texte grâce à spacy. J'utilise le corpus d'entraînement le plus petit à notre disposition.\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "# Je ne lemmatise qu'en ne gardant les noms, les adjectifs, les verbes et les adverbes\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "# Exemple du résultat sur le premier abstract\n",
    "print(data_lemmatized[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuite, les mots de chaque texte sont encodés sous la forme de tuple où le premier entier est son identifiant et où le second est son nombre d'occurences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 3), (3, 1), (4, 1), (5, 1), (6, 2), (7, 1), (8, 1), (9, 1), (10, 5), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 2), (17, 2), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 3), (35, 3), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1), (41, 1), (42, 1), (43, 3), (44, 1), (45, 1), (46, 1), (47, 1), (48, 1), (49, 1), (50, 1), (51, 1), (52, 1), (53, 1), (54, 1), (55, 1), (56, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, 2), (64, 1), (65, 1), (66, 1), (67, 1), (68, 1)]]\n"
     ]
    }
   ],
   "source": [
    "# Je crée le dictionnaire dans lequel est stocké chaque mot différent en clé et un identifiant (nombre entier) en valeur\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# J'associe le corpus à une variable lemmatisé\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Je crée une liste avec à l'intérieur une liste de deux entiers où le premier représente un mot (la valeur de la clé du dictionnaire id2word) et le second est son nombre d'occurence\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# Visualisation un peu barbare sur le premier corpus\n",
    "print(corpus[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour mieux le comprendre, voici une visualisation adaptée à un être humain :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('also', 1),\n",
       "  ('answer', 1),\n",
       "  ('apparatus', 3),\n",
       "  ('automatic', 1),\n",
       "  ('balance', 1),\n",
       "  ('central', 1),\n",
       "  ('chapter', 2),\n",
       "  ('collation', 1),\n",
       "  ('complain', 1),\n",
       "  ('continuously', 1),\n",
       "  ('critical', 5),\n",
       "  ('current', 1),\n",
       "  ('digital', 1),\n",
       "  ('discuss', 1),\n",
       "  ('display', 1),\n",
       "  ('edition', 1),\n",
       "  ('editor', 2),\n",
       "  ('encode', 2),\n",
       "  ('end', 1),\n",
       "  ('environment', 1),\n",
       "  ('even', 1),\n",
       "  ('evolve', 1),\n",
       "  ('expectation', 1),\n",
       "  ('explain', 1),\n",
       "  ('far', 1),\n",
       "  ('feature', 1),\n",
       "  ('finally', 1),\n",
       "  ('handle', 1),\n",
       "  ('keeping', 1),\n",
       "  ('manageable', 1),\n",
       "  ('mandatory', 1),\n",
       "  ('manually', 1),\n",
       "  ('meet', 1),\n",
       "  ('mind', 1),\n",
       "  ('module', 3),\n",
       "  ('need', 3),\n",
       "  ('paper', 1),\n",
       "  ('party', 1),\n",
       "  ('possibility', 1),\n",
       "  ('possible', 1),\n",
       "  ('print', 1),\n",
       "  ('process', 1),\n",
       "  ('processing', 1),\n",
       "  ('pron', 3),\n",
       "  ('range', 1),\n",
       "  ('reach', 1),\n",
       "  ('rehaule', 1),\n",
       "  ('rely', 1),\n",
       "  ('revise', 1),\n",
       "  ('revision', 1),\n",
       "  ('scheme', 1),\n",
       "  ('scholar', 1),\n",
       "  ('set', 1),\n",
       "  ('solution', 1),\n",
       "  ('state', 1),\n",
       "  ('step', 1),\n",
       "  ('tag', 1),\n",
       "  ('take', 1),\n",
       "  ('term', 1),\n",
       "  ('therefore', 1),\n",
       "  ('think', 1),\n",
       "  ('third', 1),\n",
       "  ('tool', 1),\n",
       "  ('user', 2),\n",
       "  ('varied', 1),\n",
       "  ('various', 1),\n",
       "  ('wide', 1),\n",
       "  ('willing', 1),\n",
       "  ('work', 1)]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Voici une version un peu plus lisible. C'est purement de la term-frequency, à l'état brut.\n",
    "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construire l'attribution des sujets ###\n",
    "\n",
    "La LDA (latent Dirichlet Allocation) est une méthode permettant de définir que si certains thèmes sont observés dans un corpus, nous pouvons alors dire que chacun des mots du corpus peuvent être attribués à l'un de ces thèmes.\n",
    "\n",
    "Nous allons appliquer l'algorithme LDA de Gensim à notre corpus en souhaitant 7 topics dans lequel les corpus seront rangés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Je construis mon algorithme de LDA\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=7, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1, #update_every détermine avec quelle régularité il faut remettre le modèle à jour\n",
    "                                           chunksize=100, #chunksize définit le nombre de documents à utiliser dans chaque passage\n",
    "                                           passes=10, #passes est le nombre total de passage\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons alors trouver les mots clés les plus pertinents de chaque cluster que la machine a trouvé :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.026*\"mad\" + 0.015*\"record\" + 0.010*\"make\" + 0.009*\"project\" + '\n",
      "  '0.008*\"text\" + 0.008*\"type\" + 0.008*\"include\" + 0.007*\"system\" + '\n",
      "  '0.006*\"note\" + 0.006*\"name\"'),\n",
      " (1,\n",
      "  '0.026*\"text\" + 0.018*\"source\" + 0.013*\"word\" + 0.011*\"textual\" + '\n",
      "  '0.011*\"process\" + 0.011*\"datum\" + 0.010*\"unit\" + 0.008*\"tag\" + 0.006*\"pron\" '\n",
      "  '+ 0.006*\"tool\"'),\n",
      " (2,\n",
      "  '0.039*\"text\" + 0.017*\"project\" + 0.017*\"service\" + 0.013*\"pron\" + '\n",
      "  '0.013*\"encode\" + 0.012*\"collection\" + 0.010*\"way\" + 0.010*\"model\" + '\n",
      "  '0.010*\"support\" + 0.010*\"level\"'),\n",
      " (3,\n",
      "  '0.022*\"document\" + 0.017*\"content\" + 0.013*\"also\" + 0.011*\"site\" + '\n",
      "  '0.010*\"romantic\" + 0.008*\"circle\" + 0.008*\"web\" + 0.008*\"directly\" + '\n",
      "  '0.008*\"browser\" + 0.008*\"css\"'),\n",
      " (4,\n",
      "  '0.013*\"project\" + 0.012*\"tool\" + 0.012*\"tag\" + 0.011*\"encode\" + '\n",
      "  '0.009*\"pron\" + 0.009*\"word\" + 0.009*\"structure\" + 0.007*\"support\" + '\n",
      "  '0.007*\"transcription\" + 0.007*\"present\"'),\n",
      " (5,\n",
      "  '0.021*\"teach\" + 0.014*\"academic\" + 0.011*\"teaching\" + 0.011*\"course\" + '\n",
      "  '0.008*\"project\" + 0.008*\"type\" + 0.008*\"specific\" + 0.008*\"different\" + '\n",
      "  '0.008*\"training\" + 0.006*\"research\"'),\n",
      " (6,\n",
      "  '0.014*\"linguistic\" + 0.010*\"content\" + 0.010*\"annotation\" + '\n",
      "  '0.010*\"creation\" + 0.010*\"automatic\" + 0.007*\"process\" + 0.007*\"also\" + '\n",
      "  '0.007*\"analysis\" + 0.007*\"language\" + 0.007*\"list\"')]\n"
     ]
    }
   ],
   "source": [
    "# J'affiche les mot-clés des 7 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Critique du calcul : le score de cohérence ###\n",
    "\n",
    "La perplexité et le score de cohérence permettent de définir si les groupes générés par topic sont cohérents, c'est à dire s'il y a trop ou pas assez de groupes, et si les groupes sont vraiment représentés par ce topic, si ce topic est vraiment au coeur des textes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity:  -7.076587570608902\n",
      "\n",
      "Coherence Score:  0.37073201142673795\n"
     ]
    }
   ],
   "source": [
    "# Je calcule la Perplexité\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # permet de savoir si le modèle est bon. Plus c'est bas, mieux c'est.\n",
    "\n",
    "# Je calcule le score de cohérence.\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation ###\n",
    "\n",
    "Grâce à cette carte interactive, nous pouvons visualiser la répartition des thèmes et des mots-clés dans chaque cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el103611404428975305768808566211\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el103611404428975305768808566211_data = {\"mdsDat\": {\"x\": [0.06843207440424323, -0.010047786471998419, -0.15907170130754475, 0.08331082559074586, -0.027164529790917775, 0.013244885637742084, 0.03129623193772976], \"y\": [-0.13286957106240643, -0.050845297294710955, -0.03605792563846652, 0.05307658820617781, 0.10737895650802314, 0.028557416002119248, 0.03075983327926389], \"topics\": [1, 2, 3, 4, 5, 6, 7], \"cluster\": [1, 1, 1, 1, 1, 1, 1], \"Freq\": [32.843326568603516, 20.277517318725586, 16.138782501220703, 11.683633804321289, 11.080514907836914, 4.595500469207764, 3.3807272911071777]}, \"tinfo\": {\"Term\": [\"mad\", \"text\", \"document\", \"project\", \"content\", \"service\", \"word\", \"record\", \"collection\", \"teach\", \"linguistic\", \"tag\", \"also\", \"tool\", \"site\", \"type\", \"annotation\", \"support\", \"romantic\", \"different\", \"unit\", \"source\", \"research\", \"course\", \"academic\", \"specific\", \"level\", \"structure\", \"framework\", \"circle\", \"mad\", \"record\", \"historical\", \"zone\", \"financial\", \"\\ufb01nancial_record\", \"namepart\", \"prototype\", \"transaction\", \"double_entry\", \"currency\", \"name\", \"signi\\ufb01cant\", \"personal\", \"bookkeepe\", \"railroad\", \"introduction\", \"proust\", \"account\", \"readable\", \"recommendation\", \"normalize\", \"standardization\", \"accounting\", \"ledger\", \"item\", \"economic\", \"merchant\", \"commodity\", \"digitize\", \"handle\", \"history\", \"variant\", \"make\", \"even\", \"represent\", \"note\", \"keep\", \"value\", \"include\", \"type\", \"system\", \"project\", \"order\", \"page\", \"collection\", \"change\", \"semantic\", \"digital\", \"information\", \"text\", \"standard\", \"element\", \"encode\", \"pron\", \"datum\", \"transcription\", \"source\", \"research\", \"process\", \"category\", \"hieroglyphic\", \"unit\", \"egyptology\", \"structural\", \"black\", \"unicode\", \"egyptian\", \"position\", \"relevant\", \"internal\", \"import\", \"span\", \"segmentation\", \"hack\", \"box\", \"sign\", \"transliteration\", \"adapt\", \"proprietary\", \"correspond\", \"scientific\", \"computer\", \"pagination\", \"section\", \"query\", \"pre\", \"specify\", \"precise\", \"analytical\", \"character\", \"sentence\", \"word\", \"textual\", \"platform\", \"code\", \"source\", \"technology\", \"annotation\", \"text\", \"contain\", \"process\", \"datum\", \"mean\", \"tag\", \"old\", \"usually\", \"surface\", \"complex\", \"display\", \"standard\", \"tool\", \"software\", \"metadata\", \"analysis\", \"processing\", \"specific\", \"critical\", \"pron\", \"type\", \"design\", \"work\", \"encode\", \"language\", \"system\", \"page\", \"site\", \"romantic\", \"circle\", \"browser\", \"style\", \"race\", \"link\", \"omeka\", \"directly\", \"legacy\", \"html\", \"inherent\", \"cms\", \"separation\", \"server\", \"racial\", \"drive\", \"discourse\", \"navigational\", \"easy\", \"leverage\", \"static\", \"power\", \"richness\", \"typical\", \"largely\", \"programming\", \"expressive\", \"boilerplate\", \"css\", \"document\", \"content\", \"capability\", \"web\", \"also\", \"transformation\", \"individual\", \"publishing\", \"scholar\", \"presentation\", \"format\", \"however\", \"allow\", \"deliver\", \"much\", \"access\", \"page\", \"encode\", \"pron\", \"language\", \"framework\", \"text\", \"case\", \"design\", \"datum\", \"source\", \"provide\", \"module\", \"selection\", \"participant\", \"want\", \"maker\", \"believe\", \"together\", \"sig\", \"formula\", \"quite\", \"robust\", \"visualize\", \"concern\", \"rely\", \"apparatus\", \"explain\", \"chapter\", \"volume\", \"graduate\", \"reflection\", \"sample\", \"specification\", \"seldom\", \"manually\", \"existence\", \"revise\", \"sat\", \"third\", \"willing\", \"expectation\", \"structure\", \"formation\", \"linguistic\", \"application\", \"tag\", \"complete\", \"tool\", \"article\", \"guideline\", \"end\", \"support\", \"critical\", \"word\", \"project\", \"user\", \"encode\", \"transcription\", \"present\", \"pron\", \"need\", \"new\", \"addition\", \"full\", \"manuscript\", \"language\", \"various\", \"editor\", \"system\", \"include\", \"also\", \"form\", \"service\", \"center\", \"mass\", \"advanced\", \"level\", \"faculty\", \"odd\", \"bridge\", \"institutional\", \"bauman\", \"mid\", \"block\", \"streamline\", \"extensible\", \"dalmau\", \"modular\", \"discovery\", \"emerge\", \"initiative\", \"serve\", \"gap\", \"retrieve\", \"electronic\", \"library\", \"effort\", \"heart\", \"pursuit\", \"uneasy\", \"whereby\", \"minor\", \"edit\", \"scholarly\", \"support\", \"collection\", \"permit\", \"text\", \"model\", \"project\", \"way\", \"build\", \"framework\", \"pron\", \"development\", \"encode\", \"research\", \"approach\", \"digitization\", \"rather\", \"web\", \"provide\", \"language\", \"system\", \"content\", \"digital\", \"work\", \"future\", \"access\", \"set\", \"follow\", \"mailing\", \"conversion\", \"qualitative\", \"domain\", \"exploration\", \"collaboration\", \"professional\", \"creation\", \"automatic\", \"maintenance\", \"corpu\", \"restrict\", \"mail\", \"retrieval\", \"linguistically\", \"recourse\", \"invaluable\", \"convert\", \"researchable\", \"anywhere\", \"separately\", \"curate\", \"distinct\", \"decade\", \"curation\", \"retiree\", \"coordination\", \"scale\", \"generic\", \"corenlp\", \"linguist\", \"natural\", \"linguistic\", \"list\", \"quantitative\", \"recognition\", \"resource\", \"annotation\", \"book\", \"storage\", \"archive\", \"public\", \"analysis\", \"content\", \"sentence\", \"large\", \"language\", \"also\", \"process\", \"various\", \"structure\", \"text\", \"pron\", \"make\", \"encode\", \"new\", \"form\", \"format\", \"training\", \"teach\", \"respect\", \"intensive\", \"academic\", \"learn\", \"panellist\", \"teaching\", \"school\", \"north\", \"matter\", \"generalise\", \"professionally\", \"instead\", \"ultimately\", \"survival\", \"coverage\", \"train\", \"success\", \"panel\", \"curricula\", \"aim\", \"enhance\", \"summer\", \"rudiment\", \"initiate\", \"initiation\", \"expert\", \"similarity\", \"steep\", \"course\", \"world\", \"likely\", \"workshop\", \"specific\", \"different\", \"experience\", \"people\", \"type\", \"well\", \"project\", \"need\", \"framework\", \"research\", \"tool\", \"pron\", \"also\", \"standard\", \"include\", \"form\", \"develop\"], \"Freq\": [40.0, 62.0, 24.0, 32.0, 18.0, 9.0, 18.0, 23.0, 15.0, 5.0, 7.0, 15.0, 19.0, 16.0, 8.0, 21.0, 6.0, 11.0, 7.0, 6.0, 10.0, 29.0, 15.0, 4.0, 3.0, 5.0, 6.0, 9.0, 10.0, 6.0, 39.32551193237305, 21.87318229675293, 7.948763847351074, 7.946813583374023, 6.21425724029541, 6.2087202072143555, 5.35087776184082, 5.34334135055542, 5.341363906860352, 5.339262008666992, 5.33905029296875, 8.931031227111816, 4.471616268157959, 3.6087305545806885, 3.6042447090148926, 3.6029810905456543, 3.6015751361846924, 3.6007883548736572, 7.948781967163086, 2.7356679439544678, 2.7361271381378174, 2.733642578125, 2.7341818809509277, 2.733680248260498, 2.7331879138946533, 2.732694149017334, 2.7329742908477783, 2.732685089111328, 2.7326302528381348, 2.7323217391967773, 5.344654083251953, 7.080744743347168, 7.093740463256836, 14.905004501342773, 4.473476409912109, 8.813619613647461, 9.714485168457031, 4.47196102142334, 5.3437113761901855, 11.435399055480957, 11.460298538208008, 10.558910369873047, 13.190056800842285, 5.349560260772705, 8.82613468170166, 7.957033634185791, 5.336111068725586, 5.343735694885254, 7.088520050048828, 6.211085319519043, 12.36314582824707, 7.0824294090271, 6.216891765594482, 7.99624490737915, 7.970201015472412, 7.085601806640625, 6.2113752365112305, 7.135656356811523, 6.225276470184326, 6.222770690917969, 5.7408528327941895, 4.136959075927734, 8.951692581176758, 3.334235191345215, 3.329596996307373, 2.532259941101074, 2.5296008586883545, 2.5290629863739014, 2.5284571647644043, 2.5254580974578857, 2.525883436203003, 2.5249252319335938, 2.525099992752075, 1.7265617847442627, 1.72649085521698, 1.7263455390930176, 1.724167823791504, 1.7242051362991333, 1.72428560256958, 1.7235102653503418, 1.7243455648422241, 1.72372305393219, 1.7231782674789429, 1.7225879430770874, 1.7221784591674805, 1.72234308719635, 1.7222126722335815, 1.7221089601516724, 1.7221989631652832, 1.7220945358276367, 4.138423442840576, 4.132958889007568, 12.173759460449219, 9.771122932434082, 3.3296778202056885, 3.3291678428649902, 16.99664878845215, 5.740765571594238, 4.134101867675781, 24.289461135864258, 3.328892230987549, 9.770608901977539, 9.764104843139648, 4.141037940979004, 7.369050025939941, 2.528108596801758, 2.5272812843322754, 3.3313684463500977, 4.1372199058532715, 4.134818077087402, 5.758451461791992, 5.772299289703369, 4.139161109924316, 4.135662078857422, 4.1378350257873535, 4.141077041625977, 3.3285770416259766, 4.1455769538879395, 5.778934955596924, 4.952499866485596, 4.150422096252441, 4.144792079925537, 4.969780921936035, 4.1612701416015625, 4.1610941886901855, 4.143581390380859, 7.777522563934326, 7.01039457321167, 6.244112968444824, 6.235087871551514, 4.707956314086914, 3.953212261199951, 3.1753618717193604, 3.1724274158477783, 6.236745357513428, 2.4110825061798096, 2.4091176986694336, 2.4086966514587402, 2.4084997177124023, 2.4068055152893066, 2.404637336730957, 1.6466505527496338, 1.644347906112671, 1.6436131000518799, 1.6441535949707031, 1.6431580781936646, 1.6424052715301514, 1.6421324014663696, 1.641680359840393, 1.6415979862213135, 1.6415023803710938, 1.6415776014328003, 1.641023874282837, 1.6404953002929688, 1.6405093669891357, 6.231532573699951, 16.176584243774414, 12.381150245666504, 3.1722280979156494, 6.240322113037109, 9.316631317138672, 3.1738991737365723, 2.4091248512268066, 2.4080615043640137, 5.473060131072998, 3.945362091064453, 4.704257488250732, 2.4075446128845215, 3.943265438079834, 2.409041166305542, 3.938603401184082, 3.948559522628784, 4.7227582931518555, 5.494725704193115, 4.729495048522949, 3.945350408554077, 3.1858675479888916, 4.742515563964844, 3.187113046646118, 3.171743154525757, 3.1875803470611572, 3.1828575134277344, 3.178964138031006, 2.2134552001953125, 1.5088694095611572, 1.5085264444351196, 1.507935881614685, 1.5082427263259888, 1.5079635381698608, 1.507422685623169, 1.5067944526672363, 1.507102370262146, 1.506954550743103, 1.5064176321029663, 1.5066171884536743, 2.9160118103027344, 2.213855743408203, 2.2155306339263916, 3.6182432174682617, 1.5102450847625732, 2.2109551429748535, 0.8078994154930115, 0.8078987002372742, 0.806060791015625, 0.8060384392738342, 0.8058903813362122, 0.8056315183639526, 0.8057462573051453, 0.8054616451263428, 0.805701732635498, 0.8054310083389282, 0.8053747415542603, 0.8053818345069885, 5.025774002075195, 2.209710121154785, 3.616150379180908, 2.210379123687744, 6.4380035400390625, 1.5099725723266602, 6.439326763153076, 1.506088137626648, 2.213761329650879, 2.2138783931732178, 3.626685619354248, 3.624551296234131, 5.0315775871276855, 7.161725044250488, 2.9192564487457275, 5.750770092010498, 3.6266443729400635, 3.6259236335754395, 5.042479991912842, 2.9237916469573975, 2.9202287197113037, 2.213207721710205, 2.2135722637176514, 2.2142817974090576, 2.9160614013671875, 2.2145004272460938, 2.220334053039551, 2.223919630050659, 2.2229769229888916, 2.2226388454437256, 2.214512348175049, 8.399474143981934, 3.5581564903259277, 2.1749963760375977, 2.1737897396087646, 4.9418511390686035, 1.486465334892273, 1.4840997457504272, 1.4830914735794067, 1.4829750061035156, 1.4829707145690918, 1.482951283454895, 1.4827686548233032, 1.482733130455017, 1.4822672605514526, 1.4822734594345093, 1.4821993112564087, 1.4822171926498413, 1.4817651510238647, 2.1765055656433105, 2.174078941345215, 2.173698663711548, 2.1723833084106445, 3.5581586360931396, 2.17875075340271, 2.175442934036255, 0.7949922680854797, 0.7949921488761902, 0.7918172478675842, 0.7917694449424744, 0.7916362285614014, 2.1766459941864014, 3.559159517288208, 4.943044662475586, 6.325135231018066, 1.4859428405761719, 19.503931045532227, 4.947812080383301, 8.420171737670898, 4.955929279327393, 3.5609498023986816, 3.5591962337493896, 6.34797477722168, 2.8702120780944824, 6.337843894958496, 4.257214069366455, 2.1758954524993896, 2.176473379135132, 2.178524971008301, 2.8728437423706055, 2.877397298812866, 2.880481243133545, 2.8819024562835693, 2.1950790882110596, 2.1927828788757324, 2.1825497150421143, 2.181813955307007, 2.1808230876922607, 2.177380323410034, 2.1766629219055176, 1.5212244987487793, 1.035706877708435, 1.0351978540420532, 1.0348128080368042, 1.0323556661605835, 1.032334327697754, 1.5200093984603882, 1.9996991157531738, 1.996150016784668, 0.553148627281189, 0.5531483292579651, 0.5531424880027771, 0.5531415939331055, 0.5531409382820129, 0.5524366497993469, 0.5524067878723145, 0.5522242784500122, 0.5522212982177734, 0.5519993305206299, 0.5510714650154114, 0.551064133644104, 0.5508626699447632, 0.5508190393447876, 0.5507918000221252, 0.5507038831710815, 0.550691545009613, 0.5505236387252808, 0.5502002239227295, 0.5456869602203369, 0.545596718788147, 1.0374841690063477, 1.0328235626220703, 2.9681897163391113, 1.5222463607788086, 1.0358214378356934, 1.0219899415969849, 1.5193359851837158, 2.0051660537719727, 1.5192556381225586, 1.0387780666351318, 1.0346462726593018, 1.0331714153289795, 1.5272821187973022, 2.011091709136963, 1.0308057069778442, 1.0386149883270264, 1.5239640474319458, 1.5327397584915161, 1.5363264083862305, 1.0347888469696045, 1.041085124015808, 1.0841306447982788, 1.0525200366973877, 1.0480183362960815, 1.042508840560913, 1.040603756904602, 1.0405582189559937, 1.0370724201202393, 1.2756634950637817, 3.306976079940796, 0.8703693151473999, 0.8701561689376831, 2.090501070022583, 0.8699919581413269, 0.8698036074638367, 1.6828447580337524, 0.8705130219459534, 0.4645693004131317, 0.4645995497703552, 0.4645269513130188, 0.4644976258277893, 0.46444153785705566, 0.4643792510032654, 0.46442604064941406, 0.4644317030906677, 0.4643860161304474, 0.4643682837486267, 0.46435049176216125, 0.46436014771461487, 0.4643154740333557, 0.4643327295780182, 0.46429699659347534, 0.46425721049308777, 0.4642730951309204, 0.4642544984817505, 0.4642908275127411, 0.4642495810985565, 0.4642486870288849, 1.682350516319275, 0.8714536428451538, 0.8721532225608826, 0.8707831501960754, 1.2784345149993896, 1.2763621807098389, 0.8719493746757507, 0.8712372183799744, 1.281895399093628, 0.8735963106155396, 1.2945449352264404, 0.874586820602417, 0.8759222626686096, 0.877347469329834, 0.875798761844635, 0.4781781733036041, 0.4758729338645935, 0.47430410981178284, 0.4729241728782654, 0.4716936945915222, 0.47008293867111206], \"Total\": [40.0, 62.0, 24.0, 32.0, 18.0, 9.0, 18.0, 23.0, 15.0, 5.0, 7.0, 15.0, 19.0, 16.0, 8.0, 21.0, 6.0, 11.0, 7.0, 6.0, 10.0, 29.0, 15.0, 4.0, 3.0, 5.0, 6.0, 9.0, 10.0, 6.0, 40.00753402709961, 23.31621742248535, 8.534384727478027, 8.535643577575684, 6.789307117462158, 6.788511276245117, 5.922108173370361, 5.91778039932251, 5.916881084442139, 5.915844917297363, 5.915761947631836, 9.943459510803223, 5.043087005615234, 4.174043655395508, 4.170345783233643, 4.170517444610596, 4.1706366539001465, 4.170467853546143, 9.340988159179688, 3.2982795238494873, 3.2989165782928467, 3.297274589538574, 3.297926187515259, 3.2975029945373535, 3.297424554824829, 3.2968995571136475, 3.297295331954956, 3.2972211837768555, 3.297522783279419, 3.297189235687256, 6.6260294914245605, 9.126777648925781, 9.285781860351562, 21.64210319519043, 5.751489162445068, 12.497992515563965, 14.217537879943848, 5.8131537437438965, 7.420490741729736, 20.512632369995117, 21.067228317260742, 22.404190063476562, 32.80321502685547, 7.842573642730713, 18.067298889160156, 15.278860092163086, 8.161179542541504, 8.22594928741455, 15.193655967712402, 11.493489265441895, 62.909603118896484, 15.908896446228027, 11.972456932067871, 31.66632652282715, 31.399784088134766, 21.60884666442871, 13.5540189743042, 29.80694007873535, 15.621116638183594, 21.56977653503418, 6.332202911376953, 4.715149879455566, 10.432290077209473, 3.9081873893737793, 3.908491611480713, 3.103426933288574, 3.1005094051361084, 3.1008810997009277, 3.100431442260742, 3.100456476211548, 3.101655960083008, 3.1008589267730713, 3.101280927658081, 2.2950315475463867, 2.295048713684082, 2.295039653778076, 2.293041467666626, 2.293137550354004, 2.293524980545044, 2.292957067489624, 2.2943170070648193, 2.2938146591186523, 2.2931056022644043, 2.293445110321045, 2.293065071105957, 2.2933175563812256, 2.2932374477386475, 2.29329776763916, 2.2934677600860596, 2.2933576107025146, 5.58776330947876, 5.691819667816162, 18.223962783813477, 14.763155937194824, 4.6779303550720215, 4.678550720214844, 29.80694007873535, 8.95078182220459, 6.653961181640625, 62.909603118896484, 5.264580249786377, 21.56977653503418, 21.60884666442871, 7.169860363006592, 15.672372817993164, 3.7965874671936035, 3.7962632179260254, 5.653908729553223, 8.04036808013916, 8.455780982971191, 15.908896446228027, 16.73002052307129, 8.542840003967285, 8.591410636901855, 9.27502727508545, 9.405150413513184, 5.909144401550293, 11.25944709777832, 31.399784088134766, 21.067228317260742, 11.828855514526367, 12.604763984680176, 31.66632652282715, 17.38128662109375, 22.404190063476562, 18.067298889160156, 8.380263328552246, 7.610354423522949, 6.839461803436279, 6.837320804595947, 5.298824787139893, 4.527594089508057, 3.759244680404663, 3.7575652599334717, 7.710819721221924, 2.9886906147003174, 2.988463878631592, 2.988168239593506, 2.988361358642578, 2.988593578338623, 2.9886538982391357, 2.2177517414093018, 2.218614339828491, 2.2178688049316406, 2.218782663345337, 2.2186174392700195, 2.218271493911743, 2.2184252738952637, 2.218308687210083, 2.2184674739837646, 2.2184407711029053, 2.218609094619751, 2.2180335521698, 2.2183291912078857, 2.2185022830963135, 8.450849533081055, 24.507984161376953, 18.70431137084961, 4.63252592086792, 11.788817405700684, 19.945106506347656, 5.26138162612915, 3.6838572025299072, 3.683988571166992, 11.560323715209961, 7.510522842407227, 9.631559371948242, 3.796034336090088, 7.890304088592529, 3.861884355545044, 9.305009841918945, 9.911248207092285, 18.067298889160156, 31.66632652282715, 31.399784088134766, 17.38128662109375, 10.277942657470703, 62.909603118896484, 11.128304481506348, 11.828855514526367, 21.60884666442871, 29.80694007873535, 15.009057998657227, 2.799071788787842, 2.092435121536255, 2.0919911861419678, 2.0918056964874268, 2.092423915863037, 2.092224597930908, 2.091731071472168, 2.0916483402252197, 2.092151403427124, 2.0921084880828857, 2.091479778289795, 2.0918772220611572, 4.313727855682373, 3.4949843883514404, 3.606762170791626, 5.893192291259766, 2.5028254985809326, 3.671194553375244, 1.3853472471237183, 1.3853468894958496, 1.3856199979782104, 1.3856234550476074, 1.3856441974639893, 1.385352373123169, 1.385664701461792, 1.3852428197860718, 1.3856706619262695, 1.3853201866149902, 1.3853098154067993, 1.385324478149414, 9.704509735107422, 4.338151454925537, 7.929201126098633, 4.478734016418457, 15.672372817993164, 2.899714469909668, 16.73002052307129, 2.9643216133117676, 5.174685001373291, 5.183507919311523, 11.183205604553223, 11.25944709777832, 18.223962783813477, 32.80321502685547, 8.36986255645752, 31.66632652282715, 13.5540189743042, 14.165924072265625, 31.399784088134766, 10.02663803100586, 12.653543472290039, 6.159462928771973, 6.648852348327637, 7.09976863861084, 17.38128662109375, 8.174198150634766, 9.617043495178223, 22.404190063476562, 20.512632369995117, 19.945106506347656, 13.024633407592773, 9.515420913696289, 4.15631103515625, 2.7653756141662598, 2.764930009841919, 6.4425225257873535, 2.0696399211883545, 2.0699691772460938, 2.0696656703948975, 2.069643974304199, 2.0696401596069336, 2.0696353912353516, 2.0694007873535156, 2.0695159435272217, 2.069458246231079, 2.069499969482422, 2.0694572925567627, 2.069725275039673, 2.069554090499878, 3.471794366836548, 3.5352907180786133, 3.535384178161621, 3.535564422607422, 5.799615859985352, 3.638023614883423, 3.638113260269165, 1.3740497827529907, 1.3740497827529907, 1.3740899562835693, 1.3741477727890015, 1.3741132020950317, 4.242313861846924, 7.650465965270996, 11.183205604553223, 15.278860092163086, 2.8769285678863525, 62.909603118896484, 13.21779727935791, 32.80321502685547, 15.022924423217773, 9.095832824707031, 10.277942657470703, 31.399784088134766, 7.102647304534912, 31.66632652282715, 15.621116638183594, 4.897328853607178, 5.280429840087891, 5.485649585723877, 11.788817405700684, 15.009057998657227, 17.38128662109375, 22.404190063476562, 18.70431137084961, 15.193655967712402, 12.604763984680176, 7.205368518829346, 9.911248207092285, 7.371133804321289, 5.702699184417725, 2.1305782794952393, 1.6472376585006714, 1.6475062370300293, 1.6480833292007446, 1.6495944261550903, 1.649637222290039, 2.5423331260681152, 3.42583966255188, 4.0245161056518555, 1.1624592542648315, 1.1624592542648315, 1.1624614000320435, 1.1624622344970703, 1.1624623537063599, 1.1628068685531616, 1.162826657295227, 1.162906289100647, 1.1629136800765991, 1.163011908531189, 1.1638060808181763, 1.1639145612716675, 1.1640161275863647, 1.1640580892562866, 1.1640602350234985, 1.1641236543655396, 1.1641522645950317, 1.1642554998397827, 1.164492130279541, 1.168312668800354, 1.1683634519577026, 2.353088855743408, 2.356942892074585, 7.929201126098633, 3.8105618953704834, 2.454939126968384, 2.5320980548858643, 4.418336391448975, 6.653961181640625, 5.522758483886719, 3.2237422466278076, 3.3288733959198, 4.102205753326416, 9.27502727508545, 18.70431137084961, 5.691819667816162, 6.142698764801025, 17.38128662109375, 19.945106506347656, 21.56977653503418, 8.174198150634766, 9.704509735107422, 62.909603118896484, 31.399784088134766, 21.64210319519043, 31.66632652282715, 12.653543472290039, 13.024633407592773, 9.631559371948242, 1.9096143245697021, 5.350507736206055, 1.498976707458496, 1.4989418983459473, 3.6012837886810303, 1.499108076095581, 1.4990825653076172, 3.0890326499938965, 1.9839932918548584, 1.0883432626724243, 1.0885655879974365, 1.0885887145996094, 1.0885632038116455, 1.0885820388793945, 1.0884666442871094, 1.088618516921997, 1.0886471271514893, 1.0885837078094482, 1.0886411666870117, 1.088667869567871, 1.0887140035629272, 1.0886470079421997, 1.0887326002120972, 1.088650107383728, 1.0885649919509888, 1.0886530876159668, 1.0887000560760498, 1.0888303518295288, 1.0887393951416016, 1.0887634754180908, 4.065975666046143, 2.3710684776306152, 2.9636456966400146, 3.0675041675567627, 5.909144401550293, 6.14710807800293, 4.784946918487549, 6.304502487182617, 21.067228317260742, 7.479675769805908, 32.80321502685547, 10.02663803100586, 10.277942657470703, 15.621116638183594, 16.73002052307129, 31.399784088134766, 19.945106506347656, 15.908896446228027, 20.512632369995117, 13.024633407592773, 12.182929992675781], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -3.6412999629974365, -4.22790002822876, -5.240200042724609, -5.2403998374938965, -5.486299991607666, -5.487199783325195, -5.635900020599365, -5.63730001449585, -5.637700080871582, -5.6381001472473145, -5.6381001472473145, -5.123700141906738, -5.815400123596191, -6.029799938201904, -6.031099796295166, -6.031400203704834, -6.031799793243408, -6.0320000648498535, -5.240200042724609, -6.30679988861084, -6.306600093841553, -6.307600021362305, -6.307400226593018, -6.307499885559082, -6.307700157165527, -6.3078999519348145, -6.307799816131592, -6.3078999519348145, -6.3078999519348145, -6.308000087738037, -5.6371002197265625, -5.355800151824951, -5.354000091552734, -4.611499786376953, -5.815000057220459, -5.136899948120117, -5.039599895477295, -5.815400123596191, -5.63730001449585, -4.876500129699707, -4.874300003051758, -4.956200122833252, -4.733699798583984, -5.636199951171875, -5.135499954223633, -5.239099979400635, -5.638700008392334, -5.63730001449585, -5.354700088500977, -5.4868998527526855, -4.798500061035156, -5.355599880218506, -5.485899925231934, -5.2342000007629395, -5.237500190734863, -5.355100154876709, -5.486800193786621, -5.348100185394287, -5.484600067138672, -5.485000133514404, -5.0833001136779785, -5.410999774932861, -4.639100074768066, -5.626699924468994, -5.6280999183654785, -5.901800155639648, -5.902900218963623, -5.90310001373291, -5.903299808502197, -5.9045000076293945, -5.904399871826172, -5.904699802398682, -5.904699802398682, -6.284800052642822, -6.284900188446045, -6.284900188446045, -6.286200046539307, -6.286200046539307, -6.286099910736084, -6.286600112915039, -6.286099910736084, -6.286499977111816, -6.286799907684326, -6.287099838256836, -6.287399768829346, -6.287300109863281, -6.287300109863281, -6.287399768829346, -6.287399768829346, -6.287399768829346, -5.410600185394287, -5.4120001792907715, -4.331699848175049, -4.551499843597412, -5.6280999183654785, -5.628200054168701, -3.9979000091552734, -5.083399772644043, -5.4116997718811035, -3.640899896621704, -5.628300189971924, -4.551599979400635, -4.552199840545654, -5.409999847412109, -4.833700180053711, -5.903500080108643, -5.903800010681152, -5.627600193023682, -5.410900115966797, -5.411499977111816, -5.0802998542785645, -5.077899932861328, -5.4105000495910645, -5.411300182342529, -5.410799980163574, -5.409999847412109, -5.628399848937988, -5.408899784088135, -5.076700210571289, -5.231100082397461, -5.407700061798096, -5.40910005569458, -5.22760009765625, -5.405099868774414, -5.405200004577637, -5.40939998626709, -4.551400184631348, -4.655300140380859, -4.770999908447266, -4.772500038146973, -5.053400039672852, -5.228099822998047, -5.447199821472168, -5.448200225830078, -4.772200107574463, -5.722599983215332, -5.723400115966797, -5.723599910736084, -5.723700046539307, -5.724400043487549, -5.725299835205078, -6.103899955749512, -6.105299949645996, -6.105800151824951, -6.105400085449219, -6.105999946594238, -6.106500148773193, -6.1066999435424805, -6.106900215148926, -6.10699987411499, -6.107100009918213, -6.10699987411499, -6.1072998046875, -6.107699871063232, -6.107699871063232, -4.7729997634887695, -3.8190999031066895, -4.08650016784668, -5.448200225830078, -4.771599769592285, -4.3709001541137695, -5.447700023651123, -5.723400115966797, -5.723800182342529, -4.9028000831604, -5.230100154876709, -5.054200172424316, -5.724100112915039, -5.2307000160217285, -5.723400115966797, -5.231800079345703, -5.229300022125244, -5.050300121307373, -4.898900032043457, -5.048799991607666, -5.230100154876709, -5.443900108337402, -5.04610013961792, -5.443600177764893, -5.448400020599365, -5.443399906158447, -5.444900035858154, -5.446100234985352, -5.485099792480469, -5.868299961090088, -5.868500232696533, -5.868899822235107, -5.86870002746582, -5.868899822235107, -5.869200229644775, -5.86959981918335, -5.8694000244140625, -5.869500160217285, -5.869900226593018, -5.869800090789795, -5.209400177001953, -5.484899997711182, -5.484099864959717, -4.993599891662598, -5.867400169372559, -5.486199855804443, -6.4928998947143555, -6.4928998947143555, -6.495200157165527, -6.495299816131592, -6.4953999519348145, -6.495800018310547, -6.49560022354126, -6.495999813079834, -6.495699882507324, -6.495999813079834, -6.496099948883057, -6.496099948883057, -4.66510009765625, -5.486800193786621, -4.994200229644775, -5.486499786376953, -4.417399883270264, -5.867499828338623, -4.417200088500977, -5.870100021362305, -5.484899997711182, -5.484899997711182, -4.991300106048584, -4.9918999671936035, -4.663899898529053, -4.3109002113342285, -5.2083001136779785, -4.530300140380859, -4.991300106048584, -4.991499900817871, -4.6616997718811035, -5.206699848175049, -5.208000183105469, -5.485199928283691, -5.485000133514404, -5.4847002029418945, -5.209400177001953, -5.484600067138672, -5.48199987411499, -5.480400085449219, -5.480800151824951, -5.480899810791016, -5.484600067138672, -4.098499774932861, -4.957399845123291, -5.4496002197265625, -5.450200080871582, -4.628900051116943, -5.8302001953125, -5.8317999839782715, -5.832499980926514, -5.832600116729736, -5.832600116729736, -5.832600116729736, -5.832699775695801, -5.832699775695801, -5.833099842071533, -5.833099842071533, -5.833099842071533, -5.833099842071533, -5.833399772644043, -5.44890022277832, -5.449999809265137, -5.450200080871582, -5.450799942016602, -4.957399845123291, -5.44789981842041, -5.449399948120117, -6.456099987030029, -6.456099987030029, -6.460100173950195, -6.460100173950195, -6.460299968719482, -5.448800086975098, -4.957099914550781, -4.628600120544434, -4.3821001052856445, -5.830599784851074, -3.25600004196167, -4.627699851989746, -4.0960001945495605, -4.625999927520752, -4.956600189208984, -4.957099914550781, -4.378499984741211, -5.1722002029418945, -4.380099773406982, -4.7779998779296875, -5.44920015335083, -5.44890022277832, -5.447999954223633, -5.171299934387207, -5.1697001457214355, -5.168700218200684, -5.1682000160217285, -5.440400123596191, -5.441500186920166, -5.446100234985352, -5.446499824523926, -5.446899890899658, -5.448500156402588, -5.448800086975098, -4.927000045776367, -5.311399936676025, -5.3119001388549805, -5.312300205230713, -5.314700126647949, -5.314700126647949, -4.927800178527832, -4.653500080108643, -4.655300140380859, -5.938600063323975, -5.938600063323975, -5.938700199127197, -5.938700199127197, -5.938700199127197, -5.939899921417236, -5.940000057220459, -5.940299987792969, -5.940299987792969, -5.940700054168701, -5.942399978637695, -5.942399978637695, -5.942800045013428, -5.94290018081665, -5.94290018081665, -5.9430999755859375, -5.9430999755859375, -5.943399906158447, -5.943999767303467, -5.952199935913086, -5.952400207519531, -5.309700012207031, -5.314199924468994, -4.258600234985352, -4.926300048828125, -5.311299800872803, -5.32480001449585, -4.928199768066406, -4.6508002281188965, -4.928299903869629, -5.308499813079834, -5.3125, -5.313899993896484, -4.922999858856201, -4.647799968719482, -5.316199779510498, -5.308599948883057, -4.92519998550415, -4.91949987411499, -4.917099952697754, -5.312300205230713, -5.306300163269043, -5.265699863433838, -5.295300006866455, -5.299600124359131, -5.304900169372559, -5.306700229644775, -5.30679988861084, -5.310100078582764, -4.79610013961792, -3.8434998989105225, -5.178400039672852, -5.178599834442139, -4.30210018157959, -5.178800106048584, -5.178999900817871, -4.519000053405762, -5.178199768066406, -5.80620002746582, -5.806099891662598, -5.806300163269043, -5.806300163269043, -5.80649995803833, -5.806600093841553, -5.80649995803833, -5.80649995803833, -5.806600093841553, -5.806600093841553, -5.806600093841553, -5.806600093841553, -5.806700229644775, -5.806700229644775, -5.80679988861084, -5.80679988861084, -5.80679988861084, -5.8069000244140625, -5.80679988861084, -5.8069000244140625, -5.8069000244140625, -4.5192999839782715, -5.17710018157959, -5.176300048828125, -5.1778998374938965, -4.793900012969971, -4.795499801635742, -5.176599979400635, -5.1774001121521, -4.791200160980225, -5.174699783325195, -4.781400203704834, -5.173500061035156, -5.171999931335449, -5.170400142669678, -5.1722002029418945, -5.777299880981445, -5.782100200653076, -5.785399913787842, -5.788400173187256, -5.790999889373779, -5.794400215148926], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.0961999893188477, 1.0494999885559082, 1.0422999858856201, 1.0419000387191772, 1.024899959564209, 1.0240999460220337, 1.0119999647140503, 1.011299967765808, 1.0111000537872314, 1.0109000205993652, 1.0108000040054321, 1.00600004196167, 0.9932000041007996, 0.9678999781608582, 0.9674999713897705, 0.9671000242233276, 0.96670001745224, 0.9664999842643738, 0.9520000219345093, 0.9264000058174133, 0.9264000058174133, 0.9259999990463257, 0.9259999990463257, 0.9258999824523926, 0.9257000088691711, 0.9257000088691711, 0.9257000088691711, 0.925599992275238, 0.9254999756813049, 0.9254999756813049, 0.8985000252723694, 0.8596000075340271, 0.8440999984741211, 0.7404999732971191, 0.8621000051498413, 0.76419997215271, 0.7325999736785889, 0.8511000275611877, 0.785099983215332, 0.5291000008583069, 0.5045999884605408, 0.3610999882221222, 0.20239999890327454, 0.73089998960495, 0.3970000147819519, 0.460999995470047, 0.6884999871253967, 0.6820999979972839, 0.35100001096725464, 0.49799999594688416, -0.5135999917984009, 0.3041999936103821, 0.45809999108314514, -0.2628999948501587, -0.25769999623298645, -0.0015999999595806003, 0.33309999108314514, -0.31619998812675476, 0.19339999556541443, -0.12970000505447388, 1.4975999593734741, 1.4648000001907349, 1.4426000118255615, 1.4368000030517578, 1.4354000091552734, 1.392300009727478, 1.392199993133545, 1.391800045967102, 1.391700029373169, 1.3904999494552612, 1.3903000354766846, 1.3902000188827515, 1.3901000022888184, 1.3109999895095825, 1.3109999895095825, 1.3108999729156494, 1.3105000257492065, 1.3105000257492065, 1.3104000091552734, 1.3101999759674072, 1.3100999593734741, 1.3099000453948975, 1.3099000453948975, 1.309399962425232, 1.309399962425232, 1.3092999458312988, 1.3092999458312988, 1.3092000484466553, 1.3092000484466553, 1.3092000484466553, 1.2954000234603882, 1.2755999565124512, 1.192199945449829, 1.1829999685287476, 1.2556999921798706, 1.2553999423980713, 1.0339000225067139, 1.1514999866485596, 1.1196999549865723, 0.6439999938011169, 1.1373000144958496, 0.8036999702453613, 0.8012999892234802, 1.0467000007629395, 0.8410000205039978, 1.1890000104904175, 1.1887999773025513, 1.0666999816894531, 0.9312000274658203, 0.880299985408783, 0.5794000029563904, 0.531499981880188, 0.8711000084877014, 0.8644999861717224, 0.7885000109672546, 0.7753999829292297, 1.0217000246047974, 0.5964999794960022, -0.09690000116825104, 0.1477999985218048, 0.54830002784729, 0.48339998722076416, -0.25619998574256897, 0.16609999537467957, -0.08780000358819962, 0.12309999763965607, 1.7493000030517578, 1.7417999505996704, 1.7329000234603882, 1.7316999435424805, 1.7057000398635864, 1.6883000135421753, 1.6550999879837036, 1.6547000408172607, 1.611799955368042, 1.6092000007629395, 1.6083999872207642, 1.6083999872207642, 1.608199954032898, 1.6073999404907227, 1.6065000295639038, 1.5262000560760498, 1.524399995803833, 1.5242999792099, 1.5241999626159668, 1.5236999988555908, 1.5233999490737915, 1.5231000185012817, 1.5228999853134155, 1.5227999687194824, 1.5227999687194824, 1.5226999521255493, 1.5226000547409058, 1.5221999883651733, 1.5220999717712402, 1.5192999839782715, 1.4084999561309814, 1.4113999605178833, 1.4452999830245972, 1.1878000497817993, 1.0628000497817993, 1.31850004196167, 1.3991999626159668, 1.398800015449524, 1.076200008392334, 1.1801999807357788, 1.1073999404907227, 1.3686000108718872, 1.1303000450134277, 1.3519999980926514, 0.9642000198364258, 0.9035999774932861, 0.4821999967098236, 0.07249999791383743, -0.0689999982714653, 0.3411000072956085, 0.6527000069618225, -0.7612000107765198, 0.5735999941825867, 0.5077000260353088, -0.08990000188350677, -0.4129999876022339, 0.2718000113964081, 1.9121999740600586, 1.8200000524520874, 1.8200000524520874, 1.819700002670288, 1.819599986076355, 1.8194999694824219, 1.8193999528884888, 1.819000005722046, 1.819000005722046, 1.8188999891281128, 1.8187999725341797, 1.8187999725341797, 1.7553999423980713, 1.6904000043869019, 1.6597000360488892, 1.6591999530792236, 1.641800045967102, 1.6398999691009521, 1.607699990272522, 1.607699990272522, 1.6052000522613525, 1.6052000522613525, 1.6050000190734863, 1.6049000024795532, 1.6047999858856201, 1.6047999858856201, 1.6047999858856201, 1.604699969291687, 1.604599952697754, 1.604599952697754, 1.4889999628067017, 1.4723999500274658, 1.361799955368042, 1.4407999515533447, 1.2573000192642212, 1.4945000410079956, 1.192199945449829, 1.4697999954223633, 1.2978999614715576, 1.2962000370025635, 1.020900011062622, 1.0134999752044678, 0.8600000143051147, 0.6251999735832214, 1.0937000513076782, 0.44110000133514404, 0.8285999894142151, 0.7843000292778015, 0.3181000053882599, 0.9146000146865845, 0.6807000041007996, 1.1233999729156494, 1.0470999479293823, 0.9818000197410583, 0.3617999851703644, 0.8410000205039978, 0.6811000108718872, -0.16300000250339508, -0.07519999891519547, -0.04729999974370003, 0.3752000033855438, 2.075200080871582, 2.044600009918213, 1.9598000049591064, 1.9594000577926636, 1.9348000288009644, 1.86899995803833, 1.867300033569336, 1.8667000532150269, 1.8667000532150269, 1.8667000532150269, 1.8666000366210938, 1.8666000366210938, 1.8666000366210938, 1.8662999868392944, 1.8662999868392944, 1.8661999702453613, 1.8660999536514282, 1.8659000396728516, 1.7330000400543213, 1.7137999534606934, 1.7136000394821167, 1.7129000425338745, 1.711400032043457, 1.6872999668121338, 1.6857000589370728, 1.6527999639511108, 1.6527999639511108, 1.648800015449524, 1.6486999988555908, 1.6484999656677246, 1.5326999425888062, 1.4347000122070312, 1.3835999965667725, 1.3179999589920044, 1.5392999649047852, 1.0289000272750854, 1.2173999547958374, 0.8400999903678894, 1.090999960899353, 1.2621999979019165, 1.1395000219345093, 0.6013000011444092, 1.2939000129699707, 0.5913000106811523, 0.8999999761581421, 1.388700008392334, 1.3136999607086182, 1.2764999866485596, 0.788100004196167, 0.5482000112533569, 0.4025000035762787, 0.14920000731945038, 0.05739999935030937, 0.26429998874664307, 0.446399986743927, 1.0053000450134277, 0.6859999895095825, 0.9804999828338623, 1.236799955368042, 2.7432000637054443, 2.6161000728607178, 2.6154000759124756, 2.6147000789642334, 2.6113998889923096, 2.6113998889923096, 2.565700054168701, 2.5416998863220215, 2.3789000511169434, 2.337399959564209, 2.337399959564209, 2.337399959564209, 2.337399959564209, 2.337399959564209, 2.3357999324798584, 2.3357999324798584, 2.335400104522705, 2.335400104522705, 2.33489990234375, 2.3324999809265137, 2.33240008354187, 2.331899881362915, 2.3317999839782715, 2.3317999839782715, 2.3315999507904053, 2.3315000534057617, 2.3310999870300293, 2.3303000926971436, 2.3187999725341797, 2.3185999393463135, 2.261199951171875, 2.255000114440918, 2.0975000858306885, 2.1624999046325684, 2.2172000408172607, 2.172800064086914, 2.0125999450683594, 1.8805999755859375, 1.7893999814987183, 1.94760000705719, 1.9114999771118164, 1.701200008392334, 1.2762999534606934, 0.8500000238418579, 1.371399998664856, 1.3027000427246094, 0.6460000276565552, 0.51419997215271, 0.4381999969482422, 1.0132999420166016, 0.8478000164031982, -0.9807999730110168, -0.3154999911785126, 0.052400000393390656, -0.3334999978542328, 0.5820000171661377, 0.5529999732971191, 0.8514000177383423, 2.983599901199341, 2.905900001525879, 2.8434998989105225, 2.8431999683380127, 2.8431999683380127, 2.842900037765503, 2.8427000045776367, 2.7797000408172607, 2.5632998943328857, 2.535799980163574, 2.535599946975708, 2.5355000495910645, 2.535399913787842, 2.5353000164031982, 2.5353000164031982, 2.5352001190185547, 2.5352001190185547, 2.5352001190185547, 2.535099983215332, 2.5350000858306885, 2.5350000858306885, 2.5350000858306885, 2.534899950027466, 2.534899950027466, 2.534899950027466, 2.534899950027466, 2.5348000526428223, 2.5346999168395996, 2.5346999168395996, 2.5346999168395996, 2.5046000480651855, 2.3861000537872314, 2.1638998985290527, 2.1278998851776123, 1.8561999797821045, 1.8150999546051025, 1.6845999956130981, 1.4079999923706055, 0.5877000093460083, 1.239799976348877, 0.15469999611377716, 0.9477999806404114, 0.9246000051498413, 0.5076000094413757, 0.4372999966144562, -0.7975000143051147, -0.34850001335144043, -0.1256999969482422, -0.38280001282691956, 0.06880000233650208, 0.13220000267028809]}, \"token.table\": {\"Topic\": [1, 7, 1, 2, 3, 5, 1, 2, 1, 2, 1, 2, 4, 5, 1, 2, 3, 1, 2, 3, 4, 6, 1, 2, 3, 5, 6, 2, 2, 6, 6, 2, 4, 1, 2, 4, 1, 3, 5, 6, 1, 2, 6, 1, 4, 4, 5, 6, 5, 4, 2, 5, 3, 1, 3, 6, 1, 2, 5, 3, 1, 2, 3, 5, 1, 3, 1, 2, 3, 5, 2, 5, 1, 3, 4, 4, 1, 2, 3, 3, 2, 3, 6, 1, 5, 6, 1, 2, 4, 1, 2, 4, 2, 2, 4, 1, 2, 6, 1, 2, 3, 5, 6, 6, 6, 6, 6, 6, 2, 1, 7, 2, 6, 2, 3, 4, 5, 2, 3, 6, 6, 1, 5, 1, 2, 3, 5, 6, 6, 1, 3, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 1, 2, 7, 1, 2, 3, 4, 5, 1, 3, 5, 1, 1, 3, 3, 5, 1, 2, 3, 4, 5, 6, 1, 2, 3, 6, 1, 3, 3, 1, 3, 4, 5, 1, 2, 3, 4, 1, 5, 2, 2, 1, 3, 5, 1, 2, 3, 5, 6, 5, 1, 2, 3, 4, 5, 6, 2, 3, 4, 1, 4, 4, 4, 1, 3, 7, 1, 2, 4, 6, 3, 5, 5, 1, 1, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 6, 3, 4, 4, 2, 3, 4, 5, 7, 2, 3, 4, 5, 1, 2, 3, 4, 5, 3, 5, 6, 4, 1, 2, 4, 5, 2, 1, 4, 5, 2, 1, 1, 3, 5, 2, 3, 3, 2, 1, 2, 3, 4, 5, 6, 3, 5, 1, 2, 3, 4, 3, 4, 5, 5, 7, 2, 1, 6, 1, 1, 3, 1, 2, 3, 4, 5, 6, 1, 3, 5, 6, 3, 7, 1, 3, 5, 6, 3, 1, 5, 3, 5, 7, 4, 6, 2, 4, 6, 6, 3, 1, 2, 6, 1, 6, 6, 6, 1, 2, 3, 4, 5, 6, 4, 4, 1, 2, 4, 5, 1, 2, 4, 1, 1, 2, 3, 6, 5, 5, 1, 2, 3, 4, 5, 6, 5, 4, 1, 2, 3, 4, 1, 6, 1, 4, 6, 3, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 5, 6, 1, 1, 2, 5, 5, 2, 5, 3, 1, 2, 4, 1, 2, 3, 2, 7, 4, 1, 3, 4, 7, 2, 5, 1, 2, 3, 2, 3, 2, 2, 1, 2, 3, 4, 5, 6, 1, 3, 4, 5, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 3, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 5, 6, 2, 1, 1, 1, 2, 3, 5, 1, 4, 6, 3, 5, 5, 6, 2, 6, 2, 4, 3, 3, 1, 3, 5, 1, 1, 6, 1, 1, 2, 6, 4, 2, 4, 5, 1, 2, 3, 4, 1, 2, 4, 5, 7, 6, 1, 4, 6, 7, 6, 6, 6, 3, 5, 4, 3, 4, 3, 4, 4, 6, 1, 2, 3, 4, 6, 1, 5, 6, 7, 2, 2, 2, 4, 4, 1, 3, 2, 6, 6, 3, 3, 5, 3, 5, 6, 2, 3, 4, 5, 4, 2, 1, 3, 1, 2, 3, 4, 1, 2, 3, 4, 5, 2, 2, 3, 7, 4, 2, 1, 2, 3, 4, 1, 3, 2, 3, 6, 5, 2, 1, 3, 4, 5, 6, 3, 2, 4, 5, 6, 1, 2, 1, 2, 3, 4, 5, 1, 2, 4, 6, 5, 7, 3, 7, 1, 2, 1, 2, 3, 4, 5, 6, 1, 2, 3, 5, 6, 4, 4, 1, 2, 3, 4, 5, 7, 7, 1, 1, 2, 4, 2, 3, 5, 2, 1, 2, 3, 4, 7, 3, 5, 2, 1, 2, 1, 2, 3, 4, 2, 5, 1, 2, 5, 1, 2, 1, 2, 4, 5, 6, 4, 1, 4, 4, 1, 2, 3, 4, 5, 1, 2, 3, 5, 6, 1, 2, 3, 4, 5, 6, 7, 5, 4, 2, 4, 6, 1, 2, 4, 5, 6, 1, 5, 7, 1, 7, 1, 1], \"Freq\": [0.2776787579059601, 0.5553575158119202, 0.1008954644203186, 0.3026863932609558, 0.4035818576812744, 0.2017909288406372, 0.8564404249191284, 0.10705505311489105, 0.9097793102264404, 0.8720201253890991, 0.3247036337852478, 0.3247036337852478, 0.3247036337852478, 0.7233456373214722, 0.2534756660461426, 0.2534756660461426, 0.5069513320922852, 0.2506880462169647, 0.05013761296868324, 0.4512385129928589, 0.10027522593736649, 0.10027522593736649, 0.10781639814376831, 0.43126559257507324, 0.21563279628753662, 0.10781639814376831, 0.21563279628753662, 0.8720837831497192, 0.6011456847190857, 0.30057284235954285, 0.8592496514320374, 0.2772569954395294, 0.5545139908790588, 0.22327737510204315, 0.22327737510204315, 0.4465547502040863, 0.20419295132160187, 0.20419295132160187, 0.40838590264320374, 0.20419295132160187, 0.3004019260406494, 0.3004019260406494, 0.3004019260406494, 0.33734530210494995, 0.6746906042098999, 0.24847707152366638, 0.24847707152366638, 0.49695414304733276, 0.48317578434944153, 0.9559203386306763, 0.9666733145713806, 0.48323166370391846, 0.9015091061592102, 0.5432068109512329, 0.18106894195079803, 0.36213788390159607, 0.9591530561447144, 0.8714446425437927, 0.483169823884964, 0.8775367140769958, 0.21988090872764587, 0.32982137799263, 0.10994045436382294, 0.43976181745529175, 0.21586495637893677, 0.6475948691368103, 0.3594437837600708, 0.1797218918800354, 0.2695828378200531, 0.0898609459400177, 0.9475375413894653, 0.9623918533325195, 0.6126565337181091, 0.24506261944770813, 0.12253130972385406, 0.7990968823432922, 0.17896248400211334, 0.7158499360084534, 0.8772619962692261, 0.6692631244659424, 0.641224205493927, 0.21374140679836273, 0.6061939001083374, 0.5235992670059204, 0.3926994502544403, 0.06544990837574005, 0.9097738265991211, 0.34486153721809387, 0.6897230744361877, 0.37311723828315735, 0.49748966097831726, 0.12437241524457932, 0.8721796274185181, 0.2318180501461029, 0.6954541802406311, 0.1899486631155014, 0.569845974445343, 0.1899486631155014, 0.05346360802650452, 0.05346360802650452, 0.6415632963180542, 0.10692721605300903, 0.10692721605300903, 0.6070769429206848, 0.8599090576171875, 0.8589180111885071, 0.8558980226516724, 0.8602452278137207, 0.8717191219329834, 0.49188685417175293, 0.49188685417175293, 0.2918992340564728, 0.5837984681129456, 0.3552572429180145, 0.17762862145900726, 0.3552572429180145, 0.08881431072950363, 0.2366625964641571, 0.7099878191947937, 0.8590946197509766, 0.8590152859687805, 0.8451996445655823, 0.4832085072994232, 0.3239414095878601, 0.4627734124660492, 0.13883203268051147, 0.04627734422683716, 0.04627734422683716, 0.8590620756149292, 0.2589409351348877, 0.5178818702697754, 0.2536171078681946, 0.338156133890152, 0.2536171078681946, 0.169078066945076, 0.3283282518386841, 0.16416412591934204, 0.16416412591934204, 0.16416412591934204, 0.08208206295967102, 0.140792578458786, 0.140792578458786, 0.140792578458786, 0.140792578458786, 0.42237773537635803, 0.140792578458786, 0.48803436756134033, 0.32535624504089355, 0.16267812252044678, 0.4607186019420624, 0.19745083153247833, 0.13163389265537262, 0.06581694632768631, 0.13163389265537262, 0.3787570297718048, 0.1893785148859024, 0.3787570297718048, 0.9098658561706543, 0.12968789041042328, 0.7781273722648621, 0.9017665982246399, 0.48315590620040894, 0.11826229095458984, 0.4730491638183594, 0.11826229095458984, 0.11826229095458984, 0.11826229095458984, 0.8590636849403381, 0.20401515066623688, 0.12240909039974213, 0.652848482131958, 0.6067654490470886, 0.845187783241272, 0.9014635682106018, 0.9014623165130615, 0.9098365902900696, 0.2357204258441925, 0.2357204258441925, 0.471440851688385, 0.5199102759361267, 0.10398206114768982, 0.10398206114768982, 0.20796412229537964, 0.2748677432537079, 0.5497354865074158, 0.9674669504165649, 0.7676193118095398, 0.17242521047592163, 0.17242521047592163, 0.6897008419036865, 0.5011502504348755, 0.25057512521743774, 0.08352504670619965, 0.08352504670619965, 0.08352504670619965, 0.48319587111473083, 0.25263428688049316, 0.15789642930030823, 0.15789642930030823, 0.18947571516036987, 0.18947571516036987, 0.031579285860061646, 0.3858391046524048, 0.1929195523262024, 0.3858391046524048, 0.6954720616340637, 0.17386801540851593, 0.7216753363609314, 0.7218525409698486, 0.4179774820804596, 0.4179774820804596, 0.2089887410402298, 0.16968731582164764, 0.16968731582164764, 0.6787492632865906, 0.6062096357345581, 0.9015794396400452, 0.483218252658844, 0.4831758439540863, 0.88374263048172, 0.3507111072540283, 0.17535555362701416, 0.3507111072540283, 0.17535555362701416, 0.30711036920547485, 0.23033277690410614, 0.15355518460273743, 0.15355518460273743, 0.07677759230136871, 0.07677759230136871, 0.20765069127082825, 0.20765069127082825, 0.5191267132759094, 0.10382534563541412, 0.4610258638858795, 0.4610258638858795, 0.9559537768363953, 0.09729573875665665, 0.29188722372055054, 0.1945914775133133, 0.3891829550266266, 0.09729573875665665, 0.3008037805557251, 0.3008037805557251, 0.3008037805557251, 0.15040189027786255, 0.27757081389427185, 0.13878540694713593, 0.13878540694713593, 0.13878540694713593, 0.27757081389427185, 0.28285470604896545, 0.5657094120979309, 0.855935275554657, 0.7218406796455383, 0.19324848055839539, 0.19324848055839539, 0.38649696111679077, 0.19324848055839539, 0.8714411854743958, 0.7545996904373169, 0.15091994404792786, 0.7277756929397583, 0.8483293652534485, 0.9373844861984253, 0.7669738531112671, 0.10956769436597824, 0.10956769436597824, 0.2634328007698059, 0.5268656015396118, 0.6692401170730591, 0.9674738645553589, 0.5362549424171448, 0.14625133574008942, 0.09750089794397354, 0.09750089794397354, 0.04875044897198677, 0.04875044897198677, 0.5429092049598694, 0.2714546024799347, 0.5220347046852112, 0.2610173523426056, 0.08700577914714813, 0.08700577914714813, 0.6693063378334045, 0.28803548216819763, 0.5760709643363953, 0.4831748902797699, 0.6671372652053833, 0.9672252535820007, 0.9590861797332764, 0.8599145412445068, 0.9099458456039429, 0.6880946755409241, 0.17202366888523102, 0.1150662824511528, 0.2301325649023056, 0.2301325649023056, 0.1725994199514389, 0.1725994199514389, 0.1150662824511528, 0.4883846938610077, 0.16279488801956177, 0.16279488801956177, 0.16279488801956177, 0.9014657139778137, 0.667063295841217, 0.9098009467124939, 0.6691893935203552, 0.7760935425758362, 0.155218705534935, 0.9016029238700867, 0.2748745083808899, 0.5497490167617798, 0.337422251701355, 0.337422251701355, 0.337422251701355, 0.42497333884239197, 0.42497333884239197, 0.12611611187458038, 0.5044644474983215, 0.37834832072257996, 0.8599880337715149, 0.7980326414108276, 0.26242849230766296, 0.26242849230766296, 0.5248569846153259, 0.9748163819313049, 0.860243022441864, 0.9387122988700867, 0.8602452278137207, 0.693093478679657, 0.04620622843503952, 0.09241245687007904, 0.09241245687007904, 0.04620622843503952, 0.04620622843503952, 0.9558292627334595, 0.7218379974365234, 0.5633986592292786, 0.14084966480731964, 0.2816993296146393, 0.7232290506362915, 0.2789454460144043, 0.5578908920288086, 0.13947272300720215, 0.9098570942878723, 0.349185973405838, 0.4655812978744507, 0.11639532446861267, 0.11639532446861267, 0.4831768870353699, 0.727742075920105, 0.15131114423274994, 0.2269667088985443, 0.15131114423274994, 0.07565557211637497, 0.37827786803245544, 0.07565557211637497, 0.4832184910774231, 0.7145226001739502, 0.2149379849433899, 0.2149379849433899, 0.4298759698867798, 0.2149379849433899, 0.9051175713539124, 0.10056862235069275, 0.8442939519882202, 0.4242784082889557, 0.4242784082889557, 0.9013952016830444, 0.29920297861099243, 0.19946865737438202, 0.09973432868719101, 0.29920297861099243, 0.09973432868719101, 0.09973432868719101, 0.31611698865890503, 0.07902924716472626, 0.15805849432945251, 0.23708774149417877, 0.07902924716472626, 0.07902924716472626, 0.909842312335968, 0.7033566832542419, 0.2110069990158081, 0.07033566385507584, 0.48309898376464844, 0.7901833057403564, 0.26339444518089294, 0.7983893156051636, 0.6375458240509033, 0.12750916182994843, 0.12750916182994843, 0.49813756346702576, 0.22139446437358856, 0.276743084192276, 0.8720505237579346, 0.6670746803283691, 0.9560269713401794, 0.4758504033088684, 0.1586167961359024, 0.3172335922718048, 0.1586167961359024, 0.34759292006492615, 0.34759292006492615, 0.9583033323287964, 0.6413092613220215, 0.21376974880695343, 0.9676072597503662, 0.9015877842903137, 0.872129499912262, 0.8720418810844421, 0.35295969247817993, 0.07059193402528763, 0.14118386805057526, 0.2823677361011505, 0.07059193402528763, 0.07059193402528763, 0.1331465244293213, 0.5325860977172852, 0.2662930488586426, 0.1331465244293213, 0.2781670093536377, 0.4636116623878479, 0.0927223339676857, 0.04636116698384285, 0.0927223339676857, 0.0927223339676857, 0.21264944970607758, 0.42529889941215515, 0.21264944970607758, 0.10632472485303879, 0.10632472485303879, 0.7866789698600769, 0.9016996026039124, 0.39630261063575745, 0.06096963211894035, 0.030484816059470177, 0.21339371800422668, 0.2438785284757614, 0.030484816059470177, 0.25477883219718933, 0.1910841166973114, 0.15923675894737244, 0.15923675894737244, 0.1910841166973114, 0.031847354024648666, 0.8722361326217651, 0.8449113965034485, 0.9591249823570251, 0.33313217759132385, 0.19987930357456207, 0.19987930357456207, 0.19987930357456207, 0.48754259943962097, 0.24377129971981049, 0.24377129971981049, 0.5428898334503174, 0.2714449167251587, 0.7277756929397583, 0.6069779992103577, 0.4073420763015747, 0.4073420763015747, 0.8720990419387817, 0.9559733867645264, 0.8834714293479919, 0.9018142223358154, 0.959113597869873, 0.3645876348018646, 0.3645876348018646, 0.9095650911331177, 0.3949294090270996, 0.3949294090270996, 0.9093894958496094, 0.9435492753982544, 0.04288860410451889, 0.8599734306335449, 0.7218408584594727, 0.9675994515419006, 0.572248637676239, 0.2861243188381195, 0.7201156616210938, 0.1600257009267807, 0.08001285046339035, 0.08001285046339035, 0.3840954601764679, 0.19204773008823395, 0.12803182005882263, 0.25606364011764526, 0.06401591002941132, 0.8598363995552063, 0.22632953524589539, 0.45265907049179077, 0.45265907049179077, 0.6671217679977417, 0.8602436184883118, 0.85899418592453, 0.8602429032325745, 0.2828402817249298, 0.5656805634498596, 0.7218950986862183, 0.901523232460022, 0.9562607407569885, 0.9197994470596313, 0.7216985821723938, 0.7216721773147583, 0.8587434887886047, 0.34601107239723206, 0.08650276809930801, 0.4325138330459595, 0.08650276809930801, 0.08650276809930801, 0.5228439569473267, 0.5228439569473267, 0.5040339827537537, 0.5040339827537537, 0.871910035610199, 0.8721950650215149, 0.871447741985321, 0.7216860055923462, 0.9558241367340088, 0.6078326106071472, 0.2431330382823944, 0.7027629613876343, 0.17569074034690857, 0.859169602394104, 0.6692110896110535, 0.2828621566295624, 0.5657243132591248, 0.6691976189613342, 0.8407405018806458, 0.10509256273508072, 0.40699303150177, 0.13566434383392334, 0.2713286876678467, 0.2713286876678467, 0.956183671951294, 0.8722040057182312, 0.7931649684906006, 0.9546239376068115, 0.11705709248781204, 0.46822836995124817, 0.23411418497562408, 0.23411418497562408, 0.2348446398973465, 0.5703369975090027, 0.1006477028131485, 0.0335492342710495, 0.067098468542099, 0.96734219789505, 0.5076876878738403, 0.16922923922538757, 0.16922923922538757, 0.7216967940330505, 0.8721065521240234, 0.4400053918361664, 0.37714746594429016, 0.06285791099071503, 0.12571582198143005, 0.9096625447273254, 0.9015403985977173, 0.31019848585128784, 0.31019848585128784, 0.31019848585128784, 0.48320478200912476, 0.7675595283508301, 0.10304487496614456, 0.20608974993228912, 0.5152243971824646, 0.10304487496614456, 0.10304487496614456, 0.9436054825782776, 0.17883959412574768, 0.35767918825149536, 0.4470990002155304, 0.08941979706287384, 0.35373759269714355, 0.5306063890457153, 0.4909795820713043, 0.17853803932666779, 0.08926901966333389, 0.08926901966333389, 0.13390351831912994, 0.06380654871463776, 0.4466458261013031, 0.38283929228782654, 0.06380654871463776, 0.1868981570005417, 0.5606944561004639, 0.32372593879699707, 0.6474518775939941, 0.33516624569892883, 0.6703324913978577, 0.1907498985528946, 0.3814997971057892, 0.07947912067174911, 0.015895824879407883, 0.31791648268699646, 0.015895824879407883, 0.1354723870754242, 0.6773619651794434, 0.0677361935377121, 0.0677361935377121, 0.0677361935377121, 0.7218548059463501, 0.9561458826065063, 0.119545578956604, 0.358636736869812, 0.059772789478302, 0.358636736869812, 0.059772789478302, 0.059772789478302, 0.5236659646034241, 0.8450397849082947, 0.44267311692237854, 0.22133655846118927, 0.2951154112815857, 0.19006414711475372, 0.5701924562454224, 0.19006414711475372, 0.8721674680709839, 0.5221379995346069, 0.2373354434967041, 0.09493418037891388, 0.09493418037891388, 0.04746709018945694, 0.9015340805053711, 0.7277544140815735, 0.9675829410552979, 0.09585622698068619, 0.8627060651779175, 0.23895254731178284, 0.11947627365589142, 0.23895254731178284, 0.35842883586883545, 0.7902507781982422, 0.26341691613197327, 0.6738098859786987, 0.1347619742155075, 0.1347619742155075, 0.7538406848907471, 0.21538305282592773, 0.12233615666627884, 0.3670084774494171, 0.24467231333255768, 0.12233615666627884, 0.12233615666627884, 0.9560790657997131, 0.2723909020423889, 0.5447818040847778, 0.95611172914505, 0.3328246772289276, 0.06656493246555328, 0.13312986493110657, 0.06656493246555328, 0.3328246772289276, 0.08482614904642105, 0.08482614904642105, 0.5089569091796875, 0.25447845458984375, 0.08482614904642105, 0.26739126443862915, 0.13369563221931458, 0.26739126443862915, 0.13369563221931458, 0.13369563221931458, 0.13369563221931458, 0.13369563221931458, 0.7277237772941589, 0.7218601703643799, 0.6584736704826355, 0.27436402440071106, 0.05487280711531639, 0.31734034419059753, 0.31734034419059753, 0.15867017209529877, 0.15867017209529877, 0.07933508604764938, 0.325997918844223, 0.325997918844223, 0.325997918844223, 0.4217507839202881, 0.4217507839202881, 0.9372462630271912, 0.8838462233543396], \"Term\": [\"academic\", \"academic\", \"access\", \"access\", \"access\", \"access\", \"account\", \"account\", \"accounting\", \"adapt\", \"addition\", \"addition\", \"addition\", \"advanced\", \"allow\", \"allow\", \"allow\", \"also\", \"also\", \"also\", \"also\", \"also\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analytical\", \"annotation\", \"annotation\", \"anywhere\", \"apparatus\", \"apparatus\", \"application\", \"application\", \"application\", \"approach\", \"approach\", \"approach\", \"approach\", \"archive\", \"archive\", \"archive\", \"article\", \"article\", \"automatic\", \"automatic\", \"automatic\", \"bauman\", \"believe\", \"black\", \"block\", \"boilerplate\", \"book\", \"book\", \"book\", \"bookkeepe\", \"box\", \"bridge\", \"browser\", \"build\", \"build\", \"build\", \"build\", \"capability\", \"capability\", \"case\", \"case\", \"case\", \"case\", \"category\", \"center\", \"change\", \"change\", \"change\", \"chapter\", \"character\", \"character\", \"circle\", \"cms\", \"code\", \"code\", \"collaboration\", \"collection\", \"collection\", \"collection\", \"commodity\", \"complete\", \"complete\", \"complex\", \"complex\", \"complex\", \"computer\", \"concern\", \"concern\", \"contain\", \"contain\", \"contain\", \"content\", \"content\", \"content\", \"content\", \"content\", \"conversion\", \"convert\", \"coordination\", \"corenlp\", \"corpu\", \"correspond\", \"course\", \"course\", \"creation\", \"creation\", \"critical\", \"critical\", \"critical\", \"critical\", \"css\", \"css\", \"curate\", \"curation\", \"currency\", \"dalmau\", \"datum\", \"datum\", \"datum\", \"datum\", \"datum\", \"decade\", \"deliver\", \"deliver\", \"design\", \"design\", \"design\", \"design\", \"develop\", \"develop\", \"develop\", \"develop\", \"develop\", \"development\", \"development\", \"development\", \"development\", \"development\", \"development\", \"different\", \"different\", \"different\", \"digital\", \"digital\", \"digital\", \"digital\", \"digital\", \"digitization\", \"digitization\", \"digitization\", \"digitize\", \"directly\", \"directly\", \"discourse\", \"discovery\", \"display\", \"display\", \"display\", \"display\", \"display\", \"distinct\", \"document\", \"document\", \"document\", \"domain\", \"double_entry\", \"drive\", \"easy\", \"economic\", \"edit\", \"edit\", \"edit\", \"editor\", \"editor\", \"editor\", \"editor\", \"effort\", \"effort\", \"egyptian\", \"egyptology\", \"electronic\", \"electronic\", \"electronic\", \"element\", \"element\", \"element\", \"element\", \"element\", \"emerge\", \"encode\", \"encode\", \"encode\", \"encode\", \"encode\", \"encode\", \"end\", \"end\", \"end\", \"even\", \"even\", \"existence\", \"expectation\", \"experience\", \"experience\", \"experience\", \"explain\", \"explain\", \"explain\", \"exploration\", \"expressive\", \"extensible\", \"faculty\", \"financial\", \"follow\", \"follow\", \"follow\", \"follow\", \"form\", \"form\", \"form\", \"form\", \"form\", \"form\", \"format\", \"format\", \"format\", \"format\", \"formation\", \"formation\", \"formula\", \"framework\", \"framework\", \"framework\", \"framework\", \"framework\", \"full\", \"full\", \"full\", \"full\", \"future\", \"future\", \"future\", \"future\", \"future\", \"gap\", \"gap\", \"generic\", \"graduate\", \"guideline\", \"guideline\", \"guideline\", \"guideline\", \"hack\", \"handle\", \"handle\", \"heart\", \"hieroglyphic\", \"historical\", \"history\", \"history\", \"history\", \"however\", \"however\", \"html\", \"import\", \"include\", \"include\", \"include\", \"include\", \"include\", \"include\", \"individual\", \"individual\", \"information\", \"information\", \"information\", \"information\", \"inherent\", \"initiative\", \"initiative\", \"institutional\", \"intensive\", \"internal\", \"introduction\", \"invaluable\", \"item\", \"keep\", \"keep\", \"language\", \"language\", \"language\", \"language\", \"language\", \"language\", \"large\", \"large\", \"large\", \"large\", \"largely\", \"learn\", \"ledger\", \"legacy\", \"level\", \"level\", \"leverage\", \"library\", \"library\", \"likely\", \"likely\", \"likely\", \"linguist\", \"linguist\", \"linguistic\", \"linguistic\", \"linguistic\", \"linguistically\", \"link\", \"list\", \"list\", \"list\", \"mad\", \"mail\", \"mailing\", \"maintenance\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"maker\", \"manually\", \"manuscript\", \"manuscript\", \"manuscript\", \"mass\", \"mean\", \"mean\", \"mean\", \"merchant\", \"metadata\", \"metadata\", \"metadata\", \"metadata\", \"mid\", \"minor\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"modular\", \"module\", \"much\", \"much\", \"much\", \"much\", \"name\", \"name\", \"namepart\", \"natural\", \"natural\", \"navigational\", \"need\", \"need\", \"need\", \"need\", \"need\", \"need\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"normalize\", \"note\", \"note\", \"note\", \"odd\", \"old\", \"old\", \"omeka\", \"order\", \"order\", \"order\", \"page\", \"page\", \"page\", \"pagination\", \"panellist\", \"participant\", \"people\", \"people\", \"people\", \"people\", \"permit\", \"permit\", \"personal\", \"platform\", \"platform\", \"position\", \"power\", \"pre\", \"precise\", \"present\", \"present\", \"present\", \"present\", \"present\", \"present\", \"presentation\", \"presentation\", \"presentation\", \"presentation\", \"process\", \"process\", \"process\", \"process\", \"process\", \"process\", \"processing\", \"processing\", \"processing\", \"processing\", \"processing\", \"professional\", \"programming\", \"project\", \"project\", \"project\", \"project\", \"project\", \"project\", \"pron\", \"pron\", \"pron\", \"pron\", \"pron\", \"pron\", \"proprietary\", \"prototype\", \"proust\", \"provide\", \"provide\", \"provide\", \"provide\", \"public\", \"public\", \"public\", \"publishing\", \"publishing\", \"pursuit\", \"qualitative\", \"quantitative\", \"quantitative\", \"query\", \"quite\", \"race\", \"racial\", \"railroad\", \"rather\", \"rather\", \"readable\", \"recognition\", \"recognition\", \"recommendation\", \"record\", \"record\", \"recourse\", \"reflection\", \"relevant\", \"rely\", \"rely\", \"represent\", \"represent\", \"represent\", \"represent\", \"research\", \"research\", \"research\", \"research\", \"research\", \"researchable\", \"resource\", \"resource\", \"resource\", \"respect\", \"restrict\", \"retiree\", \"retrieval\", \"retrieve\", \"retrieve\", \"revise\", \"richness\", \"robust\", \"romantic\", \"sample\", \"sat\", \"scale\", \"scholar\", \"scholar\", \"scholar\", \"scholar\", \"scholar\", \"scholarly\", \"scholarly\", \"school\", \"school\", \"scientific\", \"section\", \"segmentation\", \"seldom\", \"selection\", \"semantic\", \"semantic\", \"sentence\", \"sentence\", \"separately\", \"separation\", \"serve\", \"serve\", \"server\", \"service\", \"service\", \"set\", \"set\", \"set\", \"set\", \"sig\", \"sign\", \"signi\\ufb01cant\", \"site\", \"software\", \"software\", \"software\", \"software\", \"source\", \"source\", \"source\", \"source\", \"source\", \"span\", \"specific\", \"specific\", \"specific\", \"specification\", \"specify\", \"standard\", \"standard\", \"standard\", \"standard\", \"standardization\", \"static\", \"storage\", \"storage\", \"storage\", \"streamline\", \"structural\", \"structure\", \"structure\", \"structure\", \"structure\", \"structure\", \"style\", \"support\", \"support\", \"support\", \"support\", \"surface\", \"surface\", \"system\", \"system\", \"system\", \"system\", \"system\", \"tag\", \"tag\", \"tag\", \"tag\", \"teach\", \"teach\", \"teaching\", \"teaching\", \"technology\", \"technology\", \"text\", \"text\", \"text\", \"text\", \"text\", \"text\", \"textual\", \"textual\", \"textual\", \"textual\", \"textual\", \"third\", \"together\", \"tool\", \"tool\", \"tool\", \"tool\", \"tool\", \"tool\", \"training\", \"transaction\", \"transcription\", \"transcription\", \"transcription\", \"transformation\", \"transformation\", \"transformation\", \"transliteration\", \"type\", \"type\", \"type\", \"type\", \"type\", \"typical\", \"uneasy\", \"unicode\", \"unit\", \"unit\", \"user\", \"user\", \"user\", \"user\", \"usually\", \"usually\", \"value\", \"value\", \"value\", \"variant\", \"variant\", \"various\", \"various\", \"various\", \"various\", \"various\", \"visualize\", \"volume\", \"volume\", \"want\", \"way\", \"way\", \"way\", \"way\", \"way\", \"web\", \"web\", \"web\", \"web\", \"web\", \"well\", \"well\", \"well\", \"well\", \"well\", \"well\", \"well\", \"whereby\", \"willing\", \"word\", \"word\", \"word\", \"work\", \"work\", \"work\", \"work\", \"work\", \"workshop\", \"workshop\", \"workshop\", \"world\", \"world\", \"zone\", \"\\ufb01nancial_record\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [1, 2, 4, 5, 3, 7, 6]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el103611404428975305768808566211\", ldavis_el103611404428975305768808566211_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el103611404428975305768808566211\", ldavis_el103611404428975305768808566211_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el103611404428975305768808566211\", ldavis_el103611404428975305768808566211_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
       "topic                                                \n",
       "0      0.068432 -0.132870       1        1  32.843327\n",
       "1     -0.010048 -0.050845       2        1  20.277517\n",
       "3     -0.159072 -0.036058       3        1  16.138783\n",
       "4      0.083311  0.053077       4        1  11.683634\n",
       "2     -0.027165  0.107379       5        1  11.080515\n",
       "6      0.013245  0.028557       6        1   4.595500\n",
       "5      0.031296  0.030760       7        1   3.380727, topic_info=          Term       Freq      Total Category  logprob  loglift\n",
       "1188       mad  40.000000  40.000000  Default  30.0000  30.0000\n",
       "246       text  62.000000  62.000000  Default  29.0000  29.0000\n",
       "120   document  24.000000  24.000000  Default  28.0000  28.0000\n",
       "535    project  32.000000  32.000000  Default  27.0000  27.0000\n",
       "107    content  18.000000  18.000000  Default  26.0000  26.0000\n",
       "...        ...        ...        ...      ...      ...      ...\n",
       "0         also   0.475873  19.945107   Topic7  -5.7821  -0.3485\n",
       "416   standard   0.474304  15.908896   Topic7  -5.7854  -0.1257\n",
       "345    include   0.472924  20.512632   Topic7  -5.7884  -0.3828\n",
       "503       form   0.471694  13.024633   Topic7  -5.7910   0.0688\n",
       "303    develop   0.470083  12.182930   Topic7  -5.7944   0.1322\n",
       "\n",
       "[442 rows x 6 columns], token_table=      Topic      Freq             Term\n",
       "term                                  \n",
       "688       1  0.277679         academic\n",
       "688       7  0.555358         academic\n",
       "270       1  0.100895           access\n",
       "270       2  0.302686           access\n",
       "270       3  0.403582           access\n",
       "...     ...       ...              ...\n",
       "264       7  0.325998         workshop\n",
       "907       1  0.421751            world\n",
       "907       7  0.421751            world\n",
       "269       1  0.937246             zone\n",
       "910       1  0.883846  ﬁnancial_record\n",
       "\n",
       "[686 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[1, 2, 4, 5, 3, 7, 6])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA Mallet Model ##\n",
    "\n",
    "### Passage de l'algorithme de Gensim à celui de Mallet ###\n",
    "\n",
    "Pour la suite, nous utiliserons l'algorithme de topic modeling de Mallet plutôt que celui proposé par Gensim. Bien que Gensim offre une meilleure approche de Topic Modeling, car chaque étape est bien séparée et peut être personnalisée (nettoyage de la donnée ou *preprocessing*, construction et paramètrage du modèle de Topic Modeling), l'algorithme de Topic Modeling proposé par Mallet est un peu plus précis que celui de Gensim, en offrant notamment une meilleure qualité de sujets.\n",
    "\n",
    "En 2014 Radim Řehůřek, créateur de Gensim, a créé un Wrapper permettant d'utiliser l'algorithme de Mallet. Nous allons donc télécharger les dossiers de Mallet depuis GitHub et utiliser ce wrapper pour utiliser l'algorithme de Mallet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les Topics mesurés seront légèrement différents. Tout d'abord, il faut télécharger Mallet dans le cache :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('./cache2012/Mallet'):\n",
    "    git.Git(\"./cache2012\").clone(\"https://github.com/mimno/Mallet.git\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___Important___ : Il faut maintenant utiliser le terminal pour aller dans ./cache2012/Mallet et taper simplement la commande \" ant \" (rendu possible suite à votre installation de Ant Apache comme indiqué dans le README). Cela permet de compiler le script Java de Mallet. Si le terminal affiche \"Building Successfully\", vous pouvez passer à l'étape suivante, c'est à dire implémenter l'algorithme de LDA Mallet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "mallet_path = \"./cache2012/Mallet/bin/mallet\"\n",
    "ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=7, id2word=id2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important**\n",
    "Si une erreur se déclenche, c'est que vous n'avez pas compilé manuellement le script Java de Mallet. Allez dans cache2019/Mallet, ouvrez votre terminal et tapez simplement \" ant \". Relancez ensuite la cellule, cela fonctionnera normalement à chaque lancement.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  [('make', 0.0497131931166348),\n",
      "   ('page', 0.03824091778202677),\n",
      "   ('present', 0.03441682600382409),\n",
      "   ('edition', 0.022944550669216062),\n",
      "   ('output', 0.022944550669216062),\n",
      "   ('element', 0.021032504780114723),\n",
      "   ('user', 0.019120458891013385),\n",
      "   ('zone', 0.017208413001912046),\n",
      "   ('change', 0.017208413001912046),\n",
      "   ('order', 0.017208413001912046)]),\n",
      " (1,\n",
      "  [('content', 0.04780114722753346),\n",
      "   ('digital', 0.03441682600382409),\n",
      "   ('critical', 0.0248565965583174),\n",
      "   ('pron', 0.022944550669216062),\n",
      "   ('image', 0.019120458891013385),\n",
      "   ('site', 0.019120458891013385),\n",
      "   ('teach', 0.019120458891013385),\n",
      "   ('romantic', 0.017208413001912046),\n",
      "   ('future', 0.017208413001912046),\n",
      "   ('circle', 0.015296367112810707)]),\n",
      " (2,\n",
      "  [('pron', 0.056795131845841784),\n",
      "   ('tool', 0.04462474645030426),\n",
      "   ('work', 0.032454361054766734),\n",
      "   ('support', 0.030425963488843813),\n",
      "   ('simple', 0.02028397565922921),\n",
      "   ('set', 0.018255578093306288),\n",
      "   ('full', 0.016227180527383367),\n",
      "   ('open', 0.016227180527383367),\n",
      "   ('annotation', 0.016227180527383367),\n",
      "   ('transcription', 0.016227180527383367)]),\n",
      " (3,\n",
      "  [('text', 0.05742296918767507),\n",
      "   ('encode', 0.05182072829131653),\n",
      "   ('document', 0.04201680672268908),\n",
      "   ('project', 0.03361344537815126),\n",
      "   ('collection', 0.02661064425770308),\n",
      "   ('model', 0.023809523809523808),\n",
      "   ('provide', 0.02100840336134454),\n",
      "   ('web', 0.02100840336134454),\n",
      "   ('framework', 0.0196078431372549),\n",
      "   ('scholar', 0.0196078431372549)]),\n",
      " (4,\n",
      "  [('mad', 0.09336099585062241),\n",
      "   ('type', 0.05394190871369295),\n",
      "   ('note', 0.03319502074688797),\n",
      "   ('structure', 0.026970954356846474),\n",
      "   ('linguistic', 0.024896265560165973),\n",
      "   ('editor', 0.022821576763485476),\n",
      "   ('variant', 0.02074688796680498),\n",
      "   ('database', 0.014522821576763486),\n",
      "   ('record', 0.014522821576763486),\n",
      "   ('automatic', 0.012448132780082987)]),\n",
      " (5,\n",
      "  [('text', 0.057803468208092484),\n",
      "   ('source', 0.05202312138728324),\n",
      "   ('process', 0.03901734104046243),\n",
      "   ('word', 0.033236994219653176),\n",
      "   ('language', 0.033236994219653176),\n",
      "   ('tag', 0.028901734104046242),\n",
      "   ('textual', 0.02601156069364162),\n",
      "   ('datum', 0.024566473988439308),\n",
      "   ('design', 0.02023121387283237),\n",
      "   ('unit', 0.017341040462427744)]),\n",
      " (6,\n",
      "  [('system', 0.03907380607814761),\n",
      "   ('include', 0.0361794500723589),\n",
      "   ('project', 0.027496382054992764),\n",
      "   ('standard', 0.02604920405209841),\n",
      "   ('record', 0.02604920405209841),\n",
      "   ('develop', 0.02170767004341534),\n",
      "   ('represent', 0.020260492040520984),\n",
      "   ('information', 0.01881331403762663),\n",
      "   ('case', 0.01881331403762663),\n",
      "   ('research', 0.015918958031837915)])]\n"
     ]
    }
   ],
   "source": [
    "# Voici les \"nouveaux\" topics calculés par Mallet\n",
    "pprint(ldamallet.show_topics(formatted=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons comparer le score de cohérence de ces algorithmes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coherence Score of LDA Mallet :  0.3790087221476967\n",
      "\n",
      "Coherence Score of LDA Gensim :  0.37073201142673795\n"
     ]
    }
   ],
   "source": [
    "# Je calcule le score de cohérence de Mallet\n",
    "coherence_model_ldamallet = CoherenceModel(model=ldamallet, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_ldamallet = coherence_model_ldamallet.get_coherence()\n",
    "print('\\nCoherence Score of LDA Mallet : ', coherence_ldamallet)\n",
    "\n",
    "# Je calcule le score de Gensim\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score of LDA Gensim : ', coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme on peut le voir, Mallet est légèrement meilleur que Gensim. Il y a donc un certain intérêt à continuer avec Mallet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Critique du choix du nombre de clusters ###\n",
    "\n",
    "Nous allons ici voir si le nombre de clusters choisi est vraiment représentatif et optimal. Pour calculer cela, nous nous baserons sur la \" cohérence \", un coefficient calculé par l'ordinateur. Tout d'abord, nous créons une fonction permettant de calculer la cohérence :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "    \"\"\"\n",
    "    Calcule la cohérence c_v pour un nombre variable de topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Dictionnaire de Gensim\n",
    "    corpus : Corpus de Gensim\n",
    "    texts : Liste des corpus\n",
    "    limit : Nombre maximum de corpus\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : Liste des modèles de topic LDA topic\n",
    "    coherence_values : Valeur de la cohérence correspondance au model de LDA et son nombre de topics respectif\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=num_topics, id2word=id2word)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puis nous cherchons à voir si cela est vraiment cohérent. Nous appliquons cette fonction au corpus. La fonction lancée ci-dessous va attribuer à chaque \"model_list\", une élément propre à Gensim regroupant la liste des topics attribués à un corpus (élément copié sur celui de Mallet mais avec cet avantage de ne pas stocker dans la RAM les textes, allégeant grandement le processus) la variable \"coherence_values\" qui est une liste constituée de nombres décimaux représentant la cohérence de chaque topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Peut prendre du temps\n",
    "model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=data_lemmatized, start=2, limit=40, step=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puis nous affichons le résultat sous la forme d'un graphique :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEGCAYAAABy53LJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3deXyV1Z348c83GwmQBUgCCQETFkV2IYLiWkcU64IWUeuG2J+KS+tMp47293Oc1tqZ2k6dXzs/q2JlsYrUgBY6ahmXARUUuWEPiEBYckOEQAiBkP1+f3/cJ3iJWW4gl+fe5Pt+vfLKfc7zPCffe8V8c855zjmiqhhjjDHBinI7AGOMMZHFEocxxph2scRhjDGmXSxxGGOMaRdLHMYYY9olxu0AzoTU1FTNzs52OwxjjIko+fn5B1U1rWl5l0gc2dnZeDwet8MwxpiIIiJ7miu3ripjjDHtYonDGGNMu1jiMMYY0y5dYoyjOXV1dXi9Xqqrq90OpVnx8fFkZWURGxvrdijGGHOSLps4vF4viYmJZGdnIyJuh3MSVeXQoUN4vV5ycnLcDscYY07SZbuqqqur6dOnT9glDQARoU+fPmHbGjLGdG1dNnEAYZk0GoVzbMaYri2kiUNEpojINhHZISJPtHLdNBFREcl1ju8QkfUBXz4RGeucW+7U2XguPZTvwRjTusqaet5cU8TybQfw+Wybhq4gZGMcIhINPA9MBrzAGhFZqqpbmlyXCDwKrG4sU9XXgded86OAv6jq+oDb7lBVm9FnjIsOHqvh1VW7mf/ZHo5U1QEwKLUH91yUzbRxWfTo1mWHUDu9UP6XnQDsUNVCABFZCEwFtjS57hfAs8BjLdTzfWBhqII0xrTP3kPHefmTQt70FFHb4GPyuX2579JBFB+uYu7KXTy1pIDfLNvGrbkDmDEpmwG9u7sdsulgoUwc/YGigGMvMDHwAhEZBwxQ1XdEpKXEcSv+hBNorog0AIuBZ7SZbQxF5H7gfoCBAwee2jsIsVdffZV///d/R0QYPXo0f/rTn9wOyZgWbS4+wosrdvLuphJioqK46bz+3HfpIIak9wTg/GyYOjaTtXvLmbtyF3NX7eaVlbu48ty+zLwomwsHhefDKKb9XGtLikgU8BxwTyvXTASOq+rmgOI7VLXY6eJaDNwFvNr0XlWdDcwGyM3NbbXj9ed/LWDLvop2v4fWDM9M4l+uH9Hi+YKCAp555hlWrVpFamoqZWVlHfrzjekIqsqnOw7y0opCPt1xkMRuMdx36SDuvSiHvknx37peRBh/Vi/Gn9WLkiNVvPb5Hhas3sv7W/YzrF8iMy/KZurY/sTHRrvwbkxHCWXiKAYGBBxnOWWNEoGRwHLnr5B+wFIRuSFg/OI24I3ASlW12Pl+VEQW4O8S+1biCHcfffQR06dPJzU1FYDevXu7HJEx36hv8PHu5q95acVOCvZVkJ7YjSeuGcbtEweSFB/cpNSM5AQeu3oYP7xiKEvWFzN35W4eX7yJX733JbdPHMhdF2TTL/nbyceEv1AmjjXAUBHJwZ8wbgNubzypqkeA1MZjEVkO/KQxaTgtkluASwKuiQFSVPWgiMQC1wEfnG6grbUMjOlKqmobyMsv4uVPCikqq2JQWg+enTaKG8/rT7eYU2slxMdGc+v5A7kldwCfFR5i7srd/GH5Tl5aUcg1ozKYeVE24wb26uB3YkIpZIlDVetF5BFgGRANzFHVAhF5GvCo6tI2qrgUKGocXHd0A5Y5SSMaf9J4OQThh9wVV1zBTTfdxI9//GP69OlDWVmZtTqMaw5X1vLqZ3uY/9luyiprOW9gCk9eO5zJ5/YlKqpjxiVEhEmDU5k0OJW9h44z/7PdvLmmiL9u2MeYASnMnJTNd0dlEBfTpaeXRQRpZly508nNzdWm+3Fs3bqVc88916WI/ObPn89vfvMboqOjOe+885g3b95J58MhRtO5eQ8f54+f7OLPa4qoqmvg74al88Blgzk/u9cZGciurKln8Vov81bupvBgJemJ3bjzgrO4feJAUnt2C/nPN60TkXxVzf1WuSWO8BUJMZrItLWkgpdW7OSvG0sQYOrY/tx/6SDO6ZfoSjw+n7JieylzV+7m469KiYuJ4oYxmcy8KJsRmcmuxGRaThw2Q8eYLkJV+bywjBdX7GTFV6X0iItm5qRs7r04h8yUBFdji4oSvnNOOt85J50dB44yb9VuFucXsyjfy4Sc3tx7UTaTh/cjuoO6zczpscRhTCfX4FP+u+BrXlyxkw3eI6T2jOOxq8/hzolnkdw9/JbtH5KeyDM3juKxq4bxZ89e5q/aw6zX1tI/JYEZk87i1tyBYRl3uFFVSo/WkN7MY9Onq0snDlUN2wlJXaEL0YRWdV0Db60t5uVPCtl1sJKz+nTnlzeNZNq4rIiYR5HcPZb7Lx3MvRfl8MHW/cxZuZt/ffdL/uP97Uwb3597JuWcmHxo/EvAbPSWs6HoCBu95Wz0HuFIVR2bf351h//37rKJIz4+nkOHDoXl0uqN+3HEx9sz7qb9jlTV8drne5i7cjcHj9UwOiuZP9wxjqtHRGZXT0x0FFNGZjBlZAYF+44wb+Vu3vR4ee3zvVwyNJV7L8rhsrPTOuzpr0hwtLqOTcVHTkoSxeVVAEQJDEnvyXeGpTNmQAq+EPwR2mUHx20HQNPZlBypYs6nu1iwei+VtQ1cenYasy4b1CmX+jh4rIY3Vu/lT5/v4cDRGgal9mDGpGymjc+iZydbXLG6roEtJRVsLPIniA3ecgoPVtL4q3tg7+6MzkpmTFYKo7OSGdk/ucMWmLSnqjy2mK7pnLbvP8pLHxeyZH0xPoXrRmdw/6WDusTTSLX1Pt7bXMKclbvZUFROYrcYbjl/ADMuzGZgn8hbXLG+wcf2A8f8XU7eI2woKmfb10epd5arT0vsxpisZEY7SWJ0Vgq9e8SFLB5LHJY4TCezZncZL63YyQdbDxAfG8Vt5w/kBxfndNnVaNfuPczclbt5b1MJDaphv7iiqrL70PGTxiUK9lVQVdcAQGJ8zIlWxOisFMYMSKZfUvwZfS+WOCxxmE7A51M+2Lqflz4uJH/PYXp1j2XGpGzuvjA7pH95RpKvj1T7F1f8Yi9llbVhs7ji10eq2eAtPzEm0Th4DRAfG8WIzOSTupyy+/RwfdzGEoclDhPBauobWLJuHy99vJOdpZVk9UrgvksGMT03i+5xnatPv6NU1zWwdP0+5qzcxZdfH6VX91hunziQOy84i4zk0M5bKT9e6x+PKPJ3OW30lnPgaA0A0VHCOX0TGTMg5US309l9exITHX5LrVjisMRhItDR6jre+GIvr3y6i/0VNZybkcSsywZx7aiMsPxFE44aJz7OXbmL97fuJ1qEKSP7MfOiHMYNTDntrp/jtfVsLq44MS6x0VvOnkPHT5wflNbjpC6nEZlJEfE4NFjisMRhIsqBimrmrtrNa5/v4Wh1PZMG92HWZYO5ZGhqWPbXR4qisuPMX7WbP3uKOFpdz5isZGZelBP04oq19T62fX30pC6nr/YfpXGr9czkeP/A9QB/l9PI/skkJ0Tuk5GWOCxxmAhQWHqMlz8pZHF+MfU+H9eMzOCBywYxOivF7dA6lWAWV/T5lMKDx1jvDFxv8B5ha0kFtfU+AHp1j2XMgBT/wLXTmkhL7FwLM1risMRhwti6vYd5aUUhy7Z8TWx0FNPHZ3HfJYPITu3hdmidms+nfOwsrrjiq1LioqO4cng6ZZW1bC6u4FhNPQA94qIZ2T/ZSRT+1kRWr4RO3/qzRQ6NCTOqyvJtpby4Yierd5WRFB/Dw5cPYcak7E73l2u4iooSLj8nncvPSWfHgWPMX7Wb9zaX0D8lgZvO639iAHtQWs+InHUfKtbiMMYFx2rqeej1tXz8VSkZyfH84OIcbpswsNPNejaRzVocxoSJQ8dqmDlvDQX7KviX64dz5wVnEWtPSJkIYonDmDOoqOw4M+Z8QXF5FS/dOZ4rh/d1OyRj2s0ShzFnyJdfVzBjzhdU1Tbw2v+ayPnZtse8iUyWOIw5A9bsLuMH89aQEBdN3qxJrm3RakxHCGnHqohMEZFtIrJDRJ5o5bppIqIikuscZ4tIlYisd75eDLh2vIhscur8vXT25+FMxHt/y37u/ONqUhO7sfhBSxom8oWsxSEi0cDzwGTAC6wRkaWquqXJdYnAo8DqJlXsVNWxzVT9AnCfc/27wBTgvQ4O35gO8eaaIp54ayOj+iczd+YEW4jQdAqhbHFMAHaoaqGq1gILganNXPcL4FmgzR2VRCQDSFLVz9X/HPGrwI0dGLMxHUJV+cPyHfzT4o1cNCSVBfddYEnDdBqhTBz9gaKAY69TdoKIjAMGqOo7zdyfIyLrRGSFiFwSUKe3tToD6r5fRDwi4iktLT3lN2FMe/l8yi/+ayu//ts2bhiTySszzu+wHdmMCQeu/WsWkSjgOeCeZk6XAANV9ZCIjAf+IiIj2lO/qs4GZoN/AuBphmtMUGrrfTy2aANL1u9j5kXZ/PO1w13fU8GYjhbKxFEMDAg4znLKGiUCI4Hlzvh2P2CpiNygqh6gBkBV80VkJ3C2c39WK3Ua45rKmnoedGaD/9OUc3jwssGdfi0j0zWFsqtqDTBURHJEJA64DVjaeFJVj6hqqqpmq2o28Dlwg6p6RCTNGVxHRAYBQ4FCVS0BKkTkAudpqruBJSF8D8YEpayyltv/uJpPt5fy7LRRPHT5EEsaptMKWYtDVetF5BFgGRANzFHVAhF5GvCo6tJWbr8UeFpE6gAfMEtVy5xzDwHzgAT8T1PZE1XGVd7Dx7l7zhcUH67ipbtymWyzwU0nZ4scGnMatn19lLvnrOZ4bQOvzDifCTk2G9x0HrbIoTEdzLO7jHvnrSE+Npq8WRcyrF+S2yEZc0ZY4jDmFHywZT8PL1hL/5QE5t87gQG9u7sdkjFnjCUOY9rpTU8RP31rEyMyk5h7z/n06WmbLpmuxRKHMUFSVV5cUcizf/uSS4am8sKd423jJdMl2b96Y4Lg8ym/fHcrr3y6i+vHZPLb6WOIi7HNl0zXZInDmDbUNfj4p0UbeXtdMfdMyuap62w2uOnaLHEY04rjtfU8+NpaVnxVymNXn8NDl9tscGMscRjTgsOVtcyct4aN3nJ+9b1R3DZhoNshGRMWLHEY04zi8irufmU1RYereOHO8Vw9op/bIRkTNixxGNPEV/uPcvcrX1BZW8+f7p3AxEF93A7JmLBiicOYAPl7yrh3noduMVG8+cCFnJths8GNacoShzGOj77cz0OvryUjOYFXbTa4MS2yxGEMsCjfy+OLNzI8I4m5M88n1WaDG9MiSxymy3tpxU7+7b0vuXhIKi/eZbPBjWmL/R9iuiyfT/m397by8ie7uG50Br+9ZQzdYqLdDsuYsNfmmgkicraIfCgim53j0SLyZOhDMyZ06hp8/CRvAy9/sosZF57F7287z5KGMUEKZrGdl4GfAnUAqroR/zawxkSk47X13Peqh7fWFfOTq87mZzeMsCVEjGmHYLqquqvqF02WWagPUTzGhNThylrunb+GDUXl/Nv3RvF9mw1uTLsFkzgOishgQAFE5GagJKRRGRMC+8qruHvOF+wtO84f7hjPlJE2G9yYUxFM4ngYmA0ME5FiYBdwZ0ijMqaDbd9/lLvnfMGx6npevXcCF9hscGNOWZtjHKpaqKpXAmnAMFW9WFV3B1O5iEwRkW0iskNEnmjlumkioiKS6xxPFpF8EdnkfL8i4NrlTp3rna/0YGIxXVf+nsNMf+kz6n3Knx+40JKGMacpmKeq/lVEUlS1UlWPikgvEXkmiPuigeeBa4DhwPdFZHgz1yUCjwKrA4oPAter6ihgBvCnJrfdoapjna8DbcViuq7/+fIAd/zxc1ISYlk8axLDM20JEWNOVzBPVV2jquWNB6p6GPhuEPdNAHY4LZZaYCEwtZnrfgE8C1QH/Ix1qrrPOSwAEkTEpvKadlmc7+V/vephSHpPFj04iYF9bAkRYzpCMIkjOvCXtogkAMH8Eu8PFAUce52yE0RkHDBAVd9ppZ5pwFpVrQkom+t0U/2ztLCrjojcLyIeEfGUlpYGEa7pTGZ/vJN/zNvABYN688Z9F9gSIsZ0oGAGx18HPhSRuc7xTGD+6f5gEYkCngPuaeWaEfhbI1cFFN+hqsVOF9di4C7g1ab3qups/IP65Obm6unGayKDqvKr977kpY8LuXZ0Bs/ZbHBjOlybiUNVnxWRjcDfOUW/UNVlQdRdDAwIOM5yyholAiOB5U6joR+wVERuUFWPiGQBbwN3q+rOgHiKne9HRWQB/i6xbyUO0/XUNfh4YvEmFq/1cveFZ/Ev148g2ib2GdPhglqrSlXfA95rZ91rgKEikoM/YdwG3B5Q5xEgtfFYRJYDP3GSRgrwDvCEqq4MuCYGSFHVgyISC1wHfNDOuLqcBp/y+w+341MlIzmBjJR4Mp3vSfGxbofXIapqG3h4wVo++vIAP558Nj+8YojtDW5MiLSZOETke/i7i9IBcb5UVVt9PEVV60XkEWAZEA3MUdUCEXka8Kjq0lZufwQYAjwlIk85ZVcBlcAyJ2lE408aL7f1Hrq6FV8d4Hcfbm/2XM9uMfRLjicj+Ztk0vg9IzmBjOR4eoT5arHlx2u5d94a1heV8683jeL2iTYb3JhQEtXWu/9FZAf+R2O3npmQOl5ubq56PB63w3DNg6/ls3pXGaueuIKyylpKjlSxr7z6pO9fH6lm35FqSo/WfOv+pPgYMlP8SSQjJYHM5G+SSoZTHh/rzjhCyZEq7n7lC/aUHef3t41lysgMV+IwpjMSkXxVzW1aHsyfkvsjOWl0dWWVtXywdT93XZBNfGw0mSkJZKYkMP6s5q+vrfexv6KafeVVlBypdr6+STAbvEcoq6z91n29e8T5E0lyApkp8fRrbMEkx5OZkkDfpHjiYoJ5iC94Ow749wY/Wl3P/JkTuHCwTewz5kwIJnF4ROTPwF+AE3+OqupbIYvKdJgl64upa1BuOT8rqOvjYqIY0Lt7q9umVtc1nEgoJY0tlyPVlJRX4T18nC92HaKi+uR1MEUgtWc3MpPjna4xf4IJ/J6e2I2Y6OCSy7q9h5k5bw0xUVEsfOACRmQmB3WfMeb0BZM4koDjnPxIrAKWOCJAnsfLqP7JDOvXcTOm42OjyUntQU5qjxavqaypPym57Av4XlhaycodhzhWc3JyiRJIT4z/ZpwlsGvM6RJL69mNFdtLeei1taQndeNP9060iX3GnGHBPI4780wEYjpewb4jbCmp4OmpI874z+7RLYYh6T0Zkt6zxWsqqutOSiolR6pOJJutJRV8+OV+qut8J90TEyX4VDk3I4l5MyeQlmgT+4w504J5qups4AWgr6qOFJHRwA2q2uZ6VcZdeR4vcdFR3DAm0+1QmpUUH0tSv1jO6ZfY7HlVpfx4HfsCBu9LyqsAePDywSR2kkeJjYk0wXRVvQw8BrwE/h0AnYl3ljjCWE19A0vWFzN5RF9Suse5Hc4pERF69YijV484G8MwJowEMxLZXVW/aFJmOwCGuQ+3HuDw8Tqmjw9uUNwYY4IVTOKwHQAjUJ6niH5J8VwyNM3tUIwxncyp7gB4R0ijMqdlf0U1K74qZdZlg22tJmNMh2s1cTibMT2kqleKSA8gSlWPnpnQzKl6a20xPoWbrZvKGBMCrSYOVW0QkYud15VnJiRzOlSVvPwics/qxaC0lh+FNcaYUxVMV9U6EVkK5OFfZBCwmePhau3ecgpLK3lg2iC3QzHGdFLBJI544BBwRUCZzRwPU4vyi0iIjeba0eE5d8MYE/ls5ngnUlXbwF83lPDdURn0DPOl0I0xkavNx3FF5GwR+VBENjvHo0XkydCHZtrrbwUlHKupZ3quDYobY0Kn2cQhIrNEZJhz+DLwU6AO/DPH8e/mZ8JMnsfLwN7dmZjT2+1QjDGdWEstjteAJ5zXNnM8AhSVHWfVzkPcPD7Ltkw1xoRUs4lDVY8B9zmHNnM8AizK9yIC02zuhjEmxFocQVXVOudlczPH7zwDsZkg+XzKonwvFw1OpX9KgtvhGGM6uTYHx1W1UFWvBNKAYap6saruDqZyEZkiIttEZIeIPNHKddNEREUkN6Dsp85920Tk6vbW2ZV8XniI4vIqGxQ3xpwRwezH0Q2YBmQDMY3956r6dBv3RQPPA5MBL7BGRJaq6pYm1yUCjwKrA8qG4x+AHwFkAh84+4IQTJ1dTV6+l8T4GK4e0c/tUIwxXUAwq+MuAabiHxCvDPhqywRgh9NiqQUWOvU09QvgWaA6oGwqsFBVa1R1F7DDqS/YOruMiuo63ttcwvVjMomPjXY7HGNMFxDMLLEsVZ1yCnX3B4oCjr3AxMALRGQcMEBV3xGRx5rc+3mTe/s7r1uts6t5Z2MJ1XU+23fDGHPGBNPiWCUiozr6B4tIFPAc8I8dXbdT//0i4hERT2lpaSh+RFjI8xQxJL0nYwekuB2KMaaLaLHFISKb8D+CGwPMFJFCoAYQQFV1dBt1FwMDAo6znLJGicBIYLkzbtIPWCoiN7Rxb2t1nqCqs/E/DUZubq62EWtE2nHgGGv3lvO/vzvM5m4YY86Y1rqqrjvNutcAQ0UkB/8v99uA2xtPquoRILXxWESWAz9RVY+IVAELROQ5/IPjQ4Ev8CetFuvsahble4mOEm48r3/bFxtjTAdpbR7HnsbXIjIGuMQ5/ERVN7RVsarWi8gjwDIgGpijqgUi8jTgUdWlrdxbICJvAlvwD8o/rKoNTizfqrOtWDqj+gYfb6318p1z0khPjHc7HGNMFxLM47iP4p9F3riM+msiMltV/7Ote1X1XeDdJmVPtXDt5U2Ofwn8Mpg6u6JPth/kwNEabh4/oO2LjTGmAwXzVNUPgImNOwCKyLPAZ0CbicOETl5+Eb17xHHFsHS3QzHGdDHBPFUlQEPAcYNTZlxSVlnL+1v2c+PY/sTFBPOf0BhjOk4wLY65wGoReds5vhF4JXQhmbYsWV9MXYPaEiPGGFcEswPgc84TTxc7RTNVdV1IozKtyvN4Gdk/iXMzktwOxRjTBQUzOH4BUKCqa53jJBGZqKqr27jVhEDBviNsKang5zeMcDsUY0wXFUwH+QvAsYDjY06ZcUGex0tcdBRTx2a6HYoxposKanBcVU/MvFZVH8GNjZgOVlvvY8n6YiYP70tK9zi3wzHGdFHBJI5CEfmRiMQ6X48ChaEOzHzbh1v3c/h4HTfboLgxxkXBJI5ZwCT8S3w0rkZ7fyiDMs3Ly/fSLymeS4emuR2KMaYLC+apqgP414QyLjpQUc3ybQeYddlgoqNsGo0xxj02eyxCvLWuGJ/CzbbvhjHGZZY4IoCqkucpIvesXgxK6+l2OMaYLs4SRwRYV1TOztJKmylujAkLbSYOEekrIq+IyHvO8XAR+UHoQzON8jxFJMRGc+1om7thjHFfMC2Oefj3v2j8rfUV8PehCsicrKq2gb9uKOGaUf3o2c2mzxhj3BdM4khV1TcBH/g3aOLk1XJNCP2toIRjNfVMt303jDFhIpjEUSkiffDvP964dtWRkEZlTsjzeBnQO4GJOb3dDsUYY4Dglg75MbAUGCwiK4E04OaQRmUAKCo7zqqdh/iHK88myuZuGGPCRDATANeKyGXAOfg3cNqmqnUhj8yweK0XEZg2vr/boRhjzAktJg4R+V4Lp84WEVT1rRbOmw7g8ymL8r1cNDiVrF7d3Q7HGGNOaK3Fcb3zPR3/WlUfOcffAVYBbSYOEZkC/A6IBv6oqr9qcn4W8DD+wfZjwP2qukVE7gAeC7h0NDBOVdc7m0plAFXOuaucZVE6lc93HcJ7uIrHrj7H7VCMMeYkLSYOVZ0JICL/DQxX1RLnOAP/I7qtEpFo4HlgMv7FEdeIyFJV3RJw2QJVfdG5/gbgOWCKqr4OvO6UjwL+oqrrA+67Q1U9Qb/LCLTI4yUxPoarR/RzOxRjjDlJME9VDWhMGo79wMAg7psA7FDVQlWtBRYCUwMvUNWKgMMeOE9uNfF9594u42h1He9uLuH6MZnEx0a7HY4xxpwkmKeqPhSRZcAbzvGtwAdB3NcfKAo4blyS/SQi8jD+J7figCuaqedWmiQcYK6INACLgWcCN5oKqPd+nOXfBw4MJs+Fj3c2llBd52O6LWhojAlDbbY4VPUR4EVgjPM1W1V/2FEBqOrzqjoYeBx4MvCciEwEjqvq5oDiO1R1FHCJ83VXC/XOVtVcVc1NS4us/Sve9BQxJL0nYwekuB2KMcZ8S1BrWKjq28Db7ay7GAic7pzllLVkId/ey/w2vmnpNMZS7Hw/KiIL8HeJvdrO2MLWjgPHWLu3nJ9eMwwRm7thjAk/oVwddw0wVERyRCQOfxJYGniBiAwNOLwW2B5wLgq4hYDxDRGJEZFU53UscB0Q2BqJeIvyvURHCTeNs7kbxpjwFLJV81S1XkQewb9AYjQwR1ULRORpwKOqS4FHRORKoA44DMwIqOJSoEhVA/c37wYsc5JGNP6xlpdD9R7OtPoGH2+t9XL52WmkJ8a7HY4xxjQrqMQhIgnAQFXd1p7KVfVd4N0mZU8FvH60lXuXAxc0KasExrcnhkjyyfaDHDhaY/tuGGPCWjD7cVwPrAf+5hyPFZGlrd9lTkVefhG9e8RxxbC+bodijDEtCmaM42f4B6DLAZyJeDkhjKlLOlxZywdbDjB1bCZxMbYxozEmfAXzG6pOVZsuo97cRD1zGpasL6a2wWf7bhhjwl4wYxwFInI7EO08BfUj/GtVmQ6Ul+9lZP8khmcmuR2KMca0KpgWxw+BEUANsAD/Jk62dWwH2rKvgoJ9FdbaMMZEhFZbHM5Che+o6neA/3NmQup68vKLiIuOYurYzLYvNsYYl7Xa4lDVBsAnIslnKJ4up7bex1/WFTN5eF9Suse5HY4xxrQpmDGOY8AmEXkfqGwsVNUfhSyqLuTDrfs5fLyOm23uhjEmQgSTON4iiE2bzKnJy/fSN6kblw6NrIUYjTFdVzB7js931po62ymyPcc7yIGKapZvO8ADlw0mOsoWNDTGRIY2E4eIXA7MB23jEs0AABIiSURBVHYDAgwQkRmq+nFoQ+v83lpXjE+xfTeMMRElmK6q3+Lf13sbgIicjX+p8067ZtSZoKrkeYoYf1YvBqX1dDscY4wJWjDzOGIDFzdU1a+A2NCF1DWsKypnZ2mltTaMMREnmBaHR0T+CLzmHN8BeEIXUteQ5/GSEBvNtaMz3A7FGGPaJZjE8SDwMP6lRgA+Af4Qsoi6gKraBv5rwz6uGdWPxHhrvBljIkswiSMG+J2qPgcnZpN3C2lUndyygq85WlNvS4wYYyJSMGMcHwIJAccJ+HfeM6coL7+IAb0TmJjT2+1QjDGm3YJJHPGqeqzxwHndPXQhdW7ew8dZtfMQN48bQJTN3TDGRKBgEkeliIxrPBCR8UBV6ELq3BbnFwMwbXx/lyMxxphTE8wYx98DeSKyD/8EwH7ArSGNqpPy+ZS8/CImDe5DVi9rtBljIlObLQ5VXQMMw/901SzgXFXND6ZyEZkiIttEZIeIPNHM+VkisklE1ovIpyIy3CnPFpEqp3y9iLwYcM94554dIvJ7EYmY/p7Pdx3Ce7jKBsWNMRGtxcQhIueLSD8AZ22qccAvgd+KSJujus7TV88D1wDDge83JoYAC1R1lKqOBX4NPBdwbqeqjnW+ZgWUvwDcBwx1vqa0FUu4WOTxktgthqtH9HM7FGOMOWWttTheAmoBRORS4FfAq/h3AJwdRN0TgB2qWqiqtcBCYGrgBapaEXDYgzb2MheRDCBJVT9XVXXiuTGIWFx3tLqOdzeXcN2YTBLiot0OxxhjTllriSNaVcuc17cCs1V1sar+MzAkiLr7A0UBx16n7CQi8rCI7MTf4gjc4yNHRNaJyAoRuSSgTm9bdTr13i8iHhHxlJaWBhFuaL2zsYTqOh/Tbd8NY0yEazVxiEjj4PnfAR8FnAtmUD0oqvq8qg4GHgeedIpLgIGqeh7wY2CBiCS1s97Zqpqrqrlpae7vdZGX72VwWg/OG5DidijGGHNaWkscbwArRGQJ/sdvPwEQkSH4u6vaUgwEjgJnOWUtWYjT7aSqNap6yHmdD+zEvx9IsVNPsHWGhZ2lx8jfc5hbcgcQQWP5xhjTrBYTh6r+EvhHYB5wsTOm0HjPD4Ooew0wVERynI2gbgOWBl4gIkMDDq8Ftjvlac7gOiIyCP8geKGqlgAVInKB8zTV3cCSIGJx1aJ8L9FRwk3jbO6GMSbytdrlpKqfN1P2VTAVq2q9iDwCLAOigTmqWiAiTwMeVV0KPCIiVwJ1wGFghnP7pcDTIlIH+IBZAeMtD+FPZgnAe85X2GrwKW+t9XL52WmkJ8a7HY4xxpy2DhuraI6qvgu826TsqYDXj7Zw32JgcQvnPMDIDgwzpD7eXsr+ihp+foMNihtjOodglhwxp2GRx0vvHnFcMayv26EYY0yHsMQRQocra3l/y36mjs0kLsY+amNM52C/zUJoyfpiaht8tsSIMaZTscQRQnn5XkZkJjE8s11TUIwxJqxZ4giRLfsqKNhXwfTxNihujOlcLHGESF5+EXHRUUwda3M3jDGdiyWOEKit97Fk/T6uHJ5Orx5xbodjjDEdyhJHCHz05X7KKmuZnmuD4saYzscSRwjkebz0TerGpUPdX1zRGGM6miWODnbgaDXLvyrle+OyiI6yBQ2NMZ2PJY4O9vbaYhp8ak9TGWM6LUscHUhVycv3Mv6sXgxK6+l2OMYYExKWODrQ+qJydhw4Zq0NY0ynZomjA73p8RIfG8W1ozPcDsUYY0LGEkcHqapt4L827OO7IzNIjI91OxxjjAkZSxwdZFnB1xytqefmXOumMsZ0bpY4OkhefhFZvRK4IKeP26EYY0xIWeLoAN7Dx1m18xA3j88iyuZuGGM6OUscHWBxfjGqMG2cdVMZYzo/SxynyedTFq0t4qIhfRjQu7vb4RhjTMiFNHGIyBQR2SYiO0TkiWbOzxKRTSKyXkQ+FZHhTvlkEcl3zuWLyBUB9yx36lzvfKWH8j20ZfWuMorKqmyXP2NMlxETqopFJBp4HpgMeIE1IrJUVbcEXLZAVV90rr8BeA6YAhwErlfVfSIyElgGBG5scYeqekIVe3vk5ReR2C2Gq0f0czsUY4w5I0LZ4pgA7FDVQlWtBRYCUwMvUNWKgMMegDrl61R1n1NeACSISLcQxnpKjtXU896mr7luTCYJcdFuh2OMMWdEKBNHf6Ao4NjLya0GAETkYRHZCfwa+FEz9UwD1qpqTUDZXKeb6p9FpNnHmETkfhHxiIintLT01N9FK97ZuI+qugam29wNY0wX4vrguKo+r6qDgceBJwPPicgI4FnggYDiO1R1FHCJ83VXC/XOVtVcVc1NSwvNvhh5Hi+D03pw3oCUkNRvjDHhKJSJoxgIHDHOcspashC4sfFARLKAt4G7VXVnY7mqFjvfjwIL8HeJnXE7S4/h2XOY6bkDaKHRY4wxnVIoE8caYKiI5IhIHHAbsDTwAhEZGnB4LbDdKU8B3gGeUNWVAdfHiEiq8zoWuA7YHML30KJF+V6io4Tvnfet3jdjjOnUQvZUlarWi8gj+J+IigbmqGqBiDwNeFR1KfCIiFwJ1AGHgRnO7Y8AQ4CnROQpp+wqoBJY5iSNaOAD4OVQvYeWNPiUt9Z6uezsNNKT4s/0jzfGGFeFLHEAqOq7wLtNyp4KeP1oC/c9AzzTQrXjOyzAU/Tx9lL2V9Tws+ttUNwY0/W4PjgeiRZ5vPTqHsvfndvX7VCMMeaMs8TRTuXHa3l/y36mju1PXIx9fMaYrsd+87XTkvX7qG3wcUuuLTFijOmaLHG0U15+ESMykxiemeR2KMYY4wpLHO2wtaSCzcUVTB9vg+LGmK7LEkc75Hm8xEVHMXWszd0wxnRdljiCVFvv4y/ri7lyeDq9esS5HY4xxrjGEkeQPvpyP2WVtbbvhjGmy7PEEaQ8j5f0xG5cMjTV7VCMMcZVljiCcOBoNcu/KuV747KIibaPzBjTtdlvwSC8vbaYBp/avhvGGIMljjapKnn5XsYNTGFwWk+3wzHGGNdZ4mjD+qJydhw4xnSbKW6MMYAljjbl5XuJj43iutEZbodijDFhwRJHK6rrGvjrhn18d2QGifGxbodjjDFhwRJHK5YVfM3R6nputkFxY4w5wRJHK/I8XrJ6JXBBTh+3QzHGmLAR0h0AI5mqMqxfIlcMSycqStwOxxhjwoYljhaICE9eN9ztMIwxJuxYV5Uxxph2CWniEJEpIrJNRHaIyBPNnJ8lIptEZL2IfCoiwwPO/dS5b5uIXB1sncYYY0IrZIlDRKKB54FrgOHA9wMTg2OBqo5S1bHAr4HnnHuHA7cBI4ApwB9EJDrIOo0xxoRQKFscE4AdqlqoqrXAQmBq4AWqWhFw2ANQ5/VUYKGq1qjqLmCHU1+bdRpjjAmtUA6O9weKAo69wMSmF4nIw8CPgTjgioB7P29yb+O2e23W6dR7P3A/wMCBA9sfvTHGmGa5Pjiuqs+r6mDgceDJDqx3tqrmqmpuWlpaR1VrjDFdXihbHMVA4MqAWU5ZSxYCLwRxb3vqNMYY08FC2eJYAwwVkRwRicM/2L008AIRGRpweC2w3Xm9FLhNRLqJSA4wFPgimDqNMcaEVshaHKpaLyKPAMuAaGCOqhaIyNOAR1WXAo+IyJVAHXAYmOHcWyAibwJbgHrgYVVtAGiuzrZiyc/PPygiezr+XXaYVOCg20EEKVJitTg7VqTECZETayTEeVZzhaKqzZWbM0hEPKqa63YcwYiUWC3OjhUpcULkxBopcTbH9cFxY4wxkcUShzHGmHaxxBEeZrsdQDtESqwWZ8eKlDghcmKNlDi/xcY4jDHGtIu1OIwxxrSLJQ5jjDHtYonDZSKyO2BpeY/b8TQSkTkickBENgeU9RaR90Vku/O9l5sxNmoh1p+JSLHzua4Xke+6GaMT0wAR+R8R2SIiBSLyqFMeVp9rK3GG1WcqIvEi8oWIbHDi/LlTniMiq52tF/7sTBYOxzjniciugM9zrJtxtoeNcbhMRHYDuaoaVhOBRORS4BjwqqqOdMp+DZSp6q+cvVB6qerjbsbpxNVcrD8Djqnqv7sZWyARyQAyVHWtiCQC+cCNwD2E0efaSpy3EEafqYgI0ENVj4lILPAp8Cj+RVPfUtWFIvIisEFVX2itLpfinAX8l6ouciu2U2UtDtMsVf0YKGtSPBWY77yej/+XietaiDXsqGqJqq51Xh8FtuJf9TmsPtdW4gwr6nfMOYx1vhT/KtuNv4zD4fNsKc6IZYnDfQr8t4jkO0vBh7O+qlrivP4a6OtmMEF4REQ2Ol1ZYdGt1khEsoHzgNWE8efaJE4Is8/U2eBtPXAAeB/YCZSrar1zSeCWDK5pGqeqNn6ev3Q+z/8QkW4uhtguljjcd7GqjsO/q+HDTrdL2FN/H2c4/9X0AjAYGAuUAL91N5xviEhPYDHw9002Mwurz7WZOMPuM1XVBmcH0Sz8G70NczmkZjWNU0RGAj/FH+/5QG/8W0tEBEscLlPVYuf7AeBt/P/4w9V+p/+7sR/8gMvxtEhV9zv/s/qAlwmTz9Xp414MvK6qbznFYfe5NhdnuH6mAKpaDvwPcCGQIiKNC7iG1dYLAXFOcboEVVVrgLmE0efZFkscLhKRHs7gIyLSA7gK2Nz6Xa5airOCsfN9iYuxtKrxF7HjJsLgc3UGSV8BtqrqcwGnwupzbSnOcPtMRSRNRFKc1wnAZPzjMf8D3OxcFg6fZ3Nxfhnwx4LgH4dx/d9osOypKheJyCD8rQzwL3G/QFV/6WJIJ4jIG8Dl+Jd+3g/8C/AX4E1gILAHuEVVXR+UbiHWy/F3qSiwG3ggYBzBFSJyMfAJsAnwOcX/G//4Qdh8rq3E+X3C6DMVkdH4B7+j8f8R/KaqPu38f7UQf/fPOuBO56/6cIvzIyANEGA9MCtgED2sWeIwxhjTLtZVZYwxpl0scRhjjGkXSxzGGGPaxRKHMcaYdrHEYYwxpl0scZhOR0RURH4bcPwTZ9HDjqg7ZI9LtqduZ/XX9SKyV0RKA1ZYzW5HHTc4iyoa0y72OK7pdESkGv+SGOer6kER+QnQU1V/1gF1H1PVnm1cExOwVlKH1t3MPffgX135kfb+PGNOlbU4TGdUj38/539oekJEskXkI2dhuQ9FZKBTPk9EXhCRz0WkUEQudxby2yoi85rU8R/OvgofikiaU7ZcRP6v+PdUeVRExovICmfxymVNZl031pMjIp+Jfz+WZ5qce0xE1jhx/jyYNy0iY534N4rI242LEDqx/c5pkWwWkQlO+T0i8v+c132dezY4X5OclQ3ecY43i8itwcRhOj9LHKazeh64Q0SSm5T/JzBfVUcDrwO/DzjXC/9aR/+AfxmQ/wBGAKPkm012egAeVR0BrMA/S71RnKrmOnX+J3Czqo4H5gDNrQjwO+AFVR2Fv4UEgIhcBQzFv3bRWGB8kItfvgo87ry3TU1i6+4ssveQE09TvwdWqOoYYBxQAEwB9qnqGGefk78FEYPpAixxmE7JWc31VeBHTU5dCCxwXv8JuDjg3F+d1Wk3AftVdZOzoF8BkO1c4wP+7Lx+rcn9jeXnACOB952ltJ/Ev9heUxcBbwTE0ugq52sdsBb/CqpDW3m7OAkyRVVXOEXzgcBk8wac2LskqXHtpABX4F/9tnEl1yP4P4fJIvKsiFzilBlDTNuXGBOx/i/+X7xzg7y+cT0jX8DrxuOW/l8JHCSsdL4LUKCqFwbxM5sbZBTg31T1pSDuD1bTn9Pm4KaqfiUi44DvAs+IyIeq+nQHxmQilLU4TKflLBT4JvCDgOJVwG3O6zvwL+bXHlF8s/Lq7fi3AW1qG5AmIheCf4lyERnRzHUrm8TSaBlwr7MfBiLSX0TSWwvKaQ0cFpFLnKK78HelNbrVqeti4EgzrYcPgQeda6JFJFlEMoHjqvoa8Bv8XVjGWIvDdHq/BQKfOPohMFdEHgNKgZntrK8S/0Y8T+LfN+NbA8aqWisiNwO/d7qQYvC3fgqaXPoosEBEHidg6W9V/W8RORf4zL/iNseAO2l7n44ZwIsi0h0obPLeqkVkHf5tS+9t5t5Hgdki8gOgAX8SSQJ+IyI+oM4pM8YexzWmsxOR5cBPVNXjdiymc7CuKmOMMe1iLQ5jjDHtYi0OY4wx7WKJwxhjTLtY4jDGGNMuljiMMca0iyUOY4wx7fL/AVPTY3/0dBJ2AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "limit=40; start=2; step=6;\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Nombre de Topics\")\n",
    "plt.ylabel(\"Score de cohérence\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous voyons qu'il y a une croissance en flèche jusqu'au nombre de 8 topics, avant de remonter et de se stabiliser à partir de 25 topics. Les organisateurs de la conférence 2012 ont prévu 7 topics, et pourtant l'agorithme de Gensim en détecte un nombre optimal de 8. Or, un topic des organisateurs a été divisé en deux journées, donc en deux sous-topic qui ont le même nom et qui sont présidés par la même personne. \n",
    "Le haut score de cohérence révèle cependant que les abstracts sont assez cohérents entre eux, et reliés par plusieurs thèmes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous affichons ici ensuite le score de cohérence précis pour une suite de nombre de topics, pour avoir une vision plus précise : nous pouvons voir avec précision que le score de cohérence à 20 topics est un vrai petit optimum, mais que la différence dans le score de cohérence avec le score de 8 topics est relativement faible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de Topics = 2  a une valeur de cohérence de :  0.3051\n",
      "Nombre de Topics = 8  a une valeur de cohérence de :  0.4212\n",
      "Nombre de Topics = 14  a une valeur de cohérence de :  0.4162\n",
      "Nombre de Topics = 20  a une valeur de cohérence de :  0.4563\n",
      "Nombre de Topics = 26  a une valeur de cohérence de :  0.4767\n",
      "Nombre de Topics = 32  a une valeur de cohérence de :  0.4576\n",
      "Nombre de Topics = 38  a une valeur de cohérence de :  0.4667\n"
     ]
    }
   ],
   "source": [
    "for m, cv in zip(x, coherence_values):\n",
    "    print(\"Nombre de Topics =\", m, \" a une valeur de cohérence de : \", round(cv, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voir les topics ###\n",
    "\n",
    "La cellule ci-dessous permet d'afficher pour chaque topic (représenté par un nombre entier) les 10 mots-clés représentant le mieux ce topic, et un facteur attribué à chacun de ces mots permettant de voir l'importance de chaque mot-clé au sein du cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.044*\"tool\" + 0.042*\"page\" + 0.028*\"pron\" + 0.024*\"edition\" + 0.022*\"make\" '\n",
      "  '+ 0.022*\"work\" + 0.020*\"element\" + 0.020*\"teach\" + 0.018*\"order\" + '\n",
      "  '0.018*\"change\"'),\n",
      " (1,\n",
      "  '0.073*\"text\" + 0.049*\"project\" + 0.032*\"collection\" + 0.029*\"model\" + '\n",
      "  '0.029*\"encode\" + 0.025*\"support\" + 0.025*\"pron\" + 0.024*\"research\" + '\n",
      "  '0.022*\"service\" + 0.019*\"framework\"'),\n",
      " (2,\n",
      "  '0.061*\"content\" + 0.041*\"present\" + 0.034*\"critical\" + 0.032*\"structure\" + '\n",
      "  '0.029*\"digital\" + 0.029*\"pron\" + 0.024*\"site\" + 0.024*\"user\" + 0.022*\"case\" '\n",
      "  '+ 0.022*\"presentation\"'),\n",
      " (3,\n",
      "  '0.036*\"transcription\" + 0.036*\"make\" + 0.027*\"linguistic\" + '\n",
      "  '0.022*\"annotation\" + 0.022*\"image\" + 0.022*\"create\" + 0.019*\"version\" + '\n",
      "  '0.019*\"full\" + 0.017*\"book\" + 0.017*\"addition\"'),\n",
      " (4,\n",
      "  '0.102*\"mad\" + 0.059*\"type\" + 0.036*\"note\" + 0.025*\"editor\" + '\n",
      "  '0.023*\"provide\" + 0.023*\"variant\" + 0.020*\"database\" + 0.020*\"complex\" + '\n",
      "  '0.020*\"standard\" + 0.016*\"add\"'),\n",
      " (5,\n",
      "  '0.034*\"system\" + 0.031*\"include\" + 0.030*\"record\" + 0.022*\"project\" + '\n",
      "  '0.020*\"information\" + 0.019*\"develop\" + 0.017*\"form\" + 0.017*\"paper\" + '\n",
      "  '0.016*\"reference\" + 0.016*\"history\"'),\n",
      " (6,\n",
      "  '0.060*\"text\" + 0.057*\"source\" + 0.042*\"process\" + 0.036*\"word\" + '\n",
      "  '0.031*\"tag\" + 0.028*\"textual\" + 0.027*\"datum\" + 0.019*\"unit\" + '\n",
      "  '0.017*\"processing\" + 0.017*\"analysis\"'),\n",
      " (7,\n",
      "  '0.062*\"document\" + 0.050*\"encode\" + 0.046*\"language\" + 0.029*\"scholar\" + '\n",
      "  '0.029*\"web\" + 0.025*\"access\" + 0.023*\"output\" + 0.021*\"css\" + '\n",
      "  '0.019*\"directly\" + 0.017*\"provide\"')]\n"
     ]
    }
   ],
   "source": [
    "optimal_model = model_list[1] #optimal_model et model_list sont des éléments gensim contenant les topics et les textes. \n",
    "#Model_list est la liste des modèles de nombres de topics et la distribution interne donnée ci-dessus. Comme le modèle le plus efficace est\n",
    "#celui à 8 topics, je le sélectionne\n",
    "model_topics = optimal_model.show_topics(formatted=False) #Model_topics est une liste produite par la méthode show_topics.\n",
    "pprint(optimal_model.print_topics(num_words=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation ###\n",
    "\n",
    "Nous allons produire sous la forme d'un tableau le sujet dominant de chaque texte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document n°</th>\n",
       "      <th>Topic dominant n°</th>\n",
       "      <th>Pourcentage de Contribution au topic</th>\n",
       "      <th>Mots-clés</th>\n",
       "      <th>Texte</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>content, present, critical, structure, digital...</td>\n",
       "      <td>get critical with the apparatus : how to re - ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4926</td>\n",
       "      <td>tool, page, pron, edition, make, work, element...</td>\n",
       "      <td>deliver the Digital Edition : time and space o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.3953</td>\n",
       "      <td>content, present, critical, structure, digital...</td>\n",
       "      <td>romantic circle and the Problem of Legacy Data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.1688</td>\n",
       "      <td>mad, type, note, editor, provide, variant, dat...</td>\n",
       "      <td>resolve the Durand Conundrum \\n Lou Burnard th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1972</td>\n",
       "      <td>tool, page, pron, edition, make, work, element...</td>\n",
       "      <td>what do -PRON- mean to \" support TEI \" for man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.3884</td>\n",
       "      <td>transcription, make, linguistic, annotation, i...</td>\n",
       "      <td>TEI and the Early Modern OCR Project ( eMOP ) ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.6335</td>\n",
       "      <td>system, include, record, project, information,...</td>\n",
       "      <td>encode Financial Records for Historical Resear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1664</td>\n",
       "      <td>tool, page, pron, edition, make, work, element...</td>\n",
       "      <td>Curation , Exploration , and Collaboration in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.3018</td>\n",
       "      <td>content, present, critical, structure, digital...</td>\n",
       "      <td>More Hack , More yack : Juxta , Parallel Segme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.3168</td>\n",
       "      <td>transcription, make, linguistic, annotation, i...</td>\n",
       "      <td>TEI for Linguists : progress and perspective \\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.2781</td>\n",
       "      <td>transcription, make, linguistic, annotation, i...</td>\n",
       "      <td>the LINGUIST List Corpus : a large mailing Lis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.6675</td>\n",
       "      <td>text, project, collection, model, encode, supp...</td>\n",
       "      <td>Electronic Text Services , from project to Por...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1368</td>\n",
       "      <td>text, project, collection, model, encode, supp...</td>\n",
       "      <td>conversation , Translation , Materiality :    ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.1893</td>\n",
       "      <td>transcription, make, linguistic, annotation, i...</td>\n",
       "      <td>the Project Gutenberg book archive as a TEI P5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.6406</td>\n",
       "      <td>mad, type, note, editor, provide, variant, dat...</td>\n",
       "      <td>-PRON- be all mad here : map TEI to the Metada...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.1757</td>\n",
       "      <td>content, present, critical, structure, digital...</td>\n",
       "      <td>Library Support for the TEI : tutorial , teach...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.6037</td>\n",
       "      <td>text, source, process, word, tag, textual, dat...</td>\n",
       "      <td>construct Analytic Data Categories for Corpus ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.2954</td>\n",
       "      <td>mad, type, note, editor, provide, variant, dat...</td>\n",
       "      <td>Clarin - D : TEI for Egyptologists \\n   this p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.2041</td>\n",
       "      <td>content, present, critical, structure, digital...</td>\n",
       "      <td>why and how to encode word structure and word ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.1727</td>\n",
       "      <td>transcription, make, linguistic, annotation, i...</td>\n",
       "      <td>Automatic Linguistic Annotation with TEI - Out...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1679</td>\n",
       "      <td>text, project, collection, model, encode, supp...</td>\n",
       "      <td>reboot TEI xpointer \\n the TEI XPointer scheme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.5968</td>\n",
       "      <td>document, encode, language, scholar, web, acce...</td>\n",
       "      <td>TEI Boilerplate \\n TEI BOILERPLATE John A. Wal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4371</td>\n",
       "      <td>tool, page, pron, edition, make, work, element...</td>\n",
       "      <td>teach the TEI : from training to academic curr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.3096</td>\n",
       "      <td>content, present, critical, structure, digital...</td>\n",
       "      <td>ambiguous term : race and TEI Markup \\n Theres...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Document n°  Topic dominant n°  Pourcentage de Contribution au topic  \\\n",
       "0             0                2.0                                0.2871   \n",
       "1             1                0.0                                0.4926   \n",
       "2             2                2.0                                0.3953   \n",
       "3             3                4.0                                0.1688   \n",
       "4             4                0.0                                0.1972   \n",
       "5             5                3.0                                0.3884   \n",
       "6             6                5.0                                0.6335   \n",
       "7             7                0.0                                0.1664   \n",
       "8             8                2.0                                0.3018   \n",
       "9             9                3.0                                0.3168   \n",
       "10           10                3.0                                0.2781   \n",
       "11           11                1.0                                0.6675   \n",
       "12           12                1.0                                0.1368   \n",
       "13           13                3.0                                0.1893   \n",
       "14           14                4.0                                0.6406   \n",
       "15           15                2.0                                0.1757   \n",
       "16           16                6.0                                0.6037   \n",
       "17           17                4.0                                0.2954   \n",
       "18           18                2.0                                0.2041   \n",
       "19           19                3.0                                0.1727   \n",
       "20           20                1.0                                0.1679   \n",
       "21           21                7.0                                0.5968   \n",
       "22           22                0.0                                0.4371   \n",
       "23           23                2.0                                0.3096   \n",
       "\n",
       "                                            Mots-clés  \\\n",
       "0   content, present, critical, structure, digital...   \n",
       "1   tool, page, pron, edition, make, work, element...   \n",
       "2   content, present, critical, structure, digital...   \n",
       "3   mad, type, note, editor, provide, variant, dat...   \n",
       "4   tool, page, pron, edition, make, work, element...   \n",
       "5   transcription, make, linguistic, annotation, i...   \n",
       "6   system, include, record, project, information,...   \n",
       "7   tool, page, pron, edition, make, work, element...   \n",
       "8   content, present, critical, structure, digital...   \n",
       "9   transcription, make, linguistic, annotation, i...   \n",
       "10  transcription, make, linguistic, annotation, i...   \n",
       "11  text, project, collection, model, encode, supp...   \n",
       "12  text, project, collection, model, encode, supp...   \n",
       "13  transcription, make, linguistic, annotation, i...   \n",
       "14  mad, type, note, editor, provide, variant, dat...   \n",
       "15  content, present, critical, structure, digital...   \n",
       "16  text, source, process, word, tag, textual, dat...   \n",
       "17  mad, type, note, editor, provide, variant, dat...   \n",
       "18  content, present, critical, structure, digital...   \n",
       "19  transcription, make, linguistic, annotation, i...   \n",
       "20  text, project, collection, model, encode, supp...   \n",
       "21  document, encode, language, scholar, web, acce...   \n",
       "22  tool, page, pron, edition, make, work, element...   \n",
       "23  content, present, critical, structure, digital...   \n",
       "\n",
       "                                                Texte  \n",
       "0   get critical with the apparatus : how to re - ...  \n",
       "1   deliver the Digital Edition : time and space o...  \n",
       "2   romantic circle and the Problem of Legacy Data...  \n",
       "3   resolve the Durand Conundrum \\n Lou Burnard th...  \n",
       "4   what do -PRON- mean to \" support TEI \" for man...  \n",
       "5   TEI and the Early Modern OCR Project ( eMOP ) ...  \n",
       "6   encode Financial Records for Historical Resear...  \n",
       "7   Curation , Exploration , and Collaboration in ...  \n",
       "8   More Hack , More yack : Juxta , Parallel Segme...  \n",
       "9   TEI for Linguists : progress and perspective \\...  \n",
       "10  the LINGUIST List Corpus : a large mailing Lis...  \n",
       "11  Electronic Text Services , from project to Por...  \n",
       "12  conversation , Translation , Materiality :    ...  \n",
       "13  the Project Gutenberg book archive as a TEI P5...  \n",
       "14  -PRON- be all mad here : map TEI to the Metada...  \n",
       "15  Library Support for the TEI : tutorial , teach...  \n",
       "16  construct Analytic Data Categories for Corpus ...  \n",
       "17  Clarin - D : TEI for Egyptologists \\n   this p...  \n",
       "18  why and how to encode word structure and word ...  \n",
       "19  Automatic Linguistic Annotation with TEI - Out...  \n",
       "20  reboot TEI xpointer \\n the TEI XPointer scheme...  \n",
       "21  TEI Boilerplate \\n TEI BOILERPLATE John A. Wal...  \n",
       "22  teach the TEI : from training to academic curr...  \n",
       "23  ambiguous term : race and TEI Markup \\n Theres...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=documents):\n",
    "    # Iniatilise la sortie en un tableau pandas\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Récupère le sujet principal de chaque document\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        #Récupère pour chaque document le sujet dominant, le pourcentage de contribution à ce sujet et les mots-clés\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Ajout du titre dans la dernière colonne en prenant les premiers mots de chaque texte\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "\n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=optimal_model, corpus=corpus, texts=documents)\n",
    "\n",
    "# Mise en place du tableau\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document n°', 'Topic dominant n°', 'Pourcentage de Contribution au topic', 'Mots-clés', 'Texte']\n",
    "\n",
    "# Affiche le tableau en présentant seulement les 10 premiers textes. Changez le chiffre 10 si vous le souhaitez pour obtenir l'étude de plus de textes.\n",
    "df_dominant_topic.head(46)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons remarquer que le modèle représenté fonctionnerait autour d'un texte central souvent très représentatif (plus de 50% en moyenne) avec des textes \" périphériques \" représentatif à hauteur de minimum 15%.\n",
    "Néanmoins, les scores de contribution restent particulièrement élevés globalement. Je pense que c'est trompeur, et que c'est dû au faible nombre de textes par rapport au relativement grand nombre de topic, donc que les topics se calquent plus naturellement sur les 1 à 3 textes qui composent en moyenne un topic.\n",
    "\n",
    "Il y a donc une fausse impression d'homogénéité des textes du corpus causée par la faible taille du corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chercher les abstracts les plus représentatifs ###\n",
    "\n",
    "Nous pouvons alors chercher à sortir les abstracts les plus représentatifs de chaque topic :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic n°</th>\n",
       "      <th>Pourcentage de contribution à un topic</th>\n",
       "      <th>Mots-Clés</th>\n",
       "      <th>Texte</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4926</td>\n",
       "      <td>tool, page, pron, edition, make, work, element...</td>\n",
       "      <td>deliver the Digital Edition : time and space o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.6675</td>\n",
       "      <td>text, project, collection, model, encode, supp...</td>\n",
       "      <td>Electronic Text Services , from project to Por...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.3953</td>\n",
       "      <td>content, present, critical, structure, digital...</td>\n",
       "      <td>romantic circle and the Problem of Legacy Data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.3884</td>\n",
       "      <td>transcription, make, linguistic, annotation, i...</td>\n",
       "      <td>TEI and the Early Modern OCR Project ( eMOP ) ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.6406</td>\n",
       "      <td>mad, type, note, editor, provide, variant, dat...</td>\n",
       "      <td>-PRON- be all mad here : map TEI to the Metada...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.6335</td>\n",
       "      <td>system, include, record, project, information,...</td>\n",
       "      <td>encode Financial Records for Historical Resear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.6037</td>\n",
       "      <td>text, source, process, word, tag, textual, dat...</td>\n",
       "      <td>construct Analytic Data Categories for Corpus ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.5968</td>\n",
       "      <td>document, encode, language, scholar, web, acce...</td>\n",
       "      <td>TEI Boilerplate \\n TEI BOILERPLATE John A. Wal...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic n°  Pourcentage de contribution à un topic  \\\n",
       "0       0.0                                  0.4926   \n",
       "1       1.0                                  0.6675   \n",
       "2       2.0                                  0.3953   \n",
       "3       3.0                                  0.3884   \n",
       "4       4.0                                  0.6406   \n",
       "5       5.0                                  0.6335   \n",
       "6       6.0                                  0.6037   \n",
       "7       7.0                                  0.5968   \n",
       "\n",
       "                                           Mots-Clés  \\\n",
       "0  tool, page, pron, edition, make, work, element...   \n",
       "1  text, project, collection, model, encode, supp...   \n",
       "2  content, present, critical, structure, digital...   \n",
       "3  transcription, make, linguistic, annotation, i...   \n",
       "4  mad, type, note, editor, provide, variant, dat...   \n",
       "5  system, include, record, project, information,...   \n",
       "6  text, source, process, word, tag, textual, dat...   \n",
       "7  document, encode, language, scholar, web, acce...   \n",
       "\n",
       "                                               Texte  \n",
       "0  deliver the Digital Edition : time and space o...  \n",
       "1  Electronic Text Services , from project to Por...  \n",
       "2  romantic circle and the Problem of Legacy Data...  \n",
       "3  TEI and the Early Modern OCR Project ( eMOP ) ...  \n",
       "4  -PRON- be all mad here : map TEI to the Metada...  \n",
       "5  encode Financial Records for Historical Resear...  \n",
       "6  construct Analytic Data Categories for Corpus ...  \n",
       "7  TEI Boilerplate \\n TEI BOILERPLATE John A. Wal...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group top 5 sentences under each topic\n",
    "sent_topics_sorteddf_mallet = pd.DataFrame()\n",
    "\n",
    "sent_topics_outdf_grpd = df_topic_sents_keywords.groupby('Dominant_Topic')\n",
    "\n",
    "for i, grp in sent_topics_outdf_grpd:\n",
    "    sent_topics_sorteddf_mallet = pd.concat([sent_topics_sorteddf_mallet, \n",
    "                                             grp.sort_values(['Perc_Contribution'], ascending=[0]).head(1)], \n",
    "                                            axis=0)\n",
    "\n",
    "# Reset Index    \n",
    "sent_topics_sorteddf_mallet.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Format\n",
    "sent_topics_sorteddf_mallet.columns = ['Topic n°', \"Pourcentage de contribution à un topic\", \"Mots-Clés\", \"Texte\"]\n",
    "\n",
    "# Show\n",
    "sent_topics_sorteddf_mallet.head(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La théorie formulée précédemment semble se vérifier : certains abstracts incarnent à eux seuls leur topic.\n",
    "\n",
    "Cependant, les résultats que nous venons de voir montrent aussi que les topics restent représentatifs et cohérents : globalement chaque abstract participe à son Topic à hauteur d'au moins 20% (cf le tableau un peu plus haut), mais aucun abstract ne dépasse les 70% de participation au topic. Il n'y a donc pas d'emprise démesurée d'un abstract sur les autres."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Des topics égaux ? ###\n",
    "\n",
    "Nous allons maintenant regarder le nombre de documents par topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>N° du topic</th>\n",
       "      <th>Mots-clés du topic</th>\n",
       "      <th>Nombre de documents relevant de ce topic</th>\n",
       "      <th>Proportion des documents parfaitement représentatifs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>content, present, critical, structure, digital...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.1667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>tool, page, pron, edition, make, work, element...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>content, present, critical, structure, digital...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>mad, type, note, editor, provide, variant, dat...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.2083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>tool, page, pron, edition, make, work, element...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>transcription, make, linguistic, annotation, i...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6.0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>system, include, record, project, information,...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>tool, page, pron, edition, make, work, element...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8.0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>content, present, critical, structure, digital...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9.0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>transcription, make, linguistic, annotation, i...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10.0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>transcription, make, linguistic, annotation, i...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11.0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>text, project, collection, model, encode, supp...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12.0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>text, project, collection, model, encode, supp...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13.0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>transcription, make, linguistic, annotation, i...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14.0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>mad, type, note, editor, provide, variant, dat...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15.0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>content, present, critical, structure, digital...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16.0</th>\n",
       "      <td>6.0</td>\n",
       "      <td>text, source, process, word, tag, textual, dat...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17.0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>mad, type, note, editor, provide, variant, dat...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18.0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>content, present, critical, structure, digital...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19.0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>transcription, make, linguistic, annotation, i...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20.0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>text, project, collection, model, encode, supp...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21.0</th>\n",
       "      <td>7.0</td>\n",
       "      <td>document, encode, language, scholar, web, acce...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>tool, page, pron, edition, make, work, element...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23.0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>content, present, critical, structure, digital...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      N° du topic                                 Mots-clés du topic  \\\n",
       "0.0           2.0  content, present, critical, structure, digital...   \n",
       "1.0           0.0  tool, page, pron, edition, make, work, element...   \n",
       "2.0           2.0  content, present, critical, structure, digital...   \n",
       "3.0           4.0  mad, type, note, editor, provide, variant, dat...   \n",
       "4.0           0.0  tool, page, pron, edition, make, work, element...   \n",
       "5.0           3.0  transcription, make, linguistic, annotation, i...   \n",
       "6.0           5.0  system, include, record, project, information,...   \n",
       "7.0           0.0  tool, page, pron, edition, make, work, element...   \n",
       "8.0           2.0  content, present, critical, structure, digital...   \n",
       "9.0           3.0  transcription, make, linguistic, annotation, i...   \n",
       "10.0          3.0  transcription, make, linguistic, annotation, i...   \n",
       "11.0          1.0  text, project, collection, model, encode, supp...   \n",
       "12.0          1.0  text, project, collection, model, encode, supp...   \n",
       "13.0          3.0  transcription, make, linguistic, annotation, i...   \n",
       "14.0          4.0  mad, type, note, editor, provide, variant, dat...   \n",
       "15.0          2.0  content, present, critical, structure, digital...   \n",
       "16.0          6.0  text, source, process, word, tag, textual, dat...   \n",
       "17.0          4.0  mad, type, note, editor, provide, variant, dat...   \n",
       "18.0          2.0  content, present, critical, structure, digital...   \n",
       "19.0          3.0  transcription, make, linguistic, annotation, i...   \n",
       "20.0          1.0  text, project, collection, model, encode, supp...   \n",
       "21.0          7.0  document, encode, language, scholar, web, acce...   \n",
       "22.0          0.0  tool, page, pron, edition, make, work, element...   \n",
       "23.0          2.0  content, present, critical, structure, digital...   \n",
       "\n",
       "      Nombre de documents relevant de ce topic  \\\n",
       "0.0                                        4.0   \n",
       "1.0                                        3.0   \n",
       "2.0                                        6.0   \n",
       "3.0                                        5.0   \n",
       "4.0                                        3.0   \n",
       "5.0                                        1.0   \n",
       "6.0                                        1.0   \n",
       "7.0                                        1.0   \n",
       "8.0                                        NaN   \n",
       "9.0                                        NaN   \n",
       "10.0                                       NaN   \n",
       "11.0                                       NaN   \n",
       "12.0                                       NaN   \n",
       "13.0                                       NaN   \n",
       "14.0                                       NaN   \n",
       "15.0                                       NaN   \n",
       "16.0                                       NaN   \n",
       "17.0                                       NaN   \n",
       "18.0                                       NaN   \n",
       "19.0                                       NaN   \n",
       "20.0                                       NaN   \n",
       "21.0                                       NaN   \n",
       "22.0                                       NaN   \n",
       "23.0                                       NaN   \n",
       "\n",
       "      Proportion des documents parfaitement représentatifs  \n",
       "0.0                                              0.1667     \n",
       "1.0                                              0.1250     \n",
       "2.0                                              0.2500     \n",
       "3.0                                              0.2083     \n",
       "4.0                                              0.1250     \n",
       "5.0                                              0.0417     \n",
       "6.0                                              0.0417     \n",
       "7.0                                              0.0417     \n",
       "8.0                                                 NaN     \n",
       "9.0                                                 NaN     \n",
       "10.0                                                NaN     \n",
       "11.0                                                NaN     \n",
       "12.0                                                NaN     \n",
       "13.0                                                NaN     \n",
       "14.0                                                NaN     \n",
       "15.0                                                NaN     \n",
       "16.0                                                NaN     \n",
       "17.0                                                NaN     \n",
       "18.0                                                NaN     \n",
       "19.0                                                NaN     \n",
       "20.0                                                NaN     \n",
       "21.0                                                NaN     \n",
       "22.0                                                NaN     \n",
       "23.0                                                NaN     "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Permet de calculer le nombre de documents par topic\n",
    "topic_counts = df_topic_sents_keywords['Dominant_Topic'].value_counts()\n",
    "\n",
    "# Proportion des documents étant parfaitement représentatif du topic\n",
    "topic_contribution = round(topic_counts/topic_counts.sum(), 4)\n",
    "\n",
    "# N° du topic et mot-clés\n",
    "topic_num_keywords = df_topic_sents_keywords[['Dominant_Topic', 'Topic_Keywords']]\n",
    "\n",
    "# Lie les colonnes\n",
    "df_dominant_topics = pd.concat([topic_num_keywords, topic_counts, topic_contribution], axis=1)\n",
    "\n",
    "# Renomme les colonnes\n",
    "df_dominant_topics.columns = ['N° du topic', 'Mots-clés du topic', 'Nombre de documents relevant de ce topic', 'Proportion des documents parfaitement représentatifs']\n",
    "\n",
    "# Représentation\n",
    "df_dominant_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons conclure qu'en regardant la répartition des abstracts au sein des clusters, il y a des textes qui sont en réalité réellement reliés aux autres, et que l'on trouve également des abstracts n'ayant aucun lien avec les autres. Certains topics ont donc plutôt la fonction d'écarter des abstracts trop différents des autres plutôt que de dégager différents sujets à partir d'un ensemble de documents."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
